{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was tested with the following Python packages:\n",
    "- torch==2.5.1\n",
    "- transformers==4.48.1\n",
    "- datasets==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-style Encoder Language Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows the model to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. Thus it can be seen as a model producing a contextualized embedding that can be used as a replacement for e.g., GloVe.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Bidirectional Context**: Unlike traditional models that read text sequentially (left-to-right or right-to-left), BERT reads the entire sequence of words at once, allowing it to understand the context of a word based on both its preceding and following words.\n",
    "\n",
    "2. **Pre-training Objectives**:\n",
    "    - **Masked Language Model (MLM)**: Randomly masks some of the tokens in the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.\n",
    "    - **Next Sentence Prediction (NSP)**: Predicts whether a given pair of sentences is consecutive in the original text, helping the model understand sentence relationships.\n",
    "\n",
    "## Applications\n",
    "\n",
    "BERT can be fine-tuned for various downstream tasks, including:\n",
    "\n",
    "- **Text Classification**: Sentiment analysis, spam detection, etc.\n",
    "- **Question Answering**: Extracting answers from a given context.\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
    "- **Text Summarization**: Generating concise summaries of longer texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first install the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\uni stuttgart\\è¯¾ç¨‹\\intro to dl\\my_env\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the SST2 dataset. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with pre-trained models, you will have to download the weights from somewhere.\\\n",
    "These weights have to fit the model architecture, therefore we rely on a library for the models and its weights.\\\n",
    "We here use the Hugging Face transformers library for loading the model and its tokenizer.\n",
    "\n",
    "There are so many encoder language models; here we use RoBERTa which is an optimized (in terms of hyperparameters) version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "'''\"roberta-base\"æ˜¯ä¸€ä¸ªç”±Facebook AIç ”ç©¶é™¢å¼€å‘çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå®ƒæ˜¯BERTæ¨¡å‹çš„æ”¹è¿›ç‰ˆæœ¬ã€‚\n",
    "é€šè¿‡ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒçš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿå¼€å§‹å¤„ç†å„ç§NLPä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿç­‰'''\n",
    "'''è¿™æ®µä»£ç ä¸ºåç»­çš„NLPä»»åŠ¡åšå¥½äº†å‡†å¤‡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨tokenizeræ¥å¤„ç†è¾“å…¥æ–‡æœ¬ï¼Œ\n",
    "ç„¶åå°†å¤„ç†åçš„è¾“å…¥ä¼ é€’ç»™robertaæ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†æˆ–å¾®è°ƒã€‚'''\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the model's architecture by printing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we prepare the dataset, but this time the tokenizer does most of the work converting text to token ids for the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate if you want to use a subset of the dataset for faster training\n",
    "USE_SUBSET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4950737d9b5c40228fbed978212261fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c6f642539468489f0b5dade75097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df435b4e2e64528bf91107a3422ec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value(dtype='int32', id=None), 'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}\n",
      "{'idx': 0, 'sentence': 'hide new secretions from the parental units ', 'label': 0, 'input_ids': [0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize(examples, tokenizer):\n",
    "    # tokenizerçš„ä¸»è¦åŠŸèƒ½æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—\n",
    "    # è¿™æ˜¯è¾“å…¥æ•°æ®ã€‚examplesæ˜¯ä¸€ä¸ªå­—å…¸æˆ–ç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼ŒåŒ…å«äº†å¤šä¸ªæ ·æœ¬ã€‚\n",
    "    # [\"sentence\"]è¡¨ç¤ºæˆ‘ä»¬æ­£åœ¨è®¿é—®æ¯ä¸ªæ ·æœ¬çš„\"sentence\"å­—æ®µï¼Œè¿™é‡Œé¢å­˜å‚¨ç€éœ€è¦è¢«å¤„ç†çš„æ–‡æœ¬ã€‚\n",
    "    # truncation=Trueå¯ç”¨äº†æˆªæ–­åŠŸèƒ½ã€‚å¦‚æœè¾“å…¥çš„å¥å­é•¿åº¦è¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¯¹äºRoBERTaé€šå¸¸æ˜¯512ä¸ªtokenï¼‰ï¼Œtokenizerä¼šè‡ªåŠ¨æˆªæ–­å¥å­ä»¥é€‚åº”æ¨¡å‹ã€‚\n",
    "    # æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰é€šå¸¸ç”¨äºæŒ‡ç¤ºå“ªäº›tokenæ˜¯çœŸå®çš„è¾“å…¥ï¼Œå“ªäº›æ˜¯å¡«å……ï¼ˆpaddingï¼‰ã€‚\n",
    "    # å°†æ­¤å‚æ•°è®¾ç½®ä¸ºFalseæ„å‘³ç€tokenizerä¸ä¼šè¿”å›æ³¨æ„åŠ›æ©ç ã€‚\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, return_attention_mask=False)\n",
    "\n",
    "\n",
    "# Prepare the datasets\n",
    "# dataset.map()ï¼šmap æ˜¯æ•°æ®é›†å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬åº”ç”¨ä¸€ä¸ªå‡½æ•°ã€‚è¿™ä¸ªæ–¹æ³•ä¼šè¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†ç»è¿‡å¤„ç†çš„æ ·æœ¬ã€‚\n",
    "# tokenizeï¼šè¿™æ˜¯æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ã€‚å®ƒè¢«ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ é€’ç»™ map æ–¹æ³•ï¼Œè¡¨ç¤ºè¦å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨è¿™ä¸ªå‡½æ•°ã€‚\n",
    "# fn_kwargs å‚æ•°ç”¨äºæŒ‡å®šè¦ä¼ é€’ç»™æ˜ å°„å‡½æ•°ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯ tokenize å‡½æ•°ï¼‰çš„é¢å¤–å…³é”®å­—å‚æ•°\n",
    "# dict(tokenizer=tokenizer)åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªé”®å€¼å¯¹ã€‚é”®æ˜¯ 'tokenizer'ï¼Œå€¼æ˜¯ä¹‹å‰å®šä¹‰çš„ tokenizer å¯¹è±¡ã€‚\n",
    "'''å½“ dataset.map() è°ƒç”¨ tokenize å‡½æ•°æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å°† examples ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ é€’ã€‚\n",
    "ç„¶åï¼Œå®ƒä¼šä½¿ç”¨ fn_kwargs ä¸­æŒ‡å®šçš„å‚æ•°ä½œä¸ºé¢å¤–çš„å…³é”®å­—å‚æ•°ã€‚\n",
    "å› æ­¤ï¼Œtokenize å‡½æ•°ä¼šæ”¶åˆ°ä¸¤ä¸ªå‚æ•°ï¼šexamplesï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰å’Œ tokenizerï¼ˆæ¥è‡ª fn_kwargsï¼‰ã€‚'''\n",
    "# è®¾ç½® batched=True æ„å‘³ç€ map æ–¹æ³•ä¼šä»¥æ‰¹å¤„ç†æ¨¡å¼è¿è¡Œã€‚\n",
    "# num_proc=10è®¾ç½®ä¸º 10 æ„å‘³ç€å°†ä½¿ç”¨ 10 ä¸ªå¹¶è¡Œè¿›ç¨‹æ¥å¤„ç†æ•°æ®é›†ã€‚\n",
    "tokenized_datasets = dataset.map(tokenize, fn_kwargs=dict(tokenizer=tokenizer), batched=True, num_proc=10)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Use a subset for quick training\n",
    "# å¦‚æœUSE_SUBSETä¸ºTrueï¼Œåˆ™ä¼šä½¿ç”¨æ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†ã€‚è¿™é€šå¸¸ç”¨äºå¿«é€Ÿæµ‹è¯•æˆ–è°ƒè¯•ã€‚\n",
    "if USE_SUBSET:\n",
    "    # shuffle(seed=42): ä½¿ç”¨å›ºå®šçš„éšæœºç§å­42å¯¹æ•°æ®é›†è¿›è¡Œéšæœºæ‰“ä¹±ã€‚select(range(1000)): ä»æ‰“ä¹±åçš„æ•°æ®é›†ä¸­é€‰æ‹©å‰1000ä¸ªæ ·æœ¬\n",
    "    train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "    val_dataset = val_dataset.shuffle(seed=42).select(range(100))\n",
    "    test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
    "# è¿™ä¸¤è¡Œä»£ç ç”¨äºæ‰“å°è®­ç»ƒé›†çš„ç‰¹å¾å’Œç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œä»¥ä¾¿æ£€æŸ¥æ•°æ®å¤„ç†çš„ç»“æœã€‚\n",
    "# Print the features\n",
    "print(train_dataset.features)\n",
    "# Print the first example\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Feed the first training example through the LM.\n",
    ">Note: The model takes the token ids with the argument `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Call model on the first example\n",
    "# train_dataset[0]: è·å–è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚[\"input_ids\"]: æå–è¯¥æ ·æœ¬çš„è¾“å…¥IDåºåˆ—ã€‚torch.tensor(): å°†è¾“å…¥IDè½¬æ¢ä¸ºPyTorchå¼ é‡ã€‚\n",
    "#.unsqueeze(0): åœ¨ç¬¬0ç»´æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œå°†å½¢çŠ¶ä» [sequence_length] å˜ä¸º [1, sequence_length]ï¼Œè¿™æ˜¯å› ä¸ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥æ˜¯ä¸€ä¸ªæ‰¹æ¬¡ã€‚\n",
    "# output = roberta(input_ids=...)å°†å¤„ç†åçš„è¾“å…¥ä¼ é€’ç»™RoBERTaæ¨¡å‹ã€‚æ¨¡å‹è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ªè¾“å‡ºçš„å¯¹è±¡ã€‚\n",
    "output = roberta(input_ids=torch.tensor(train_dataset[0][\"input_ids\"]).unsqueeze(0))\n",
    "'''last_hidden_state: è¿™æ˜¯æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ã€‚\n",
    "æ‰“å°å…¶å½¢çŠ¶ï¼Œé€šå¸¸ä¸º [batch_size, sequence_length, hidden_size]ã€‚\n",
    "batch_size åœ¨è¿™é‡Œæ˜¯1ã€‚\n",
    "sequence_length æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚\n",
    "hidden_size æ˜¯æ¨¡å‹çš„éšè—å±‚å¤§å°ï¼ˆå¯¹äºRoBERTa-baseé€šå¸¸æ˜¯768ï¼‰ã€‚'''\n",
    "print(output.last_hidden_state.shape)\n",
    "'''pooler_output: è¿™æ˜¯åºåˆ—çš„æ± åŒ–è¡¨ç¤ºï¼Œé€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚\n",
    "æ‰“å°å…¶å½¢çŠ¶ï¼Œé€šå¸¸ä¸º [batch_size, hidden_size]ã€‚\n",
    "batch_size åœ¨è¿™é‡Œæ˜¯1ã€‚\n",
    "hidden_size ä¸ä¸Šé¢ç›¸åŒã€‚'''\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output is a contextualized representation of the input, and therefor can be used as such in your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¿™ä¸ªæ¨¡å‹çš„å·¥ä½œæµç¨‹æ˜¯ï¼š\\nè¾“å…¥æ–‡æœ¬é€šè¿‡é¢„è®­ç»ƒçš„transformeræ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰è¿›è¡Œå¤„ç†ã€‚\\nä½¿ç”¨transformeræ¨¡å‹çš„æ± åŒ–è¾“å‡ºä½œä¸ºæ–‡æœ¬çš„è¡¨ç¤ºã€‚\\nå°†è¿™ä¸ªè¡¨ç¤ºä¼ å…¥ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨ã€‚\\nåˆ†ç±»å™¨è¾“å‡ºä¸¤ä¸ªæ•°å€¼ï¼Œå¯¹åº”ä¸¤ç§æƒ…æ„Ÿç±»åˆ«çš„å¾—åˆ†ã€‚\\nè¿™ç§è®¾è®¡å…è®¸åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡å¾®è°ƒæˆ–æ·»åŠ ä»»åŠ¡ç‰¹å®šå±‚æ¥é€‚åº”æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚\\nfreeze_embedderé€‰é¡¹æä¾›äº†çµæ´»æ€§ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œè¿™åœ¨ä¸åŒçš„åº”ç”¨åœºæ™¯ä¸­å¯èƒ½ä¼šæœ‰ä¸åŒçš„æ•ˆæœã€‚'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentAnalysisModel(torch.nn.Module):\n",
    "    # base_modelï¼šé¢„è®­ç»ƒçš„transformeræ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰\n",
    "    # freeze_embedderï¼šå¸ƒå°”å€¼ï¼Œå†³å®šæ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹çš„å‚æ•°ã€‚\n",
    "    def __init__(self, base_model, freeze_embedder=False):\n",
    "        super().__init__()\n",
    "        # We are using the transformer model as base_model\n",
    "        self.base_model = base_model\n",
    "        # Freeze the base_model\n",
    "        if freeze_embedder:\n",
    "            # self.base_model.parameters() è¿”å›åŸºç¡€æ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰çš„æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ã€‚è¿™ä¸ªå¾ªç¯éå†æ¯ä¸€ä¸ªå‚æ•°ã€‚\n",
    "            for param in self.base_model.parameters():\n",
    "                # å½“ä¸€ä¸ªå‚æ•°çš„ requires_grad ä¸º False æ—¶ï¼ŒPyTorch ä¸ä¼šä¸ºè¯¥å‚æ•°è®¡ç®—æ¢¯åº¦ï¼Œä¹Ÿä¸ä¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ›´æ–°è¿™ä¸ªå‚æ•°\n",
    "                param.requires_grad = False\n",
    "        # We add a linear layer on top of the base_model\n",
    "        # æ·»åŠ ä¸€ä¸ªçº¿æ€§å±‚ä½œä¸ºåˆ†ç±»å™¨ã€‚è¾“å…¥ç»´åº¦æ˜¯åŸºç¡€æ¨¡å‹çš„éšè—å±‚å¤§å°ï¼Œè¾“å‡ºç»´åº¦æ˜¯2ï¼ˆå¯¹åº”ä¸¤ä¸ªæƒ…æ„Ÿç±»åˆ«ï¼‰\n",
    "        '''é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰æ˜¯ä¸ºé€šç”¨è¯­è¨€ç†è§£ä»»åŠ¡è®¾è®¡çš„ï¼Œè€Œä¸æ˜¯ä¸“é—¨ä¸ºæƒ…æ„Ÿåˆ†æè®¾è®¡ã€‚\n",
    "åˆ†ç±»å™¨å±‚å°†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºè½¬åŒ–ä¸ºç‰¹å®šä»»åŠ¡ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰çš„è¾“å‡ºã€‚\n",
    "åŸºç¡€æ¨¡å‹çš„è¾“å‡ºé€šå¸¸æ˜¯é«˜ç»´çš„ï¼ˆä¾‹å¦‚ï¼Œ768ç»´ï¼‰ã€‚\n",
    "æƒ…æ„Ÿåˆ†æä»»åŠ¡éœ€è¦çš„æ˜¯ä½ç»´è¾“å‡ºï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯2ç»´ï¼Œå¯¹åº”ä¸¤ç§æƒ…æ„Ÿï¼‰ã€‚\n",
    "åˆ†ç±»å™¨å®ç°äº†ä»é«˜ç»´åˆ°ä½ç»´çš„æ˜ å°„ã€‚'''\n",
    "        self.classifier = torch.nn.Linear(base_model.config.hidden_size, 2)\n",
    "        # Initialize the classifier weights\n",
    "        # ä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–åˆ†ç±»å™¨çš„æƒé‡ã€‚\n",
    "        # å°†åˆ†ç±»å™¨çš„åç½®åˆå§‹åŒ–ä¸ºé›¶ã€‚\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight) # Xavier uniform initialization\n",
    "        torch.nn.init.zeros_(self.classifier.bias) # Initialize bias with zeros\n",
    "\n",
    "    def forward(self, **model_inputs):# **ç”¨äºæ¥æ”¶ä»»æ„æ•°é‡çš„å…³é”®å­—å‚æ•°å’Œè§£åŒ…å­—å…¸\n",
    "        '''**model_inputså…è®¸ä¼ å…¥ä»»æ„å…³é”®å­—å‚æ•°ï¼Œmodel_inputs é€šå¸¸æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«äº†æ¨¡å‹æ‰€éœ€çš„å„ç§è¾“å…¥ã€‚\n",
    "é¦–å…ˆé€šè¿‡åŸºç¡€æ¨¡å‹å¤„ç†è¾“å…¥ã€‚\n",
    "ç„¶åä½¿ç”¨åŸºç¡€æ¨¡å‹çš„pooler_outputä½œä¸ºå¥å­è¡¨ç¤ºï¼Œä¼ å…¥åˆ†ç±»å™¨ã€‚\n",
    "è¿”å›åˆ†ç±»å™¨çš„è¾“å‡ºã€‚'''\n",
    "        # model_inputs is a dict\n",
    "        # Pass the inputs to the model to produce an embedding\n",
    "        base_model_output = self.base_model(**model_inputs)\n",
    "        # Pass the embedding through the classifier\n",
    "        # here we use the pooler_output as the representation of the sentence (depends on the model)\n",
    "        output = self.classifier(base_model_output.pooler_output)\n",
    "        return output\n",
    "'''è¿™ä¸ªæ¨¡å‹çš„å·¥ä½œæµç¨‹æ˜¯ï¼š\n",
    "è¾“å…¥æ–‡æœ¬é€šè¿‡é¢„è®­ç»ƒçš„transformeræ¨¡å‹ï¼ˆå¦‚RoBERTaï¼‰è¿›è¡Œå¤„ç†ã€‚\n",
    "ä½¿ç”¨transformeræ¨¡å‹çš„æ± åŒ–è¾“å‡ºä½œä¸ºæ–‡æœ¬çš„è¡¨ç¤ºã€‚\n",
    "å°†è¿™ä¸ªè¡¨ç¤ºä¼ å…¥ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨ã€‚\n",
    "åˆ†ç±»å™¨è¾“å‡ºä¸¤ä¸ªæ•°å€¼ï¼Œå¯¹åº”ä¸¤ç§æƒ…æ„Ÿç±»åˆ«çš„å¾—åˆ†ã€‚\n",
    "è¿™ç§è®¾è®¡å…è®¸åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡å¾®è°ƒæˆ–æ·»åŠ ä»»åŠ¡ç‰¹å®šå±‚æ¥é€‚åº”æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚\n",
    "freeze_embedderé€‰é¡¹æä¾›äº†çµæ´»æ€§ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œè¿™åœ¨ä¸åŒçš„åº”ç”¨åœºæ™¯ä¸­å¯èƒ½ä¼šæœ‰ä¸åŒçš„æ•ˆæœã€‚'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¿™æ®µä»£ç çš„ç›®çš„å’Œä½œç”¨ï¼š\\nè®¾å¤‡ä¼˜åŒ–ï¼šé€šè¿‡é€‰æ‹©åˆé€‚çš„è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè¿è¡Œã€‚\\næ¨¡å‹åˆå§‹åŒ–ï¼šåˆ›å»ºå¹¶é…ç½®æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œä¸ºåç»­çš„è®­ç»ƒåšå‡†å¤‡ã€‚\\nçµæ´»æ€§ï¼šé€šè¿‡ freeze_embedder å‚æ•°ï¼Œæä¾›äº†æ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹çš„é€‰é¡¹ï¼Œå…è®¸åœ¨ä¸åŒåœºæ™¯ä¸‹è¿›è¡Œå®éªŒã€‚\\nåˆæ­¥æµ‹è¯•ï¼šé€šè¿‡è¿è¡Œä¸€ä¸ªæ ·æœ¬ï¼ŒéªŒè¯æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†å’Œå‰å‘ä¼ æ’­ã€‚\\nè°ƒè¯•å‡†å¤‡ï¼šè¿™ç§è®¾ç½®æœ‰åŠ©äºå¿«é€Ÿå‘ç°ä»»ä½•åˆå§‹åŒ–æˆ–å…¼å®¹æ€§é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# åˆ›å»º SentimentAnalysisModel çš„å®ä¾‹ã€‚\n",
    "# roberta æ˜¯é¢„å…ˆåŠ è½½çš„RoBERTaæ¨¡å‹ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹ä¼ å…¥ã€‚\n",
    "# .to(device) å°†æ¨¡å‹ç§»åŠ¨åˆ°ä¹‹å‰å®šä¹‰çš„è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰ä¸Šã€‚\n",
    "model = SentimentAnalysisModel(roberta, freeze_embedder=False).to(device) # Set to True to freeze the embedder for faster training\n",
    "# è¿™è¡Œä»£ç å¯¹æ¨¡å‹è¿›è¡Œäº†ä¸€ä¸ªç®€å•çš„æµ‹è¯•ã€‚\n",
    "# train_dataset[0][\"input_ids\"] è·å–è®­ç»ƒé›†ä¸­ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥IDã€‚\n",
    "# torch.tensor([...]) å°†è¾“å…¥IDè½¬æ¢ä¸ºPyTorchå¼ é‡ã€‚\n",
    "# .device=device ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆä¸æ¨¡å‹ç›¸åŒï¼‰ã€‚\n",
    "# æ•´ä¸ªè¡¨è¾¾å¼å°†ä¸€ä¸ªæ ·æœ¬è¾“å…¥æ¨¡å‹ï¼Œæµ‹è¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€‚\n",
    "model(input_ids=torch.tensor([train_dataset[0][\"input_ids\"]], device=device))\n",
    "\n",
    "'''è¿™æ®µä»£ç çš„ç›®çš„å’Œä½œç”¨ï¼š\n",
    "è®¾å¤‡ä¼˜åŒ–ï¼šé€šè¿‡é€‰æ‹©åˆé€‚çš„è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè¿è¡Œã€‚\n",
    "æ¨¡å‹åˆå§‹åŒ–ï¼šåˆ›å»ºå¹¶é…ç½®æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œä¸ºåç»­çš„è®­ç»ƒåšå‡†å¤‡ã€‚\n",
    "çµæ´»æ€§ï¼šé€šè¿‡ freeze_embedder å‚æ•°ï¼Œæä¾›äº†æ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹çš„é€‰é¡¹ï¼Œå…è®¸åœ¨ä¸åŒåœºæ™¯ä¸‹è¿›è¡Œå®éªŒã€‚\n",
    "åˆæ­¥æµ‹è¯•ï¼šé€šè¿‡è¿è¡Œä¸€ä¸ªæ ·æœ¬ï¼ŒéªŒè¯æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†å’Œå‰å‘ä¼ æ’­ã€‚\n",
    "è°ƒè¯•å‡†å¤‡ï¼šè¿™ç§è®¾ç½®æœ‰åŠ©äºå¿«é€Ÿå‘ç°ä»»ä½•åˆå§‹åŒ–æˆ–å…¼å®¹æ€§é—®é¢˜ã€‚'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only passed the token ids to the model.\\\n",
    "However, there are more inputs for transformer-based models which you need to be aware of:\n",
    "\n",
    "1. **input_ids**: Token IDs to be fed to the model.\n",
    "2. **attention_mask**: Mask to avoid performing attention on padding token indices.\n",
    "3. **token_type_ids**: Segment token indices to indicate different portions of the inputs (used in models like BERT for tasks like question answering).\n",
    "4. **position_ids**: Indices of positions of each input sequence token in the position embeddings.\n",
    "\n",
    "If you don't pass them into the model, the library will take care of it!\\\n",
    "However, if you perform padding during batching, the model might not know where padding was applied and therefore you will also have to provide the `attention_mask`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the processed data and the model in place, we create the appropriate dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   627,  5581,     9,   209,    80, 17205,  2415,  2156,     8,\n",
      "           190,     5,   498,    11,    61,    51,  3033,  1437,     2],\n",
      "        [    0, 23702,  1461,  1437,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'labels': tensor([1, 0])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# features æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†å¤šä¸ªæ ·æœ¬ï¼ˆæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå­—å…¸ï¼‰\n",
    "def collate_fn(features):\n",
    "    # We need to pad the input to make sure all sentences have the same length\n",
    "    # å°†æ¯ä¸ªæ ·æœ¬çš„ input_ids è½¬æ¢ä¸ºå¼ é‡ã€‚\n",
    "    # ä½¿ç”¨ pad_sequence å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ã€‚\n",
    "    # batch_first=True ç¡®ä¿è¾“å‡ºçš„å½¢çŠ¶æ˜¯ (batch_size, sequence_length)ã€‚\n",
    "    # padding_value ä½¿ç”¨tokenizerçš„pad_token_idã€‚pad_token_idåœ¨æŸäº›æ¨¡å‹ä¸­ï¼Œå®ƒå¯èƒ½æ˜¯0æˆ–1ï¼Œè€Œåœ¨å…¶ä»–æ¨¡å‹ä¸­å¯èƒ½æ˜¯ä¸åŒçš„å€¼ã€‚\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(f[\"input_ids\"]) for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    if \"attention_mask\" in features[0]:\n",
    "        # If the features contain attention_mask, we should pad them as well\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(f[\"attention_mask\"]) for f in features],\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "    else:\n",
    "        '''input_ids != tokenizer.pad_token_id:\n",
    "è¿™ä¸ªæ“ä½œä¼šæ¯”è¾ƒ input_ids å¼ é‡ä¸­çš„æ¯ä¸ªå…ƒç´ ä¸ tokenizer.pad_token_id çš„å€¼ã€‚\n",
    "ç»“æœæ˜¯ä¸€ä¸ªå¸ƒå°”å¼ é‡,.int() è½¬æ¢:å°†å¸ƒå°”å¼ é‡è½¬æ¢ä¸ºæ•´æ•°å¼ é‡ã€‚True å˜ä¸º 1ï¼ŒFalse å˜ä¸º 0ã€‚\n",
    "å¾—åˆ°çš„ attention_mask æ˜¯ä¸€ä¸ªä¸ input_ids å½¢çŠ¶ç›¸åŒçš„å¼ é‡ã€‚\n",
    "å€¼ä¸º 1 çš„ä½ç½®è¡¨ç¤ºå®é™…çš„ tokenï¼ˆæ¨¡å‹åº”è¯¥å…³æ³¨çš„éƒ¨åˆ†ï¼‰ã€‚\n",
    "å€¼ä¸º 0 çš„ä½ç½®è¡¨ç¤ºå¡«å…… tokenï¼ˆæ¨¡å‹åº”è¯¥å¿½ç•¥çš„éƒ¨åˆ†ï¼‰ã€‚\n",
    "æŒ‡å¯¼æ¨¡å‹æ³¨æ„åŠ›:\n",
    "å‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®åŒ…å«å®é™…çš„è¾“å…¥ä¿¡æ¯ï¼Œå“ªäº›ä½ç½®æ˜¯å¡«å……ã€‚\n",
    "æ¨¡å‹åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶ä¼šä½¿ç”¨è¿™ä¸ªæ©ç ï¼Œç¡®ä¿ä¸ä¼šå…³æ³¨åˆ°å¡«å……çš„éƒ¨åˆ†ã€‚'''\n",
    "        # We need to create an attention mask from input_ids\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).int()\n",
    "    labels = torch.tensor([f[\"label\"] for f in features])\n",
    "    # å°†å¤„ç†åçš„æ•°æ®ç»„ç»‡æˆä¸€ä¸ªå­—å…¸ã€‚\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# DataLoader æ˜¯ PyTorch ä¸­ç”¨äºåŠ è½½æ•°æ®çš„å·¥å…·\n",
    "#batch_size: è®­ç»ƒé›†ä¸º2ï¼ŒéªŒè¯é›†ä¸º8ã€‚è¿™å†³å®šäº†æ¯æ¬¡è¿­ä»£å¤„ç†çš„æ ·æœ¬æ•°ã€‚\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing the collate function ourselbes, the transformers library provides them for different applications as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   560,  7798,  ...,     1,     1,     1],\n",
      "        [    0,  1090,   991,  ...,     1,     1,     1],\n",
      "        [    0,  1116,   130,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    18,    10,  ...,     1,     1,     1],\n",
      "        [    0, 12473,    65,  ...,     1,     1,     1],\n",
      "        [    0,   354,  8045,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# with_format æ–¹æ³•ç”¨äºæŒ‡å®šæ•°æ®é›†åº”è¯¥è¿”å›çš„åˆ—\n",
    "# DataCollatorWithPadding(tokenizer, padding=\"longest\")æ˜¯ Hugging Face Transformers åº“æä¾›çš„ä¸€ä¸ªå·¥å…·ï¼Œä¸“é—¨ç”¨äºå¤„ç†æ–‡æœ¬æ•°æ®çš„æ‰¹å¤„ç†\n",
    "# DataCollatorWithPadding æ˜¯é’ˆå¯¹ transformer æ¨¡å‹ä¼˜åŒ–çš„ï¼Œå¯èƒ½æ¯”è‡ªå®šä¹‰å‡½æ•°æ›´é«˜æ•ˆ;è€Œä¸”ä¸ç”¨è‡ªå·±å†™collate_fn\n",
    "# å®ƒä¸»è¦è´Ÿè´£å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿èƒ½å¤Ÿç»„æˆä¸€ä¸ªæ‰¹æ¬¡\n",
    "#ã€€tokenizeræ˜¯æ‚¨ä½¿ç”¨çš„ tokenizer å¯¹è±¡ï¼Œé€šå¸¸æ˜¯ä¸æ‚¨çš„æ¨¡å‹ç›¸å¯¹åº”çš„ tokenizer\n",
    "# padding=\"longest\":æŒ‡å®šäº†å¡«å……ç­–ç•¥ã€‚\"longest\" è¡¨ç¤ºåœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­ï¼Œæ‰€æœ‰åºåˆ—éƒ½ä¼šè¢«å¡«å……åˆ°è¯¥æ‰¹æ¬¡ä¸­æœ€é•¿åºåˆ—çš„é•¿åº¦\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   405,   128,  ...,     1,     1,     1],\n",
      "        [    0,   879,  4825,  ...,     1,     1,     1],\n",
      "        [    0, 37984,   201,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   102,  1569,  ...,     1,     1,     1],\n",
      "        [    0, 17075, 21871,  ...,     1,     1,     1],\n",
      "        [    0,   179,     5,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write an evaluation function that computes the accuracy of all samples in a dataloader using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, device=None):\n",
    "    # åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯ä»¥èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿè®¡ç®—\n",
    "    with torch.no_grad():\n",
    "        # model.eval() æ˜¯ PyTorch ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹æ³•è°ƒç”¨ï¼Œç”¨äºå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "        # dataloader: è¿™æ˜¯è¦è¿­ä»£çš„å¯¹è±¡ï¼Œåœ¨è¿™é‡Œæ˜¯ PyTorch çš„ DataLoader å®ä¾‹ã€‚\n",
    "        # desc=\"Evaluation\": è®¾ç½®è¿›åº¦æ¡çš„æè¿°æ–‡æœ¬ï¼Œè¿™é‡Œæ˜¾ç¤ºä¸º \"Evaluation\"ã€‚\n",
    "        # leave=False: è¿›åº¦æ¡å®Œæˆåæ˜¯å¦ä¿ç•™ã€‚è®¾ç½®ä¸º False æ„å‘³ç€è¿›åº¦æ¡ä¼šåœ¨å®Œæˆåæ¶ˆå¤±\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # ä½¿ç”¨æ¨¡å‹å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­ã€‚\n",
    "            # input_ids: è¾“å…¥åºåˆ—çš„token IDã€‚\n",
    "            # attention_mask: æŒ‡ç¤ºå“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆè¾“å…¥ï¼ˆéå¡«å……ï¼‰ã€‚\n",
    "            # outputs: æ¨¡å‹çš„è¾“å‡ºï¼Œé€šå¸¸æ˜¯æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹åˆ†æ•°ï¼ˆlogitsï¼‰\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # torch.argmax: è¿”å›æŒ‡å®šç»´åº¦ä¸Šæœ€å¤§å€¼çš„ç´¢å¼•ã€‚\n",
    "            # dim=1: åœ¨ç¬¬äºŒä¸ªç»´åº¦ï¼ˆç±»åˆ«ç»´åº¦ï¼‰ä¸Šå–æœ€å¤§å€¼ã€‚\n",
    "            # predicted: åŒ…å«æ¯ä¸ªæ ·æœ¬é¢„æµ‹ç±»åˆ«çš„å¼ é‡ã€‚\n",
    "            #ã€€ä¾‹å¦‚predicted = tensor([1, 1, 0, 1])ï¼Œpredicted å¼ é‡åŒ…å«äº†æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«ï¼š1 è¡¨ç¤ºæ¨¡å‹é¢„æµ‹è¯¥è¯„è®ºä¸ºæ­£é¢ï¼›0 è¡¨ç¤ºæ¨¡å‹é¢„æµ‹è¯¥è¯„è®ºä¸ºè´Ÿé¢\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # tensor.size(0) ç‰¹æŒ‡è¿”å›å¼ é‡ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆé€šå¸¸æ˜¯æ‰¹æ¬¡å¤§å°ï¼‰çš„å¤§å°ã€‚\n",
    "            # åœ¨å¤§å¤šæ•° PyTorch æ•°æ®åŠ è½½å™¨ä¸­ï¼Œç¬¬ä¸€ä¸ªç»´åº¦é€šå¸¸è¡¨ç¤ºæ‰¹æ¬¡å¤§å°ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œlabels é€šå¸¸æ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œå…¶é•¿åº¦ç­‰äºæ‰¹æ¬¡å¤§å°ã€‚\n",
    "            total += labels.size(0)\n",
    "            #ã€€.item() æ–¹æ³•:å°†å•å…ƒç´ å¼ é‡è½¬æ¢ä¸º Python æ ‡é‡ã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬è¦å°†ç»“æœåŠ åˆ° Python çš„æ•´æ•°å˜é‡ correct ä¸Š\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f577deee48f042a5bca0879411a40655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy on validation data: 0.4690366972477064\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation accuracy on validation data: {evaluate(model, val_dataloader, device=device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a training loop that is just the same as before.\\\n",
    "We have to make sure to feed in the correct arguments to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1155edae6b48758d6f951ca81fe344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d2c61a3b8149f8b31aac4feeb3e376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6315 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd5f5b59464d07a0911ac17b8cec5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a3c22b15646d1a9c7ca4366175bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5376c47bffba4c3f8ecd244eeb8ab3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œè¿™æ˜¯ Adam ä¼˜åŒ–å™¨çš„ä¸€ä¸ªå˜ä½“ï¼Œå…·æœ‰æƒé‡è¡°å‡ä¿®æ­£\n",
    "# model.parameters(): æŒ‡å®šè¦ä¼˜åŒ–çš„å‚æ•°ï¼Œè¿™é‡Œæ˜¯æ¨¡å‹çš„æ‰€æœ‰å‚æ•°\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#ã€€è¿™æ„å‘³ç€æ•´ä¸ªè®­ç»ƒæ•°æ®é›†å°†è¢«éå† 3 æ¬¡ã€‚\n",
    "num_epochs = 3\n",
    "# åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨å’Œè·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡\n",
    "metric_dict = {\"Loss\": \"-\", \"Val Acc\": evaluate(model, val_dataloader, device=device)}\n",
    "# total: è®¾ç½®ä¸ºæ€»æ‰¹æ¬¡æ•°ï¼ˆè½®æ•° * æ¯è½®æ‰¹æ¬¡æ•°ï¼‰ï¼›DataLoader è¢«è®¾è®¡ä¸ºä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¬¡è¿­ä»£è¿”å›ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼Œæ‰€ä»¥len(train_dataloader)æ˜¯è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸­çš„æ‰¹æ¬¡æ•°é‡\n",
    "# unit: è¿›åº¦å•ä½è®¾ä¸º \"batch\"\n",
    "with tqdm(\n",
    "    total=num_epochs * len(train_dataloader), desc=\"Training\", unit=\"batch\"\n",
    ") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦\n",
    "            optimizer.zero_grad()\n",
    "            # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦\n",
    "            loss.backward()\n",
    "            # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "            optimizer.step()\n",
    "            #ã€€æ›´æ–°æŸå¤±å€¼\n",
    "            # loss.item() ç”¨äºå°†å•ä¸ªå…ƒç´ çš„ PyTorch å¼ é‡è½¬æ¢ä¸º Python æ ‡é‡\n",
    "            metric_dict[\"Loss\"] = loss.item()\n",
    "            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºçš„æŒ‡æ ‡\n",
    "            pbar.set_postfix(metric_dict)\n",
    "            # æ¨è¿›è¿›åº¦æ¡\n",
    "            pbar.update(1)\n",
    "        metric_dict[\"Val Acc\"] = evaluate(model, val_dataloader, device=device)\n",
    "        pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the transformers library makes it simple to use LMs as it includes task-specific models for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•åŠ è½½æ¨¡å‹å’Œç›¸åº”çš„åˆ†è¯å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬è¿›è¡Œç®€å•çš„é¢„æµ‹ã€‚\\næ€»ä¹‹ï¼Œè¿™è¡Œä»£ç æ˜¯ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å…¸å‹èµ·ç‚¹ï¼Œ\\nå®ƒåˆ©ç”¨äº†è¿ç§»å­¦ä¹ çš„å¼ºå¤§åŠŸèƒ½ï¼Œä¸ºç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€ã€‚'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RobertaForSequenceClassification model can be used for text classification tasks like sentiment analysis\n",
    "# It has a sequence classification head, that is a linear layer on top of the RoBERTa model that outputs a classification label\n",
    "# RobertaForSequenceClassificationæ˜¯ Transformers åº“ä¸­çš„ä¸€ä¸ªç±»ï¼Œä¸“é—¨ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„ RoBERTa æ¨¡å‹\n",
    "# å®ƒåœ¨ RoBERTa åŸºç¡€æ¨¡å‹ä¹‹ä¸Šæ·»åŠ äº†ä¸€ä¸ªåˆ†ç±»å¤´ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼‰\n",
    "# .from_pretrained():æ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡ã€‚å®ƒä¸ä»…åŠ è½½æƒé‡ï¼Œè¿˜é…ç½®æ¨¡å‹æ¶æ„ä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "#ã€€roberta-baseæ˜¯è¦åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºè¯†åˆ«å’ŒåŠ è½½ç‰¹å®šçš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "# num_labels=2 æŒ‡å®šåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°é‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè®¾ç½®ä¸º 2 è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ã€‚\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "'''è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•åŠ è½½æ¨¡å‹å’Œç›¸åº”çš„åˆ†è¯å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬è¿›è¡Œç®€å•çš„é¢„æµ‹ã€‚\n",
    "æ€»ä¹‹ï¼Œè¿™è¡Œä»£ç æ˜¯ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å…¸å‹èµ·ç‚¹ï¼Œ\n",
    "å®ƒåˆ©ç”¨äº†è¿ç§»å­¦ä¹ çš„å¼ºå¤§åŠŸèƒ½ï¼Œä¸ºç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€ã€‚'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use their Trainer API so that you don't have to implement the training loop again and again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='None' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      None\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694606</td>\n",
       "      <td>0.490826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.98 GiB is allocated by PyTorch, and 376.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\trainer.py:2584\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2580\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2584\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2588\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\accelerate\\optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\optimization.py:637\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    635\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p)\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m exp_avg, exp_avg_sq \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    640\u001b[0m beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.98 GiB is allocated by PyTorch, and 376.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Define a function to compute the metrics\n",
    "# è¿™æ˜¯ä¸€ä¸ªåä¸º compute_metrics çš„å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®ç‡\n",
    "def compute_metrics(pred_and_label: EvalPrediction):\n",
    "    return {\n",
    "        # predictions.argmax(axis=-1)è·å–é¢„æµ‹ç»“æœä¸­æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•;argmax åœ¨æœ€åä¸€ä¸ªè½´ä¸Šæ“ä½œï¼Œé€šå¸¸å¯¹åº”äºç±»åˆ«ç»´åº¦\n",
    "        # == pred_and_label.label_idså°†é¢„æµ‹çš„ç±»åˆ«ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ;ç”Ÿæˆä¸€ä¸ªå¸ƒå°”æ•°ç»„ï¼ŒTrue è¡¨ç¤ºé¢„æµ‹æ­£ç¡®ï¼ŒFalse è¡¨ç¤ºé¢„æµ‹é”™è¯¯\n",
    "        # .mean()è®¡ç®—å¸ƒå°”æ•°ç»„çš„å¹³å‡å€¼ï¼Œå³æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹;.item()å°†ç»“æœä» PyTorch å¼ é‡è½¬æ¢ä¸º Python æ ‡é‡\n",
    "        \"accuracy\": (pred_and_label.predictions.argmax(axis=-1) == pred_and_label.label_ids)\n",
    "        .mean()\n",
    "        .item()\n",
    "    }\n",
    "# è¿”å›å€¼ï¼š å‡½æ•°è¿”å›ä¸€ä¸ªå­—å…¸ {\"accuracy\": è®¡ç®—å¾—åˆ°çš„å‡†ç¡®ç‡}\n",
    "\n",
    "# Define the training arguments\n",
    "# TrainingArguments å¯¹è±¡ç”¨äºé…ç½®æ¨¡å‹è®­ç»ƒçš„å„ç§å‚æ•°;æ˜¯ Hugging Face Transformers åº“ä¸­çš„ä¸€ä¸ªç±»ï¼Œç”¨äºè®¾ç½®è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§é€‰é¡¹\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # æŒ‡å®šè®­ç»ƒè¾“å‡ºï¼ˆå¦‚æ£€æŸ¥ç‚¹ã€æ—¥å¿—ç­‰ï¼‰çš„ä¿å­˜ç›®å½•\n",
    "    output_dir=\"./results\",\n",
    "    # è®¾ç½®è¯„ä¼°ç­–ç•¥ä¸ºæ¯ä¸ª epoch ç»“æŸåè¿›è¡Œè¯„ä¼°ã€‚\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # è®¾ç½®æ—¥å¿—è®°å½•ç­–ç•¥ä¸ºæ¯ä¸ª epoch ç»“æŸåè®°å½•ã€‚\n",
    "    logging_strategy=\"epoch\",\n",
    "    # ä½¿ç”¨ Hugging Face å®ç°çš„ AdamW ä¼˜åŒ–å™¨ã€‚\n",
    "    optim=\"adamw_hf\",\n",
    "    # è®¾ç½®å­¦ä¹ ç‡ä¸º 5e-5ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¾®è°ƒå­¦ä¹ ç‡ã€‚\n",
    "    learning_rate=5e-5,\n",
    "    # æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ä¸º 32ã€‚\n",
    "    per_device_train_batch_size=32,\n",
    "    # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°ä¸º 128ã€‚è¯„ä¼°æ—¶é€šå¸¸å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡ã€‚\n",
    "    per_device_eval_batch_size=128,\n",
    "    # è®¾ç½®è®­ç»ƒè½®æ•°ä¸º 3 ä¸ª epoch\n",
    "    num_train_epochs=3,\n",
    "    # è®¾ç½®æƒé‡è¡°å‡ä¸º 0.01ï¼Œç”¨äºæ­£åˆ™åŒ–ã€‚\n",
    "    weight_decay=0.01,\n",
    "    # ä¸ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒï¼Œé»˜è®¤ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "    use_cpu=False,\n",
    "    # åœ¨è®­ç»ƒå¼€å§‹å‰è¿›è¡Œä¸€æ¬¡è¯„ä¼°ã€‚\n",
    "    eval_on_start=True,\n",
    "    # ä¸ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥èŠ‚çœç£ç›˜ç©ºé—´ã€‚\n",
    "    save_strategy=\"no\",  # We will not save the model for now to save disk space\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "# è¿™æ®µä»£ç ä½¿ç”¨ Hugging Face çš„ Trainer ç±»æ¥è®¾ç½®ã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚\n",
    "trainer = Trainer(\n",
    "    # ä¹‹å‰å®šä¹‰çš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ RoBERTa æˆ–å…¶ä»– Transformer æ¨¡å‹ï¼‰ã€‚\n",
    "    model=model,\n",
    "    # args: ä¹‹å‰å®šä¹‰çš„ TrainingArguments å¯¹è±¡ã€‚\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # processing_class: è®¾ç½®ä¸º tokenizerï¼Œç”¨äºå¤„ç†è¾“å…¥æ•°æ®ï¼ˆå¦‚å¡«å……æ‰¹æ¬¡ï¼‰\n",
    "    processing_class=tokenizer,  # enables padding of batches\n",
    "    # ä¹‹å‰å®šä¹‰çš„è®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# è¿™è¡Œä»£ç å¯åŠ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚Trainer ä¼šæ ¹æ®ä¹‹å‰è®¾ç½®çš„å‚æ•°è¿›è¡Œè®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "# è¿™è¡Œä»£ç åœ¨è®­ç»ƒç»“æŸåå¯¹æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°ã€‚\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Language Models with Prompting\n",
    "\n",
    "Text generation language models, such as GPT-3, can be used with prompting to perform various natural language processing tasks, including sentiment analysis.\\\n",
    "Prompting involves providing the model with a specific input or \"prompt\" that guides it to generate the desired output.\\\n",
    "This technique leverages the model's pre-trained knowledge to perform tasks without requiring additional fine-tuning.\n",
    "\n",
    "#### Applying Prompting to Sentiment Analysis\n",
    "\n",
    "To use a text generation model for sentiment analysis, you can craft a prompt that instructs the model to classify the sentiment of a given text. The prompt should be designed to elicit a response that indicates whether the sentiment is positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformers library makes it simple to load a text generation model.\n",
    ">Note that they can be quite large and potentially do not fit or run slowly on your CPU/RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "# è¿™æ˜¯ Hugging Face Transformers åº“ä¸­çš„ä¸€ä¸ªé«˜çº§ APIï¼Œç”¨äºå¿«é€Ÿè®¾ç½®å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå„ç§ NLP ä»»åŠ¡\n",
    "pipe = pipeline(\n",
    "    # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆ,è¿™æ„å‘³ç€æ¨¡å‹å°†è¢«ç”¨äºç”Ÿæˆæ–‡æœ¬ï¼Œè€Œä¸æ˜¯åˆ†ç±»æˆ–å…¶ä»–ä»»åŠ¡\n",
    "    \"text-generation\",\n",
    "    # æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹;è¿™æ˜¯å¾®è½¯å¼€å‘çš„ Phi-3.5-mini-instruct æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°ä½†åŠŸèƒ½å¼ºå¤§çš„æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=device,  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    # è®¾ç½®æ¨¡å‹ä½¿ç”¨çš„æ•°æ®ç±»å‹ä¸º bfloat16\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format for Chat-Based Decoder Models\n",
    "\n",
    "Chat-based decoder models, such as GPT-3, typically require inputs in a specific format to generate coherent and contextually relevant responses.\\\n",
    "The input format generally consists of a sequence of messages, each with a role and content.\\\n",
    "The roles depend on the model, and often are \"system\", \"user\", or \"assistant\".\n",
    "\n",
    "#### Example Input Format\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **Role**: Indicates the role of the message sender. Common roles include:\n",
    "   - `system`: Provides instructions or context for the conversation.\n",
    "   - `user`: Represents the input from the user.\n",
    "   - `assistant`: Represents the response from the model.\n",
    "\n",
    "2. **Content**: The actual text of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "\n",
    "generation_args = {\n",
    "    #  é™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°ä¸º 3ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªç®€çŸ­çš„æ ‡ç­¾ã€‚\n",
    "    \"max_new_tokens\": 3,  # maximum number of tokens to generate\n",
    "    # è®¾ä¸º Falseï¼Œåªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å›å¤ã€‚\n",
    "    \"return_full_text\": False,\n",
    "    # temperature=0.0 å’Œ do_sample=Falseï¼šæœ€ç¡®å®šæ€§çš„è®¾ç½®ã€‚æ¨¡å‹æ€»æ˜¯é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ã€‚é€‚åˆéœ€è¦ä¸€è‡´å’Œç²¾ç¡®ç­”æ¡ˆçš„ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»æˆ–ç‰¹å®šä¿¡æ¯æå–ã€‚\n",
    "    # temperature>0.0 å’Œ do_sample=Trueï¼šå¼•å…¥éšæœºæ€§ï¼Œä½¿è¾“å‡ºæ›´åŠ å¤šæ ·åŒ–ã€‚æ¸©åº¦è¶Šé«˜ï¼Œè¾“å‡ºè¶Šéšæœºã€‚é€‚åˆåˆ›æ„å†™ä½œæˆ–éœ€è¦å¤šæ ·åŒ–å›ç­”çš„åœºæ™¯ã€‚\n",
    "    \"temperature\": 0.0,  # temperature for sampling (on if do_sample=True)\n",
    "    \"do_sample\": False,  # whether to sample from the output distribution\n",
    "}\n",
    "# å°†æ¶ˆæ¯å’Œç”Ÿæˆå‚æ•°ä¼ é€’ç»™ç®¡é“ã€‚ä½¿ç”¨ **generation_args å±•å¼€å‚æ•°å­—å…¸ã€‚\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output)\n",
    "'''æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é€šç”¨è¯­è¨€æ¨¡å‹æ‰§è¡Œç‰¹å®šçš„ NLP ä»»åŠ¡ï¼ˆåœ¨è¿™é‡Œæ˜¯æƒ…æ„Ÿåˆ†æï¼‰ï¼Œ\n",
    "é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œå‚æ•°è®¾ç½®æ¥å¼•å¯¼æ¨¡å‹äº§ç”Ÿæ‰€éœ€çš„è¾“å‡ºã€‚è¿™ç§æ–¹æ³•çš„çµæ´»æ€§ä½¿å¾—åŒä¸€ä¸ªæ¨¡å‹å¯ä»¥ç”¨äºå¤šç§ä¸åŒçš„ä»»åŠ¡ã€‚'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more that you can do with text generation models!\\\n",
    "For example, in-context learning (sometimes also called demonstration learning) is a technique where the model is provided with examples of the task it needs to perform within the prompt itself.\\\n",
    "This helps the model understand the task better and generate more accurate responses.\\\n",
    "For sentiment analysis, you can provide a few examples of sentences along with their sentiment labels in the prompt.\\\n",
    "The model will then use these examples to infer the sentiment of new sentences.\n",
    "\n",
    "Also, constraining the output tokens can help guiding the model to generate expected outputs and make it easier to parse the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
