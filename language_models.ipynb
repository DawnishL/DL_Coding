{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was tested with the following Python packages:\n",
    "- torch==2.5.1\n",
    "- transformers==4.48.1\n",
    "- datasets==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-style Encoder Language Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows the model to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. Thus it can be seen as a model producing a contextualized embedding that can be used as a replacement for e.g., GloVe.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Bidirectional Context**: Unlike traditional models that read text sequentially (left-to-right or right-to-left), BERT reads the entire sequence of words at once, allowing it to understand the context of a word based on both its preceding and following words.\n",
    "\n",
    "2. **Pre-training Objectives**:\n",
    "    - **Masked Language Model (MLM)**: Randomly masks some of the tokens in the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.\n",
    "    - **Next Sentence Prediction (NSP)**: Predicts whether a given pair of sentences is consecutive in the original text, helping the model understand sentence relationships.\n",
    "\n",
    "## Applications\n",
    "\n",
    "BERT can be fine-tuned for various downstream tasks, including:\n",
    "\n",
    "- **Text Classification**: Sentiment analysis, spam detection, etc.\n",
    "- **Question Answering**: Extracting answers from a given context.\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
    "- **Text Summarization**: Generating concise summaries of longer texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first install the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests->transformers[torch]) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the SST2 dataset. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with pre-trained models, you will have to download the weights from somewhere.\\\n",
    "These weights have to fit the model architecture, therefore we rely on a library for the models and its weights.\\\n",
    "We here use the Hugging Face transformers library for loading the model and its tokenizer.\n",
    "\n",
    "There are so many encoder language models; here we use RoBERTa which is an optimized (in terms of hyperparameters) version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "'''\"roberta-base\"是一个由Facebook AI研究院开发的预训练语言模型，它是BERT模型的改进版本。\n",
    "通过使用这些预训练的模型和分词器，您可以快速开始处理各种NLP任务，如文本分类、命名实体识别、问答系统等'''\n",
    "'''这段代码为后续的NLP任务做好了准备，您可以使用tokenizer来处理输入文本，\n",
    "然后将处理后的输入传递给roberta模型进行进一步的处理或微调。'''\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the model's architecture by printing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we prepare the dataset, but this time the tokenizer does most of the work converting text to token ids for the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate if you want to use a subset of the dataset for faster training\n",
    "USE_SUBSET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4950737d9b5c40228fbed978212261fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c6f642539468489f0b5dade75097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df435b4e2e64528bf91107a3422ec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value(dtype='int32', id=None), 'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}\n",
      "{'idx': 0, 'sentence': 'hide new secretions from the parental units ', 'label': 0, 'input_ids': [0, 37265, 92, 3556, 2485, 31, 5, 20536, 2833, 1437, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize(examples, tokenizer):\n",
    "    # tokenizer的主要功能是将文本转换为模型可以理解的数字序列\n",
    "    # 这是输入数据。examples是一个字典或类似字典的对象，包含了多个样本。\n",
    "    # [\"sentence\"]表示我们正在访问每个样本的\"sentence\"字段，这里面存储着需要被处理的文本。\n",
    "    # truncation=True启用了截断功能。如果输入的句子长度超过了模型的最大输入长度（对于RoBERTa通常是512个token），tokenizer会自动截断句子以适应模型。\n",
    "    # 注意力掩码（attention mask）通常用于指示哪些token是真实的输入，哪些是填充（padding）。\n",
    "    # 将此参数设置为False意味着tokenizer不会返回注意力掩码。\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, return_attention_mask=False)\n",
    "\n",
    "\n",
    "# Prepare the datasets\n",
    "# dataset.map()：map 是数据集对象的一个方法，用于对数据集中的每个样本应用一个函数。这个方法会返回一个新的数据集，其中包含了经过处理的样本。\n",
    "# tokenize：这是我们之前定义的函数，用于对输入的文本进行分词处理。它被作为第一个参数传递给 map 方法，表示要对每个样本应用这个函数。\n",
    "# fn_kwargs 参数用于指定要传递给映射函数（在这个例子中是 tokenize 函数）的额外关键字参数\n",
    "# dict(tokenizer=tokenizer)创建了一个字典，其中包含一个键值对。键是 'tokenizer'，值是之前定义的 tokenizer 对象。\n",
    "'''当 dataset.map() 调用 tokenize 函数时，它会自动将 examples 作为第一个参数传递。\n",
    "然后，它会使用 fn_kwargs 中指定的参数作为额外的关键字参数。\n",
    "因此，tokenize 函数会收到两个参数：examples（来自数据集）和 tokenizer（来自 fn_kwargs）。'''\n",
    "# 设置 batched=True 意味着 map 方法会以批处理模式运行。\n",
    "# num_proc=10设置为 10 意味着将使用 10 个并行进程来处理数据集。\n",
    "tokenized_datasets = dataset.map(tokenize, fn_kwargs=dict(tokenizer=tokenizer), batched=True, num_proc=10)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Use a subset for quick training\n",
    "# 如果USE_SUBSET为True，则会使用数据集的一个小子集。这通常用于快速测试或调试。\n",
    "if USE_SUBSET:\n",
    "    # shuffle(seed=42): 使用固定的随机种子42对数据集进行随机打乱。select(range(1000)): 从打乱后的数据集中选择前1000个样本\n",
    "    train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "    val_dataset = val_dataset.shuffle(seed=42).select(range(100))\n",
    "    test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
    "# 这两行代码用于打印训练集的特征和第一个样本，以便检查数据处理的结果。\n",
    "# Print the features\n",
    "print(train_dataset.features)\n",
    "# Print the first example\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Feed the first training example through the LM.\n",
    ">Note: The model takes the token ids with the argument `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Call model on the first example\n",
    "# train_dataset[0]: 获取训练集的第一个样本。[\"input_ids\"]: 提取该样本的输入ID序列。torch.tensor(): 将输入ID转换为PyTorch张量。\n",
    "#.unsqueeze(0): 在第0维添加一个维度，将形状从 [sequence_length] 变为 [1, sequence_length]，这是因为模型期望的输入是一个批次。\n",
    "# output = roberta(input_ids=...)将处理后的输入传递给RoBERTa模型。模型返回一个包含多个输出的对象。\n",
    "output = roberta(input_ids=torch.tensor(train_dataset[0][\"input_ids\"]).unsqueeze(0))\n",
    "'''last_hidden_state: 这是模型最后一层的隐藏状态。\n",
    "打印其形状，通常为 [batch_size, sequence_length, hidden_size]。\n",
    "batch_size 在这里是1。\n",
    "sequence_length 是输入序列的长度。\n",
    "hidden_size 是模型的隐藏层大小（对于RoBERTa-base通常是768）。'''\n",
    "print(output.last_hidden_state.shape)\n",
    "'''pooler_output: 这是序列的池化表示，通常用于分类任务。\n",
    "打印其形状，通常为 [batch_size, hidden_size]。\n",
    "batch_size 在这里是1。\n",
    "hidden_size 与上面相同。'''\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output is a contextualized representation of the input, and therefor can be used as such in your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这个模型的工作流程是：\\n输入文本通过预训练的transformer模型（如RoBERTa）进行处理。\\n使用transformer模型的池化输出作为文本的表示。\\n将这个表示传入一个简单的线性分类器。\\n分类器输出两个数值，对应两种情感类别的得分。\\n这种设计允许利用预训练模型的强大特征提取能力，同时通过微调或添加任务特定层来适应情感分析任务。\\nfreeze_embedder选项提供了灵活性，可以选择是否更新预训练模型的参数，这在不同的应用场景中可能会有不同的效果。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentAnalysisModel(torch.nn.Module):\n",
    "    # base_model：预训练的transformer模型（如RoBERTa）\n",
    "    # freeze_embedder：布尔值，决定是否冻结基础模型的参数。\n",
    "    def __init__(self, base_model, freeze_embedder=False):\n",
    "        super().__init__()\n",
    "        # We are using the transformer model as base_model\n",
    "        self.base_model = base_model\n",
    "        # Freeze the base_model\n",
    "        if freeze_embedder:\n",
    "            # self.base_model.parameters() 返回基础模型（如RoBERTa）的所有可学习参数。这个循环遍历每一个参数。\n",
    "            for param in self.base_model.parameters():\n",
    "                # 当一个参数的 requires_grad 为 False 时，PyTorch 不会为该参数计算梯度，也不会在反向传播过程中更新这个参数\n",
    "                param.requires_grad = False\n",
    "        # We add a linear layer on top of the base_model\n",
    "        # 添加一个线性层作为分类器。输入维度是基础模型的隐藏层大小，输出维度是2（对应两个情感类别）\n",
    "        '''预训练的基础模型（如RoBERTa）是为通用语言理解任务设计的，而不是专门为情感分析设计。\n",
    "分类器层将通用的语言表示转化为特定任务（情感分析）的输出。\n",
    "基础模型的输出通常是高维的（例如，768维）。\n",
    "情感分析任务需要的是低维输出（在这个例子中是2维，对应两种情感）。\n",
    "分类器实现了从高维到低维的映射。'''\n",
    "        self.classifier = torch.nn.Linear(base_model.config.hidden_size, 2)\n",
    "        # Initialize the classifier weights\n",
    "        # 使用Xavier均匀分布初始化分类器的权重。\n",
    "        # 将分类器的偏置初始化为零。\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight) # Xavier uniform initialization\n",
    "        torch.nn.init.zeros_(self.classifier.bias) # Initialize bias with zeros\n",
    "\n",
    "    def forward(self, **model_inputs):# **用于接收任意数量的关键字参数和解包字典\n",
    "        '''**model_inputs允许传入任意关键字参数，model_inputs 通常是一个字典，包含了模型所需的各种输入。\n",
    "首先通过基础模型处理输入。\n",
    "然后使用基础模型的pooler_output作为句子表示，传入分类器。\n",
    "返回分类器的输出。'''\n",
    "        # model_inputs is a dict\n",
    "        # Pass the inputs to the model to produce an embedding\n",
    "        base_model_output = self.base_model(**model_inputs)\n",
    "        # Pass the embedding through the classifier\n",
    "        # here we use the pooler_output as the representation of the sentence (depends on the model)\n",
    "        output = self.classifier(base_model_output.pooler_output)\n",
    "        return output\n",
    "'''这个模型的工作流程是：\n",
    "输入文本通过预训练的transformer模型（如RoBERTa）进行处理。\n",
    "使用transformer模型的池化输出作为文本的表示。\n",
    "将这个表示传入一个简单的线性分类器。\n",
    "分类器输出两个数值，对应两种情感类别的得分。\n",
    "这种设计允许利用预训练模型的强大特征提取能力，同时通过微调或添加任务特定层来适应情感分析任务。\n",
    "freeze_embedder选项提供了灵活性，可以选择是否更新预训练模型的参数，这在不同的应用场景中可能会有不同的效果。'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这段代码的目的和作用：\\n设备优化：通过选择合适的设备（GPU或CPU），确保模型能够高效运行。\\n模型初始化：创建并配置情感分析模型，为后续的训练做准备。\\n灵活性：通过 freeze_embedder 参数，提供了是否冻结基础模型的选项，允许在不同场景下进行实验。\\n初步测试：通过运行一个样本，验证模型的基本功能是否正常，包括数据处理和前向传播。\\n调试准备：这种设置有助于快速发现任何初始化或兼容性问题。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# 创建 SentimentAnalysisModel 的实例。\n",
    "# roberta 是预先加载的RoBERTa模型，作为基础模型传入。\n",
    "# .to(device) 将模型移动到之前定义的设备（GPU或CPU）上。\n",
    "model = SentimentAnalysisModel(roberta, freeze_embedder=False).to(device) # Set to True to freeze the embedder for faster training\n",
    "# 这行代码对模型进行了一个简单的测试。\n",
    "# train_dataset[0][\"input_ids\"] 获取训练集中第一个样本的输入ID。\n",
    "# torch.tensor([...]) 将输入ID转换为PyTorch张量。\n",
    "# .device=device 确保输入数据在正确的设备上（与模型相同）。\n",
    "# 整个表达式将一个样本输入模型，测试模型的前向传播。\n",
    "model(input_ids=torch.tensor([train_dataset[0][\"input_ids\"]], device=device))\n",
    "\n",
    "'''这段代码的目的和作用：\n",
    "设备优化：通过选择合适的设备（GPU或CPU），确保模型能够高效运行。\n",
    "模型初始化：创建并配置情感分析模型，为后续的训练做准备。\n",
    "灵活性：通过 freeze_embedder 参数，提供了是否冻结基础模型的选项，允许在不同场景下进行实验。\n",
    "初步测试：通过运行一个样本，验证模型的基本功能是否正常，包括数据处理和前向传播。\n",
    "调试准备：这种设置有助于快速发现任何初始化或兼容性问题。'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only passed the token ids to the model.\\\n",
    "However, there are more inputs for transformer-based models which you need to be aware of:\n",
    "\n",
    "1. **input_ids**: Token IDs to be fed to the model.\n",
    "2. **attention_mask**: Mask to avoid performing attention on padding token indices.\n",
    "3. **token_type_ids**: Segment token indices to indicate different portions of the inputs (used in models like BERT for tasks like question answering).\n",
    "4. **position_ids**: Indices of positions of each input sequence token in the position embeddings.\n",
    "\n",
    "If you don't pass them into the model, the library will take care of it!\\\n",
    "However, if you perform padding during batching, the model might not know where padding was applied and therefore you will also have to provide the `attention_mask`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the processed data and the model in place, we create the appropriate dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   627,  5581,     9,   209,    80, 17205,  2415,  2156,     8,\n",
      "           190,     5,   498,    11,    61,    51,  3033,  1437,     2],\n",
      "        [    0, 23702,  1461,  1437,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32), 'labels': tensor([1, 0])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# features 是一个列表，包含了多个样本（每个样本是一个字典）\n",
    "def collate_fn(features):\n",
    "    # We need to pad the input to make sure all sentences have the same length\n",
    "    # 将每个样本的 input_ids 转换为张量。\n",
    "    # 使用 pad_sequence 将不同长度的序列填充到相同长度。\n",
    "    # batch_first=True 确保输出的形状是 (batch_size, sequence_length)。\n",
    "    # padding_value 使用tokenizer的pad_token_id。pad_token_id在某些模型中，它可能是0或1，而在其他模型中可能是不同的值。\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(f[\"input_ids\"]) for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    if \"attention_mask\" in features[0]:\n",
    "        # If the features contain attention_mask, we should pad them as well\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(f[\"attention_mask\"]) for f in features],\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "    else:\n",
    "        '''input_ids != tokenizer.pad_token_id:\n",
    "这个操作会比较 input_ids 张量中的每个元素与 tokenizer.pad_token_id 的值。\n",
    "结果是一个布尔张量,.int() 转换:将布尔张量转换为整数张量。True 变为 1，False 变为 0。\n",
    "得到的 attention_mask 是一个与 input_ids 形状相同的张量。\n",
    "值为 1 的位置表示实际的 token（模型应该关注的部分）。\n",
    "值为 0 的位置表示填充 token（模型应该忽略的部分）。\n",
    "指导模型注意力:\n",
    "告诉模型哪些位置包含实际的输入信息，哪些位置是填充。\n",
    "模型在计算自注意力时会使用这个掩码，确保不会关注到填充的部分。'''\n",
    "        # We need to create an attention mask from input_ids\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).int()\n",
    "    labels = torch.tensor([f[\"label\"] for f in features])\n",
    "    # 将处理后的数据组织成一个字典。\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# DataLoader 是 PyTorch 中用于加载数据的工具\n",
    "#batch_size: 训练集为2，验证集为8。这决定了每次迭代处理的样本数。\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing the collate function ourselbes, the transformers library provides them for different applications as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   560,  7798,  ...,     1,     1,     1],\n",
      "        [    0,  1090,   991,  ...,     1,     1,     1],\n",
      "        [    0,  1116,   130,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    18,    10,  ...,     1,     1,     1],\n",
      "        [    0, 12473,    65,  ...,     1,     1,     1],\n",
      "        [    0,   354,  8045,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# with_format 方法用于指定数据集应该返回的列\n",
    "# DataCollatorWithPadding(tokenizer, padding=\"longest\")是 Hugging Face Transformers 库提供的一个工具，专门用于处理文本数据的批处理\n",
    "# DataCollatorWithPadding 是针对 transformer 模型优化的，可能比自定义函数更高效;而且不用自己写collate_fn\n",
    "# 它主要负责将不同长度的序列填充到相同长度，以便能够组成一个批次\n",
    "#　tokenizer是您使用的 tokenizer 对象，通常是与您的模型相对应的 tokenizer\n",
    "# padding=\"longest\":指定了填充策略。\"longest\" 表示在每个批次中，所有序列都会被填充到该批次中最长序列的长度\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   405,   128,  ...,     1,     1,     1],\n",
      "        [    0,   879,  4825,  ...,     1,     1,     1],\n",
      "        [    0, 37984,   201,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   102,  1569,  ...,     1,     1,     1],\n",
      "        [    0, 17075, 21871,  ...,     1,     1,     1],\n",
      "        [    0,   179,     5,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write an evaluation function that computes the accuracy of all samples in a dataloader using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, device=None):\n",
    "    # 在评估过程中不需要计算梯度，这可以节省内存并加速计算\n",
    "    with torch.no_grad():\n",
    "        # model.eval() 是 PyTorch 中一个非常重要的方法调用，用于将模型设置为评估模式\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        # 使用 tqdm 显示进度条\n",
    "        # dataloader: 这是要迭代的对象，在这里是 PyTorch 的 DataLoader 实例。\n",
    "        # desc=\"Evaluation\": 设置进度条的描述文本，这里显示为 \"Evaluation\"。\n",
    "        # leave=False: 进度条完成后是否保留。设置为 False 意味着进度条会在完成后消失\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # 使用模型对输入数据进行前向传播。\n",
    "            # input_ids: 输入序列的token ID。\n",
    "            # attention_mask: 指示哪些位置是有效输入（非填充）。\n",
    "            # outputs: 模型的输出，通常是每个类别的预测分数（logits）\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # torch.argmax: 返回指定维度上最大值的索引。\n",
    "            # dim=1: 在第二个维度（类别维度）上取最大值。\n",
    "            # predicted: 包含每个样本预测类别的张量。\n",
    "            #　例如predicted = tensor([1, 1, 0, 1])，predicted 张量包含了每个样本的预测类别：1 表示模型预测该评论为正面；0 表示模型预测该评论为负面\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            # tensor.size(0) 特指返回张量第一个维度（通常是批次大小）的大小。\n",
    "            # 在大多数 PyTorch 数据加载器中，第一个维度通常表示批次大小。对于分类任务，labels 通常是一个一维张量，其长度等于批次大小。\n",
    "            total += labels.size(0)\n",
    "            #　.item() 方法:将单元素张量转换为 Python 标量。这是必要的，因为我们要将结果加到 Python 的整数变量 correct 上\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f577deee48f042a5bca0879411a40655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy on validation data: 0.4690366972477064\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation accuracy on validation data: {evaluate(model, val_dataloader, device=device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a training loop that is just the same as before.\\\n",
    "We have to make sure to feed in the correct arguments to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1155edae6b48758d6f951ca81fe344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d2c61a3b8149f8b31aac4feeb3e376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6315 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd5f5b59464d07a0911ac17b8cec5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a3c22b15646d1a9c7ca4366175bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5376c47bffba4c3f8ecd244eeb8ab3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用 AdamW 优化器，这是 Adam 优化器的一个变体，具有权重衰减修正\n",
    "# model.parameters(): 指定要优化的参数，这里是模型的所有参数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#　这意味着整个训练数据集将被遍历 3 次。\n",
    "num_epochs = 3\n",
    "# 创建一个字典来存储和跟踪训练过程中的指标\n",
    "metric_dict = {\"Loss\": \"-\", \"Val Acc\": evaluate(model, val_dataloader, device=device)}\n",
    "# total: 设置为总批次数（轮数 * 每轮批次数）；DataLoader 被设计为一个可迭代对象，每次迭代返回一个批次的数据，所以len(train_dataloader)是训练数据加载器中的批次数量\n",
    "# unit: 进度单位设为 \"batch\"\n",
    "with tqdm(\n",
    "    total=num_epochs * len(train_dataloader), desc=\"Training\", unit=\"batch\"\n",
    ") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            # 清除之前的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播，计算梯度\n",
    "            loss.backward()\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            #　更新损失值\n",
    "            # loss.item() 用于将单个元素的 PyTorch 张量转换为 Python 标量\n",
    "            metric_dict[\"Loss\"] = loss.item()\n",
    "            # 更新进度条显示的指标\n",
    "            pbar.set_postfix(metric_dict)\n",
    "            # 推进进度条\n",
    "            pbar.update(1)\n",
    "        metric_dict[\"Val Acc\"] = evaluate(model, val_dataloader, device=device)\n",
    "        pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the transformers library makes it simple to use LMs as it includes task-specific models for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这个例子展示了如何加载模型和相应的分词器，并使用它们进行简单的预测。\\n总之，这行代码是使用预训练语言模型进行下游任务的典型起点，\\n它利用了迁移学习的强大功能，为特定的分类任务提供了一个强大的基础。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RobertaForSequenceClassification model can be used for text classification tasks like sentiment analysis\n",
    "# It has a sequence classification head, that is a linear layer on top of the RoBERTa model that outputs a classification label\n",
    "# RobertaForSequenceClassification是 Transformers 库中的一个类，专门用于序列分类任务的 RoBERTa 模型\n",
    "# 它在 RoBERTa 基础模型之上添加了一个分类头（通常是一个线性层）\n",
    "# .from_pretrained():是一个类方法，用于加载预训练的模型权重。它不仅加载权重，还配置模型架构以匹配预训练模型。\n",
    "#　roberta-base是要加载的预训练模型的标识符，用于识别和加载特定的预训练模型\n",
    "# num_labels=2 指定分类任务的类别数量。在这个例子中，设置为 2 表示这是一个二分类任务。\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "'''这个例子展示了如何加载模型和相应的分词器，并使用它们进行简单的预测。\n",
    "总之，这行代码是使用预训练语言模型进行下游任务的典型起点，\n",
    "它利用了迁移学习的强大功能，为特定的分类任务提供了一个强大的基础。'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use their Trainer API so that you don't have to implement the training loop again and again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='None' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      None\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694606</td>\n",
       "      <td>0.490826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.98 GiB is allocated by PyTorch, and 376.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\trainer.py:2584\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2580\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2584\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2588\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\accelerate\\optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\optimization.py:637\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    635\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p)\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m exp_avg, exp_avg_sq \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    640\u001b[0m beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.98 GiB is allocated by PyTorch, and 376.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Define a function to compute the metrics\n",
    "# 这是一个名为 compute_metrics 的函数，用于计算模型预测的准确率\n",
    "def compute_metrics(pred_and_label: EvalPrediction):\n",
    "    return {\n",
    "        # predictions.argmax(axis=-1)获取预测结果中概率最高的类别索引;argmax 在最后一个轴上操作，通常对应于类别维度\n",
    "        # == pred_and_label.label_ids将预测的类别与真实标签进行比较;生成一个布尔数组，True 表示预测正确，False 表示预测错误\n",
    "        # .mean()计算布尔数组的平均值，即正确预测的比例;.item()将结果从 PyTorch 张量转换为 Python 标量\n",
    "        \"accuracy\": (pred_and_label.predictions.argmax(axis=-1) == pred_and_label.label_ids)\n",
    "        .mean()\n",
    "        .item()\n",
    "    }\n",
    "# 返回值： 函数返回一个字典 {\"accuracy\": 计算得到的准确率}\n",
    "\n",
    "# Define the training arguments\n",
    "# TrainingArguments 对象用于配置模型训练的各种参数;是 Hugging Face Transformers 库中的一个类，用于设置训练过程中的各种选项\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # 指定训练输出（如检查点、日志等）的保存目录\n",
    "    output_dir=\"./results\",\n",
    "    # 设置评估策略为每个 epoch 结束后进行评估。\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # 设置日志记录策略为每个 epoch 结束后记录。\n",
    "    logging_strategy=\"epoch\",\n",
    "    # 使用 Hugging Face 实现的 AdamW 优化器。\n",
    "    optim=\"adamw_hf\",\n",
    "    # 设置学习率为 5e-5，这是一个常用的微调学习率。\n",
    "    learning_rate=5e-5,\n",
    "    # 每个设备（GPU/CPU）的训练批次大小为 32。\n",
    "    per_device_train_batch_size=32,\n",
    "    # 每个设备的评估批次大小为 128。评估时通常可以使用更大的批次。\n",
    "    per_device_eval_batch_size=128,\n",
    "    # 设置训练轮数为 3 个 epoch\n",
    "    num_train_epochs=3,\n",
    "    # 设置权重衰减为 0.01，用于正则化。\n",
    "    weight_decay=0.01,\n",
    "    # 不使用 CPU 进行训练，默认使用 GPU（如果可用）\n",
    "    use_cpu=False,\n",
    "    # 在训练开始前进行一次评估。\n",
    "    eval_on_start=True,\n",
    "    # 不保存模型检查点，以节省磁盘空间。\n",
    "    save_strategy=\"no\",  # We will not save the model for now to save disk space\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "# 这段代码使用 Hugging Face 的 Trainer 类来设置、训练和评估模型。\n",
    "trainer = Trainer(\n",
    "    # 之前定义的模型（可能是 RoBERTa 或其他 Transformer 模型）。\n",
    "    model=model,\n",
    "    # args: 之前定义的 TrainingArguments 对象。\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # processing_class: 设置为 tokenizer，用于处理输入数据（如填充批次）\n",
    "    processing_class=tokenizer,  # enables padding of batches\n",
    "    # 之前定义的计算指标的函数。\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# 这行代码启动模型训练过程。Trainer 会根据之前设置的参数进行训练\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "# 这行代码在训练结束后对模型进行最终评估。\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Language Models with Prompting\n",
    "\n",
    "Text generation language models, such as GPT-3, can be used with prompting to perform various natural language processing tasks, including sentiment analysis.\\\n",
    "Prompting involves providing the model with a specific input or \"prompt\" that guides it to generate the desired output.\\\n",
    "This technique leverages the model's pre-trained knowledge to perform tasks without requiring additional fine-tuning.\n",
    "\n",
    "#### Applying Prompting to Sentiment Analysis\n",
    "\n",
    "To use a text generation model for sentiment analysis, you can craft a prompt that instructs the model to classify the sentiment of a given text. The prompt should be designed to elicit a response that indicates whether the sentiment is positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformers library makes it simple to load a text generation model.\n",
    ">Note that they can be quite large and potentially do not fit or run slowly on your CPU/RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "# 这是 Hugging Face Transformers 库中的一个高级 API，用于快速设置和使用预训练模型进行各种 NLP 任务\n",
    "pipe = pipeline(\n",
    "    # 指定任务类型为文本生成,这意味着模型将被用于生成文本，而不是分类或其他任务\n",
    "    \"text-generation\",\n",
    "    # 指定使用的模型;这是微软开发的 Phi-3.5-mini-instruct 模型，是一个相对较小但功能强大的指令调优语言模型\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=device,  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    # 设置模型使用的数据类型为 bfloat16\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format for Chat-Based Decoder Models\n",
    "\n",
    "Chat-based decoder models, such as GPT-3, typically require inputs in a specific format to generate coherent and contextually relevant responses.\\\n",
    "The input format generally consists of a sequence of messages, each with a role and content.\\\n",
    "The roles depend on the model, and often are \"system\", \"user\", or \"assistant\".\n",
    "\n",
    "#### Example Input Format\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **Role**: Indicates the role of the message sender. Common roles include:\n",
    "   - `system`: Provides instructions or context for the conversation.\n",
    "   - `user`: Represents the input from the user.\n",
    "   - `assistant`: Represents the response from the model.\n",
    "\n",
    "2. **Content**: The actual text of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "\n",
    "generation_args = {\n",
    "    #  限制生成的最大 token 数为 3，因为我们只需要一个简短的标签。\n",
    "    \"max_new_tokens\": 3,  # maximum number of tokens to generate\n",
    "    # 设为 False，只返回新生成的文本，而不是完整的回复。\n",
    "    \"return_full_text\": False,\n",
    "    # temperature=0.0 和 do_sample=False：最确定性的设置。模型总是选择最可能的下一个词。适合需要一致和精确答案的任务，如分类或特定信息提取。\n",
    "    # temperature>0.0 和 do_sample=True：引入随机性，使输出更加多样化。温度越高，输出越随机。适合创意写作或需要多样化回答的场景。\n",
    "    \"temperature\": 0.0,  # temperature for sampling (on if do_sample=True)\n",
    "    \"do_sample\": False,  # whether to sample from the output distribution\n",
    "}\n",
    "# 将消息和生成参数传递给管道。使用 **generation_args 展开参数字典。\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output)\n",
    "'''总的来说，这段代码展示了如何使用通用语言模型执行特定的 NLP 任务（在这里是情感分析），\n",
    "通过精心设计的提示和参数设置来引导模型产生所需的输出。这种方法的灵活性使得同一个模型可以用于多种不同的任务。'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more that you can do with text generation models!\\\n",
    "For example, in-context learning (sometimes also called demonstration learning) is a technique where the model is provided with examples of the task it needs to perform within the prompt itself.\\\n",
    "This helps the model understand the task better and generate more accurate responses.\\\n",
    "For sentiment analysis, you can provide a few examples of sentences along with their sentiment labels in the prompt.\\\n",
    "The model will then use these examples to infer the sentiment of new sentences.\n",
    "\n",
    "Also, constraining the output tokens can help guiding the model to generate expected outputs and make it easier to parse the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
