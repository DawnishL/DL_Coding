{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n",
      "{'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 4, 'label_text': 'very positive'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"SetFit/sst5\")\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Build vocabulary and pre-processing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def build_vocab(sentences: list[str]):\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        vocab.update(tokenize(sentence))\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab(dataset[\"train\"][\"text\"])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_to_index(token):\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return vocab.get(token, -1)\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token) for token in tokenize(text)]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        lambda x: {\"token_ids\": map_text_to_indices(x[\"text\"])}, num_proc=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Create a function that takes a batch of sequences of token ids (list of list of ints) and converts them into one-hot encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding function for a batch of sentences\n",
    "def one_hot_encode_batch(sentences: list[list[int]]):\n",
    "    # Note that we are assuming that the sentences have the same length\n",
    "    sequence_length = len(sentences[0])\n",
    "    batch_size = len(sentences)\n",
    "\n",
    "    # Create a tensor of zeros with the desired shape (including the batch dimension)\n",
    "    one_hot_vectors = torch.zeros(\n",
    "        batch_size, sequence_length, vocab_size, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    for i, indices in enumerate(sentences):\n",
    "        for j, idx in enumerate(indices):\n",
    "            # Set the appropriate index to 1.0, but only if the index is not -1\n",
    "            if idx >= 0:\n",
    "                one_hot_vectors[i, j, idx] = 1.0\n",
    "\n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa04a28cd6a49a697bb80fa950faddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20681a44ebff4e3d8da4798861c3c1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578cd83325a04239b5a96d020bcc11d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 4, 'label_text': 'very positive', 'token_ids': [8286, 927, 7958, 9943, 7045, 5065, 338, 14881, 2391, 2064, 7045, 1686, 14464, 7045, 8100, 290, 11602]}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = prepare_dataset(dataset)\n",
    "print(preprocessed_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collate function for dynamic truncation\n",
    "def collate_fn(batch):\n",
    "    ## Truncate all sentences in the batch to the shortest length\n",
    "\n",
    "    # Find the minimum length of the sentences in the batch\n",
    "    min_length = min([len(example[\"token_ids\"]) for example in batch])\n",
    "    \n",
    "    # We don't convert the inputs to tensors here because we will apply one-hot encoding and therefore converting to tensors in the model on-the-fly\n",
    "    inputs = [example[\"token_ids\"][:min_length] for example in batch]\n",
    "    labels = torch.tensor([example[\"label\"] for example in batch])\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11030, 8286, 2330], [11514, 11955, 339], [3289, 14111, 7045], [14933, 3289, 15372], [13186, 8957, 8447], [15702, 7212, 13484], [1686, 12701, 5937], [1795, 3289, 12136]]\n",
      "tensor([2, 1, 0, 4, 3, 1, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for train and test datasets\n",
    "train_dataloader = DataLoader(\n",
    "    preprocessed_dataset[\"train\"], batch_size=8, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    preprocessed_dataset[\"validation\"], batch_size=32, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    preprocessed_dataset[\"test\"], batch_size=32, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inputs, labels = batch\n",
    "    print(inputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids: list[list[int]]):\n",
    "        # Instead of the embedding layer, we will use one-hot encoding\n",
    "        # Note: you could also use torch's Embedding layer initialized with the one-hot vectors\n",
    "        encodings = one_hot_encode_batch(input_ids)\n",
    "        # Sum the one-hot vectors to get the bag of words representation\n",
    "        bag_of_words = encodings.sum(dim=1)\n",
    "        # Apply the hidden layer and the output layer\n",
    "        a_1 = torch.relu(self.hidden_layer(bag_of_words))\n",
    "        # No activation function is applied to the output layer because we will use CrossEntropyLoss which applies softmax\n",
    "        z_2 = self.output_layer(a_1)\n",
    "        return z_2\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "hidden_dim = 256\n",
    "num_classes = 5\n",
    "model = SentimentModel(vocab_size, hidden_dim, num_classes)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5323\n",
      "Epoch 2/10, Loss: 1.2794\n",
      "Epoch 3/10, Loss: 0.9363\n",
      "Epoch 4/10, Loss: 0.6376\n",
      "Epoch 5/10, Loss: 0.4704\n",
      "Epoch 6/10, Loss: 0.3709\n",
      "Epoch 7/10, Loss: 0.3100\n",
      "Epoch 8/10, Loss: 0.2662\n",
      "Epoch 9/10, Loss: 0.2455\n",
      "Epoch 10/10, Loss: 0.2284\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Bonus: Evaluate the model by means of accuracy (percentage of correctly predicted classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3086\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_accuracy = evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-dl-ws-2024-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
