{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c10659b-c923-4108-932d-0110be5a11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as LDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176cee6-6ac1-426f-b0d3-7417a8bfd1b4",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c6e382-0130-4f72-8255-4dd236a2d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LDataset.from_csv(\"https://www.kaggle.com/api/v1/datasets/download/camnugent/california-housing-prices\", cache_dir=\"/scratch/singh/hf/datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c08c20-1ac8-441a-8411-046c9cefbb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity'],\n",
       "    num_rows: 20640\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fec073-1d7a-489e-a232-68bf7d8ca674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'longitude': -122.22,\n",
       " 'latitude': 37.86,\n",
       " 'housing_median_age': 21.0,\n",
       " 'total_rooms': 7099.0,\n",
       " 'total_bedrooms': 1106.0,\n",
       " 'population': 2401.0,\n",
       " 'households': 1138.0,\n",
       " 'median_income': 8.3014,\n",
       " 'median_house_value': 358500.0,\n",
       " 'ocean_proximity': 'NEAR BAY'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92647a55-9f9a-4667-8e9b-4caa901aa448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bf943-ba4c-4bbd-876f-4d6aaaddc1bc",
   "metadata": {},
   "source": [
    "#### Separating Numerical Featuress and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdf7af3-ff33-45a6-8029-be0948d04663",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "865f06c3-f7b8-42bb-a595-a269dde10bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['ocean_proximity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26480538-7f3b-4654-9f16-90f45d85f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'median_house_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870ef2a0-f6e1-42b4-b088-07defaad8375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx, row in enumerate(dataset):\n",
    "#     if any(elem is None for elem in list(row.values())):\n",
    "#         print(row)\n",
    "#         print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9ac900c-e1d1-48ee-8263-5613ed887256",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "targets = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b7aba-3691-43b0-9088-b5b0aab0f51b",
   "metadata": {},
   "source": [
    "##### Total Bedrooms has None values, dealing with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af00d540-34d6-42b6-b345-3ca8009fa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_total_bedrooms = 0\n",
    "count_valid_rows = 0\n",
    "for row in dataset:\n",
    "    if row[\"total_bedrooms\"]:\n",
    "        sum_total_bedrooms += row[\"total_bedrooms\"]\n",
    "        count_valid_rows += 1\n",
    "mean_total_bedrooms = sum_total_bedrooms/count_valid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c7eee5-ba75-4461-9b3e-6ecd6c078acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537.8705525375618"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_total_bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57972afd-1d26-4cfb-b8da-9e0e6fe970cb",
   "metadata": {},
   "source": [
    "Now, let's collect all the numerical features into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c1a2e2-f4fd-4a8f-9e22-2d51d78dfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    feature_values = list()\n",
    "    for feat in numerical_features:\n",
    "        if feat == \"total_bedrooms\":\n",
    "            if row[feat]:\n",
    "                feature_values.append(row[feat])\n",
    "            else:\n",
    "                feature_values.append(mean_total_bedrooms)\n",
    "        else:\n",
    "            feature_values.append(row[feat])\n",
    "    features.append(feature_values)\n",
    "    targets.append(row[target_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecd22102-489c-48cc-97d0-39ebe2e9da85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 126.0, 8.3252]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c1fb6b-e507-4ce9-b72f-e8c830432811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452600.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5256d-04ca-4425-9231-4d91576c95a6",
   "metadata": {},
   "source": [
    "#### Now we need to handle the categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb20761-e33f-423a-a827-b4c8cd658491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEAR BAY', 'NEAR BAY', 'NEAR BAY', 'NEAR BAY', 'NEAR BAY']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_proximity = [row['ocean_proximity'] for row in dataset]\n",
    "ocean_proximity[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68ccd4-08fd-4416-8e02-31f385a8d569",
   "metadata": {},
   "source": [
    "##### Let's look into what are the unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18f16b17-ed9a-4d0c-9096-4dd36b6dc2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1H OCEAN', 'ISLAND', 'NEAR BAY', 'INLAND', 'NEAR OCEAN']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_categories = list(set(ocean_proximity))\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb552adf-c8ad-4c5d-990e-e942c58dfd6b",
   "metadata": {},
   "source": [
    "##### Let's create a mapping from the category to the index for the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec6dce8-a08a-40f2-829f-91ecf30656d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_onehot = dict()\n",
    "for idx, category in enumerate(unique_categories):\n",
    "    category_to_onehot[category] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a739251-3a2c-4637-842f-68e9564ffa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<1H OCEAN': 0, 'ISLAND': 1, 'NEAR BAY': 2, 'INLAND': 3, 'NEAR OCEAN': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_to_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eeb8df-df81-43d5-8119-73bd901a62e8",
   "metadata": {},
   "source": [
    "##### Creating One hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff682b4-cd00-447b-abda-8aecea3c71b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors = torch.eye(len(unique_categories))\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e5e4b-130b-49a3-8b4f-b39df14f0ed7",
   "metadata": {},
   "source": [
    "##### Now, for all the values of ocean_proximity, we can add the respective one hot encoded vector for it in the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f74ece2-2c4e-4390-99e6-512bb0f1dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, category in enumerate(ocean_proximity):\n",
    "    #category_to_onehot[category] 获取当前类别对应的独热编码索引。\n",
    "    #onehot_vectors[...] 根据这个索引获取对应的独热向量。\n",
    "    #.tolist() 将独热向量转换为列表。\n",
    "    #features[i].extend(...) 将独热向量添加到对应行的特征列表末尾。\n",
    "    features[i].extend(onehot_vectors[category_to_onehot[category]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1ecf07-4b04-4ae4-b7ef-7ac7deedf739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-122.16,\n",
       " 37.77,\n",
       " 47.0,\n",
       " 1256.0,\n",
       " 537.8705525375618,\n",
       " 570.0,\n",
       " 218.0,\n",
       " 4.375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[290]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f0525-3614-4e05-a64a-b360f3b02517",
   "metadata": {},
   "source": [
    "##### Finally, we will convert these into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86cc92c5-364e-4dcf-a628-fa330f9c2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c155856-a7e1-4034-b4fa-8cc4d2465725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-122.2300,   37.8800,   41.0000,  ...,    1.0000,    0.0000,\n",
       "            0.0000],\n",
       "        [-122.2200,   37.8600,   21.0000,  ...,    1.0000,    0.0000,\n",
       "            0.0000],\n",
       "        [-122.2400,   37.8500,   52.0000,  ...,    1.0000,    0.0000,\n",
       "            0.0000],\n",
       "        ...,\n",
       "        [-121.2200,   39.4300,   17.0000,  ...,    0.0000,    1.0000,\n",
       "            0.0000],\n",
       "        [-121.3200,   39.4300,   18.0000,  ...,    0.0000,    1.0000,\n",
       "            0.0000],\n",
       "        [-121.2400,   39.3700,   16.0000,  ...,    0.0000,    1.0000,\n",
       "            0.0000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f83046e9-96df-4bc7-8b02-11b75f7427f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[452600.],\n",
       "        [358500.],\n",
       "        [352100.],\n",
       "        ...,\n",
       "        [ 92300.],\n",
       "        [ 84700.],\n",
       "        [ 89400.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.view(-1, 1):重塑（reshape）张量的维度。-1 表示自动计算这个维度的大小，以匹配总元素数。\n",
    "#1 表示将张量转换为列向量。\n",
    "y = torch.tensor(targets, dtype=torch.float32).view(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a1677-0723-4493-bff8-a305d48727a2",
   "metadata": {},
   "source": [
    "#### The final part for the preprocessing is the normalization of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afe1355d-9ebd-4213-aaca-b1989851e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 8 columns are the numerical features, \n",
    "# last 5 are the one hot encoded features for ocean_proximity\n",
    "# X[:, :8] 表示选择所有行（:）和前8列（:8）。\n",
    "numeric_features = X[:, :8] \n",
    "# We are calculating mean and the standard deviation \n",
    "# of the numerical features so that we can normalize them\n",
    "# dim=0 表示沿着第0维（列方向）计算均值。\n",
    "# keepdim=True 保持结果的维度，使结果为 (1, 8) 而不是 (8,)。\n",
    "numeric_means = numeric_features.mean(dim=0, keepdim=True)\n",
    "numeric_stds = numeric_features.std(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df47235f-806f-406f-97eb-76d2e64f0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, :8] = (numeric_features - numeric_means) / numeric_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c7d33b1-f789-42d0-bdd0-1164728f5c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3278,  1.0525,  0.9821,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [-1.3228,  1.0432, -0.6070,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [-1.3328,  1.0385,  1.8561,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.8237,  1.7782, -0.9248,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [-0.8736,  1.7782, -0.8454,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [-0.8337,  1.7501, -1.0043,  ...,  0.0000,  1.0000,  0.0000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e931d6-9c2f-4925-8832-293f923d57f1",
   "metadata": {},
   "source": [
    "#### Preparing dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30dfa419-1471-49ac-b3eb-b8384bcaf02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fc9ef94-8b4e-427e-95da-2ec6d2bba116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数将数据集随机分割成训练集和测试集（在这里用作验证集）。\n",
    "# test_size=0.2：指定测试集（这里是验证集）的大小为总数据集的 20%。\n",
    "\"\"\"X_train：训练集特征，包含 80% 的数据。\n",
    "X_val：验证集特征，包含 20% 的数据。\n",
    "y_train：训练集标签，对应 X_train。\n",
    "y_val：验证集标签，对应 X_val。\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a286949-05d3-400f-9838-288449051c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom PyTorch dataset\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X 通常是特征数据（输入）。\n",
    "        # y 通常是目标变量（输出）\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f54d376-ce92-4505-8977-c4554f6c49b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21600\\3886441515.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = RegressionDataset(torch.tensor(X_train, dtype=torch.float32),\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21600\\3886441515.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train, dtype=torch.float32))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21600\\3886441515.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = RegressionDataset(torch.tensor(X_val, dtype=torch.float32),\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21600\\3886441515.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_val, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = RegressionDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                   torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = RegressionDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                 torch.tensor(y_val, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36439600-59d9-41ef-bf8a-9a42014bea0a",
   "metadata": {},
   "source": [
    "#### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d2c60cf-9b5c-476e-843f-e568709e7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)  # Single layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06c8bcea-2687-4763-8eb2-6499587a2e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features的个数\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70fe7c47-3c5a-417b-9a05-10ec406c876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(input_dim=X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c934fd1-abfa-4288-b9f3-0ff55ee240d7",
   "metadata": {},
   "source": [
    "#### Training with GPU and Model Checkpoint Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ca7a3ee-14d8-4a29-97cb-e63d14ea9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (layer1): Linear(in_features=13, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (layer2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1df01385-bf93-4770-ac7b-5c5a6b7e0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# 初始化最佳验证损失为正无穷大。这是为了在训练过程中跟踪和保存性能最好的模型。\n",
    "best_val_loss = float('inf')\n",
    "# 定义了保存最佳模型的文件路径。\n",
    "#.pth 是 PyTorch 模型常用的文件扩展名。\n",
    "checkpoint_path = \"best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cacfe28d-14bd-4b4e-af85-baab52905605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 55966105024.4961, Val Loss: 56631327339.1628\n",
      "Epoch 2/5, Train Loss: 55951323016.9302, Val Loss: 56611359275.6589\n",
      "Epoch 3/5, Train Loss: 55925986296.0620, Val Loss: 56580129625.3023\n",
      "Epoch 4/5, Train Loss: 55889219790.3876, Val Loss: 56537293339.7829\n",
      "Epoch 5/5, Train Loss: 55840925422.1395, Val Loss: 56482729221.9535\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        # Check for NaN in predictions\n",
    "        # 检查模型的预测结果中是否有 NaN（Not a Number）值。\n",
    "        # 如果检测到 NaN，打印警告信息并中断当前 epoch。\n",
    "        # 这是一个重要的安全检查，因为 NaN 可能表示模型训练出现了问题（如梯度爆炸）。\n",
    "        if torch.isnan(predictions).any():\n",
    "            print(f\"NaN detected in predictions at epoch {epoch}\")\n",
    "            break\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        ## was getting exploding gradients problem with this model, needed to do gradient clipping\n",
    "        # 使用梯度裁剪来防止梯度爆炸问题。\n",
    "        # max_norm=1.0 设置梯度的最大范数为 1.0。\n",
    "        # 如果梯度的范数超过这个值，它们会被缩放到这个范数。\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        # print(train_loss)\n",
    "    # 如果您的数据集有 1000 个样本，批次大小为 32，那么 len(train_loader) 通常会是 ceil(1000/32) = 32（向上取整）\n",
    "   # a = a / b\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Save the best model checkpoint\n",
    "    # 当发现更好的模型时，将其保存为检查点。\n",
    "    '''保存的内容包括：\n",
    "    当前轮次（epoch）\n",
    "    模型的状态字典（包含所有层的权重和偏置）\n",
    "    优化器的状态字典（包含优化器的状态）\n",
    "    最佳验证损失\n",
    "    checkpoint_path 是之前定义的保存路径。\n",
    "    '''\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{5}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c24e43-32b9-4865-b827-4b11b8deeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 5 with validation loss 56482729221.9535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21600\\3037169124.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# 将保存的模型状态加载到当前模型中。这会恢复模型的所有参数（权重和偏置）。\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# 将保存的优化器状态加载到当前优化器中。\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with validation loss {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab97e49-f54f-4968-aae9-8bb41ba0873d",
   "metadata": {},
   "source": [
    "#### Now, let's do the device comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31542258-dc4d-4f51-ba4f-f2095b1ce0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2ba9d01-f196-43df-81a0-2fb4f17bd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_device(model, data_loader, intended_device, epochs=5):\n",
    "    \"\"\"\n",
    "    Trains the model on the specified device and returns the training time.\n",
    "    \"\"\"\n",
    "    device = torch.device(intended_device)\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    # 它返回当前时间的时间戳，表示为从 Unix 纪元（1970年1月1日 00:00:00 UTC）开始的秒数。\n",
    "    # 标记了某个操作或过程的开始时间点。\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5f2c4e5-ed4a-4beb-99f0-31a1288bd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU...\n",
      "Epoch 1/5, Loss: 55965907646.5116\n",
      "Epoch 2/5, Loss: 55950732811.9070\n",
      "Epoch 3/5, Loss: 55924951794.1085\n",
      "Epoch 4/5, Loss: 55887743785.6744\n",
      "Epoch 5/5, Loss: 55838962898.3566\n",
      "CPU Training Time: 2.13 seconds\n",
      "\n",
      "Training on GPU...\n",
      "Epoch 1/5, Loss: 55966073947.2868\n",
      "Epoch 2/5, Loss: 55951081856.9922\n",
      "Epoch 3/5, Loss: 55925628769.2403\n",
      "Epoch 4/5, Loss: 55888597734.2016\n",
      "Epoch 5/5, Loss: 55839787619.2248\n",
      "GPU Training Time: 4.00 seconds\n",
      "\n",
      "GPU is approximately 0.53x faster than CPU.\n"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[1]\n",
    "model_cpu = SimpleModel(input_dim)\n",
    "model_gpu = SimpleModel(input_dim)\n",
    "\n",
    "# Train on CPU\n",
    "print(\"Training on CPU...\")\n",
    "cpu_time = train_model_on_device(model_cpu, train_loader, intended_device=\"cpu\")\n",
    "print(f\"CPU Training Time: {cpu_time:.2f} seconds\")\n",
    "\n",
    "# Train on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nTraining on GPU...\")\n",
    "    gpu_time = train_model_on_device(model_gpu, train_loader, intended_device=\"cuda\")\n",
    "    print(f\"GPU Training Time: {gpu_time:.2f} seconds\")\n",
    "\n",
    "    # Compare CPU and GPU times\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nGPU is approximately {speedup:.2f}x faster than CPU.\")\n",
    "else:\n",
    "    print(\"\\nGPU not available. Skipping GPU comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde743e1-f835-49cd-950f-ca67aaae79e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62854f50-9a4a-45c9-aab0-4e0e63be32e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
