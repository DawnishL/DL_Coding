{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description:\n",
    "* A neural network consists of neurons\n",
    "* Most common function a neuron calculates is linear: $z = \\mathbf{w}*\\mathbf{x} + b$, where $\\mathbf{x} \\in \\mathbb{R}^{nÂ \\times 1}$ is the inputs to a neuron, $\\mathbf{w} \\in \\mathbb{R}^{1 \\times n}$ are trainable weights and $b$ is a scalar trainable bias term\n",
    "* To give a neuron more representational power (i.e. learning non-linear functions), each neuron is followed by a non-linear *activation function f*, e.g. $y = f(z) = sigmoid(z)$. \n",
    "* Neurons are arranged in layers (see image below) that can be stacked\n",
    "* The math of a single neuron then generalizes to a layer of neurons via $\\mathbf{z} = W * \\mathbf{x} + \\mathbf{b}$ with the weights for all neurons collected in a matrix $W \\in \\mathbb{R}^{m \\times n}$, the bias of all neurons as a vector $b \\in \\mathbb{R}^m$ and the input to the layer as vector $\\mathbf{x} \\in \\mathbb{R}^n$. The output of a layer will then be a vector $\\mathbf{z} \\in \\mathbb{R}^m$ - each column represents the weights w.r.t one input connection, each row one neuron, yielding the transformation $\\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{m \\times 1}$. \n",
    "* The activation function is then applied element-wise to $\\mathbf{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" width=500/>\\\n",
    "source: https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an example input (input layer), which is just a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 0], dtype=torch.float) # 4-dim float vector\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Layer Types in PyTorch\n",
    "\n",
    "PyTorch provides a variety of layer types that can be used to build neural networks. Some of the most commonly used layers include:\n",
    "\n",
    "1. **Linear Layer (`torch.nn.Linear`)**:\n",
    "    - Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "    - Parameters: `in_features` (size of each input sample), `out_features` (size of each output sample), `bias` (if set to False, the layer will not learn an additive bias)\n",
    "\n",
    "2. **Convolutional Layer (`torch.nn.Conv2d`)**:\n",
    "    - Applies a 2D convolution over an input signal composed of several input planes\n",
    "    - Parameters: `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups`, `bias`\n",
    "\n",
    "3. **Recurrent Layers (`torch.nn.RNN`, `torch.nn.LSTM`, `torch.nn.GRU`)**:\n",
    "    - `torch.nn.RNN`: Applies a simple recurrent neural network (RNN) to an input sequence\n",
    "    - `torch.nn.LSTM`: Applies a Long Short-Term Memory (LSTM) network to an input sequence\n",
    "    - `torch.nn.GRU`: Applies a Gated Recurrent Unit (GRU) network to an input sequence\n",
    "    - Parameters: `input_size`, `hidden_size`, `num_layers`, `bias`, `batch_first`, `dropout`, `bidirectional`\n",
    "\n",
    "4. **Dropout Layer (`torch.nn.Dropout`)**:\n",
    "    - Randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution\n",
    "    - Parameters: `p` (probability of an element to be zeroed)\n",
    "\n",
    "5. **Batch Normalization Layer (`torch.nn.BatchNorm1d`, `torch.nn.BatchNorm2d`)**:\n",
    "    - Applies Batch Normalization over a 2D or 4D input\n",
    "    - Parameters: `num_features`, `eps`, `momentum`, `affine`, `track_running_stats`\n",
    "\n",
    "6. **Activation Functions**:\n",
    "    - `torch.nn.ReLU`: Applies the rectified linear unit function element-wise\n",
    "    - `torch.nn.Sigmoid`: Applies the sigmoid function element-wise\n",
    "    - `torch.nn.Tanh`: Applies the hyperbolic tangent function element-wise\n",
    "    - `torch.nn.Softmax`: Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1\n",
    "\n",
    "These layers can be combined to create complex neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a linear layer of input size `n` and output size `m`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = Parameter containing:\n",
      "tensor([[-0.2029, -0.3806, -0.2161,  0.1609],\n",
      "        [-0.0146, -0.4971, -0.3508,  0.3111],\n",
      "        [ 0.0547, -0.1622,  0.2619,  0.0439],\n",
      "        [-0.0188, -0.1419,  0.4887, -0.0865],\n",
      "        [-0.4326,  0.1011, -0.4716, -0.2515]], requires_grad=True)\n",
      "b = Parameter containing:\n",
      "tensor([ 0.4935, -0.0506, -0.3719, -0.3167, -0.1664], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.Linear(4, 5, bias=True) # 4 inputs, 5 outputs -> 5 neurons\n",
    "# W\n",
    "print(\"W =\", layer.weight) # 5x4 tensor connecting all inputs to all outputs\n",
    "# b\n",
    "print(\"b =\", layer.bias) # tensor of size 5, one bias for each output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forward the input x through our layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2774, -0.4014, -0.1100,  0.1720, -0.6380], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z1 = layer(x)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now only missing an activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5689, 0.4010, 0.4725, 0.5429, 0.3457], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.sigmoid(z1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and that's our first layer!\\\n",
    "We can create the other layers in the same fashion.\\\n",
    "But to organize everything together into one model, it is useful to subclass from `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.nn.Module` Class\n",
    "\n",
    "The `torch.nn.Module` class is the base class for all neural network modules in PyTorch. It provides a way to encapsulate parameters, layers, and methods for building and training neural networks. Here are some key features and functionalities of the `torch.nn.Module` class:\n",
    "\n",
    "1. **Initialization (`__init__` method)**:\n",
    "    - This method is used to define the layers and parameters of the model.\n",
    "    - Layers are typically instances of other `torch.nn` modules (e.g., `torch.nn.Linear`, `torch.nn.Conv2d`).\n",
    "\n",
    "2. **Forward Pass (`forward` method)**:\n",
    "    - This method defines the computation performed at every call.\n",
    "    - It takes an input tensor and returns an output tensor.\n",
    "    - The `forward` method is called automatically when the module is called (e.g., `output = model(input)`).\n",
    "\n",
    "3. **Parameters**:\n",
    "    - Parameters are instances of `torch.nn.Parameter`, which are tensors that are considered as model parameters.\n",
    "    - They are automatically registered as parameters when assigned as attributes of the module.\n",
    "\n",
    "4. **Submodules**:\n",
    "    - Modules can contain other modules (submodules), which are registered as submodules when assigned as attributes.\n",
    "    - This allows for building complex models by composing simpler modules.\n",
    "\n",
    "5. **Device Management**:\n",
    "    - Modules can be moved to different devices (e.g., CPU, GPU) using the `to` method.\n",
    "    - This ensures that all parameters and buffers are moved to the specified device.\n",
    "\n",
    "6. **Training and Evaluation Modes**:\n",
    "    - Modules can be set to training mode using `model.train()` and evaluation mode using `model.eval()`.\n",
    "    - This affects certain layers like dropout and batch normalization, which behave differently during training and evaluation.\n",
    "\n",
    "Example of a simple neural network using `torch.nn.Module`:\n",
    "\n",
    "```python\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(4, 5)\n",
    "        self.layer2 = torch.nn.Linear(5, 7)\n",
    "        self.layer3 = torch.nn.Linear(7, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y1 = torch.sigmoid(self.layer1(x))\n",
    "        y2 = torch.sigmoid(self.layer2(y1))\n",
    "        y3 = self.layer3(y2)\n",
    "        return y3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "In this task, define a simple neural network with three linear layers and ReLU activations by subclassing `torch.nn.Module`.\\\n",
    "Since we want to perform binary classification, the output layer should have one output and a sigmoid activation function.\\\n",
    "You can use the following skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        # define the layers\n",
    "\n",
    "        # create input layer: 5 neurons\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 5)\n",
    "        # hidden layer with 3 neurons\n",
    "        self.layer2 = torch.nn.Linear(5, 3)\n",
    "        # output layer with 1 neuron for binary classification\n",
    "        self.layer3 = torch.nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # define the forward pass\n",
    "        y1 = torch.relu(self.layer1(x))\n",
    "        y2 = torch.relu(self.layer2(y1))\n",
    "        y3 = torch.sigmoid(self.layer3(y2))\n",
    "        return y3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell PyTorch where tensors are stored and therefore where the computations run.\\\n",
    "A GPU is usually much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5486], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# we know that the input has 4 dimensions, we could also infer this from the data, e.g., x.shape[0], x.size(dim=-1), len(x), x.dim()\n",
    "model = SimpleNN(4).to(device) # instantiate model and move to device\n",
    "x = x.to(device) # note that tensors of an operation need to be on the same device\n",
    "y = model.forward(x) # directly call the forward function, don't use this!\n",
    "y = model(x) # same as model.forward(x) but with extended pre-processing\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning works by adjusting the weights w.r.t to an objective function \n",
    "    * calculate the objective function based on calculated output vs. real labels from dataset\n",
    "    * take gradients of objective function w.r.t. network weights and biases\n",
    "    * use a gradient-based optimizer to update the existing weights using the gradients\n",
    "* An example objective function could be quadratic loss:\n",
    "    $L_2(y, y') = (y - \\hat{y})^2$\n",
    "* Extending the loss to multiple samples could e.g. be Mean-Squared-Error loss: $\\frac{1}{M}\\sum_{i=1}^{M} (y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Loss Functions\n",
    "\n",
    "Loss functions, also known as cost functions or objective functions, are used to evaluate how well a model's predictions match the target values. PyTorch provides several built-in loss functions that can be used for different types of tasks, such as regression, classification, and more. Here are some commonly used loss functions in PyTorch:\n",
    "\n",
    "1. **Mean Squared Error Loss (`torch.nn.MSELoss`)**:\n",
    "    - Measures the average squared difference between the predicted and target values.\n",
    "    - Commonly used for regression tasks.\n",
    "    - Formula: $L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "2. **Binary Cross-Entropy Loss (`torch.nn.BCELoss`)**:\n",
    "    - Measures the binary cross-entropy between the predicted and target values.\n",
    "    - Commonly used for binary classification tasks.\n",
    "    - Formula: $L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$\n",
    "\n",
    "3. **Cross-Entropy Loss (`torch.nn.CrossEntropyLoss`)**:\n",
    "    - Combines `LogSoftmax` and `NLLLoss` in one single class.\n",
    "    - Commonly used for multi-class classification tasks.\n",
    "    - Formula: $L(y, \\hat{y}) = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6434, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# These loss functions can be easily used in PyTorch by instantiating the corresponding class and passing the predicted and target values to it.\n",
    "# For our binary classification task, we use the binary cross-entropy loss, which is implemented as `torch.nn.BCELoss`.\n",
    "\n",
    "loss_func = torch.nn.BCELoss() # PyTorch knows several loss/cost/objective functions\n",
    "y_hat = torch.tensor([0.], device=device) # we assume that the target is 0\n",
    "loss = loss_func(y, y_hat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the backward pass (calculating gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dMSE/dW = tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0308,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0360,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0335, -0.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"dMSE/dW =\", model.layer1.weight.grad) # print gradients for W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the weights of the model is now a matter of choosing the step size for subtracting the gradients from the respective weights.\\\n",
    "PyTorch comes with several optimizer functions, e.g. stochastic gradient descent (SGD) or more complex ones, e.g. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_0 = Parameter containing:\n",
      "tensor([[ 0.3188,  0.4286, -0.3643,  0.0205],\n",
      "        [-0.4477, -0.4308, -0.4862,  0.3236],\n",
      "        [-0.3634,  0.4082,  0.3238,  0.4988],\n",
      "        [-0.3625, -0.0764,  0.2363, -0.0209],\n",
      "        [-0.4787, -0.4467, -0.1221, -0.4821]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "W_1 = Parameter containing:\n",
      "tensor([[ 0.3188,  0.4286, -0.3643,  0.0205],\n",
      "        [-0.4477, -0.4308, -0.4862,  0.3236],\n",
      "        [-0.3634,  0.4082,  0.3145,  0.4988],\n",
      "        [-0.3625, -0.0764,  0.2255, -0.0209],\n",
      "        [-0.4787, -0.4467, -0.1120, -0.4821]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"W_0 =\", model.layer1.weight)\n",
    "# è¿ä¸æ­¥åå»ºäºä¸ä¸ªä¼åå¨å¯¹è±¡\n",
    "# params=model.parameters()ï¼ æå®ä¼åå¨è¦æ´æ°çåæ°ã \n",
    "# model.parameters()è¿åæ¨¡åä¸­ææå¯å­¦ä¹ çåæ°ï¼å¦æéååç½®ï¼ãè¿åè¯ä¼åå¨åºè¯¥æ´æ°æ¨¡åä¸­çåªäºåæ°ã\n",
    "# ç½®å­¦ä¹ çï¼learning rateï¼ä¸º0.3ãå­¦ä¹ çæ¯ä¸ä¸ªè¶åæ°ï¼æ§å¶æ¯æ¬¡åæ°æ´æ°çæ­¥é¿ã\n",
    "# è¾å¤§çå­¦ä¹ çå¯è½å¯¼è´å¿«éå­¦ä¹ ä½ä¹å¯è½éè¿æä¼è§£ï¼èè¾å°çå­¦ä¹ çå­¦ä¹ å¯è½è¾æ¢ä½æ´ç¨³å®ã\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=0.3) # tell the optimizer what weights to update and how much\n",
    "# optim.step() æ´æ°æ¨¡åçåæ°ï¼ä½ä¸è¿åä»»ä½å¼ã\n",
    "optim.step() # update weights by applying gradients to the weights\n",
    "print(\"W_1 =\", model.layer1.weight) # updated gradients for W1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify that our parameters are \"more optimal\" with respect to our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5629, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = model(x)\n",
    "loss = loss_func(y, y_hat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only used a single input.\\\n",
    "We want to perform mini-batch SGD and therefor need to forward multiple inputs through the neural network.\\\n",
    "For this, PyTorch allows for a batch dimenion for many objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4304], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0., 0., 1., 0.], device='cuda:0')\n",
      "tensor([[0., 0., 1., 0.]], device='cuda:0')\n",
      "tensor([[0.4304]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4304],\n",
      "        [0.4391],\n",
      "        [0.4306]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward a single input rhought the NN\n",
    "print(model(x))\n",
    "# add a batch dimension and forward the batch through the NN\n",
    "# è¿æ¯å°åä¸ªæ ·æ¬ x è½¬æ¢ä¸ºæ¹å¤çæ ¼å¼çä¾å­; ä»£è¡¨äºæ¹éå¤§å°ä¸º1çæåµ\n",
    "# è¿ä¸ªæä½å°åä¸ªæ ·æ¬è½¬æ¢ä¸ºä¸ä¸ªåªåå«ä¸ä¸ªæ ·æ¬çæ¹æ¬¡ã\n",
    "x_batched = x.unsqueeze(0)\n",
    "print(model(x_batched))\n",
    "# this becomes important when training the model with mini-batches\n",
    "# å½¢ç¶ï¼[3, 4]ï¼3ä¸ªæ ·æ¬ï¼æ¯ä¸ªæ ·æ¬4ä¸ªç¹å¾ï¼\n",
    "# è¿æ¯ä¸ä¸ªçæ­£çå°æ¹éï¼mini-batchï¼ä¾å­\n",
    "X = torch.tensor([[0, 0, 1, 0], [0, 1, 1, 0], [1, 0, 1, 0]], dtype=torch.float).to(device) # 3 samples with 4 features\n",
    "print(model(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
