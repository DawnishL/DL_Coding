{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Gradients\n",
    "\n",
    "In PyTorch, it's as easy as calling `backward()` on a tensor.\n",
    "Only requirement: the tensor has to be enabled for gradient calculation.\n",
    "If you look at previously used tensors e.g., they won't have the `requires_grad` attribute set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy to fix: we can just tell PyTorch that a tensor should record gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], requires_grad=True)\n",
      "y = tensor([2.], requires_grad=True)\n",
      "z = x * 3 + y + 2 = tensor([7.], grad_fn=<AddBackward0>)\n",
      "u = z * z = tensor([49.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "z = x * 3 + y + 2\n",
    "print(\"z = x * 3 + y + 2 =\", z)\n",
    "u = z * z\n",
    "print(\"u = z * z =\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, PyTorch will record all differentiable operations (see `grad_fn`) for the contributing tensors (that were not done in-place) and we can calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "du/dz = tensor([14.])\n",
      "du/dx = tensor([42.])\n",
      "du/dy = tensor([14.])\n"
     ]
    }
   ],
   "source": [
    "z.retain_grad() # keeps gradient info for z instead of only for the leave nodes x,y\n",
    "# backward pass from u\n",
    "u.backward()\n",
    "print(\"du/dz =\", z.grad) # du/dz\n",
    "print(\"du/dx =\", x.grad) # du/dx\n",
    "print(\"du/dy =\", y.grad) # du/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Don't try to backward multiple times on the same graph instance, it will throw an error (you probably don't want to do this anyway)! \n",
    "If you really have to do this, pass the parameter `retain_graph=True` as an argument to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We usually only need to call backward on the output of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also assign elements in a vector.\\\n",
    "There is an important difference though:\\\n",
    "**NEVER modify individual tensor entries in-place if you want to calculate a gradient later** (those operations are not recorded for the computational graph)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbad\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m# in-place modification\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "bad = torch.zeros(2,2, requires_grad=True)\n",
    "bad[0,0] = 1.0 # in-place modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose to use PyTorch operations instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "good = torch.zeros(2,2, requires_grad=True)\n",
    "good = good + torch.eye(2) # re-assignment: new tensor is created, but you loose access to original tensor `good`\n",
    "good.retain_grad() # only to be able to access `grad` attribute later; by default, `grad` is not stored for intermediate tensors\n",
    "print(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will need a different view on your tensor.\\\n",
    "That is, a tensor with the same data but of different shape.\\\n",
    "For this, PyTorch provides the `view` function on tensors.\\\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# create a 1D tensor with values 0,1,2,...,15\n",
    "tensor_1d = torch.tensor(range(16))\n",
    "print(tensor_1d)\n",
    "\n",
    "# view tensor as 4x4 tensor\n",
    "tensor_2d = tensor_1d.view(4,4)\n",
    "print(tensor_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the 2x2 tensors is created: The inner dimensions are filled first.\n",
    "You can use the same function to view any tensor as 1D tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "# view tensor as vector of length 16\n",
    "tensor_2d_as_1d = tensor_2d.view(-1) # -1 means \"infer this dimension\"\n",
    "print(tensor_2d_as_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to our tensor `good`, we can compute a loss as the sum (not really a loss, but we'd like to have a scalar we can compute the derivative for):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<SumBackward1>)\n",
      "tensor(2., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "good_loss = good.view(-1).sum(dim=0) # flatten tensor first, then sum all values\n",
    "print(good_loss)\n",
    "good_loss = good.sum() # equivalent to above\n",
    "print(good_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "good_loss.backward() # success -> gradients computed\n",
    "print(good.grad) # the computed gradient for tensor `good`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION: Gradients are accumulated hence you have to zero them after you've used them (e.g., after optimizing your weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "good_loss.backward() # compute gradients again\n",
    "print(good.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
