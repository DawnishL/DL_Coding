{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca38af-8bad-475f-b7b4-3c3862670d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/singh/hf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2eeb7-c014-4bd7-a7b5-825f68f33975",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee7dab-91f2-420a-a1a0-4a5e1989e8e2",
   "metadata": {},
   "source": [
    "#### Loading the dataset and making another split for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf0837-32cc-449f-9857-22d36785d36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9528dd4267e34e0784f22766c05c9f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\datasets--batterydata--pos_tagging. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998851f04ddb49f58f6820a2dca02e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/5.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272a454c84c2489684d44022b6cf1ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/601k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade21de069414daaa6d552db1403f4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096ec804181f4f47a6684e3c9c39b319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 11748\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1306\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset (assuming the dataset path is 'batterydata/pos_tagging')\n",
    "dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset[\"validation\"] = dataset_split[\"test\"]\n",
    "dataset[\"train\"] = dataset_split[\"train\"]\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8415e74-bb10-475d-b08b-ba51cfceb632",
   "metadata": {},
   "source": [
    "#### Same indexing as before for class to index and index to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32e9e13-65df-4c10-9fa1-4c2ee042dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBS', 'CC', 'CD', 'PRP', 'VB', 'LS', 'JJ', '-LRB-', 'NNS', '``', \"''\", 'JJR', 'JJS', '(', ')', 'WP', 'IN', 'UH', 'SYM', 'POS', 'NNP', '-RRB-', 'VBN', '$', ',', 'WDT', 'PDT', 'WRB', 'TO', 'RP', 'DT', '-NONE-', ':', 'RB', 'PRP$', 'VBG', 'RBR', 'MD', 'NN', 'EX', 'VBD', 'FW', 'VBP', 'NNPS', 'WP$', '#', 'VBZ', '.']\n",
      "Number of classes: 48\n",
      "{'RBS': 0, 'CC': 1, 'CD': 2, 'PRP': 3, 'VB': 4, 'LS': 5, 'JJ': 6, '-LRB-': 7, 'NNS': 8, '``': 9, \"''\": 10, 'JJR': 11, 'JJS': 12, '(': 13, ')': 14, 'WP': 15, 'IN': 16, 'UH': 17, 'SYM': 18, 'POS': 19, 'NNP': 20, '-RRB-': 21, 'VBN': 22, '$': 23, ',': 24, 'WDT': 25, 'PDT': 26, 'WRB': 27, 'TO': 28, 'RP': 29, 'DT': 30, '-NONE-': 31, ':': 32, 'RB': 33, 'PRP$': 34, 'VBG': 35, 'RBR': 36, 'MD': 37, 'NN': 38, 'EX': 39, 'VBD': 40, 'FW': 41, 'VBP': 42, 'NNPS': 43, 'WP$': 44, '#': 45, 'VBZ': 46, '.': 47}\n",
      "{0: 'RBS', 1: 'CC', 2: 'CD', 3: 'PRP', 4: 'VB', 5: 'LS', 6: 'JJ', 7: '-LRB-', 8: 'NNS', 9: '``', 10: \"''\", 11: 'JJR', 12: 'JJS', 13: '(', 14: ')', 15: 'WP', 16: 'IN', 17: 'UH', 18: 'SYM', 19: 'POS', 20: 'NNP', 21: '-RRB-', 22: 'VBN', 23: '$', 24: ',', 25: 'WDT', 26: 'PDT', 27: 'WRB', 28: 'TO', 29: 'RP', 30: 'DT', 31: '-NONE-', 32: ':', 33: 'RB', 34: 'PRP$', 35: 'VBG', 36: 'RBR', 37: 'MD', 38: 'NN', 39: 'EX', 40: 'VBD', 41: 'FW', 42: 'VBP', 43: 'NNPS', 44: 'WP$', 45: '#', 46: 'VBZ', 47: '.'}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = list(\n",
    "    set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]])\n",
    ")\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cb1ec-70c0-4fdc-a295-2f8fd64a7323",
   "metadata": {},
   "source": [
    "#### Initializing model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea793428-768c-4786-8aaf-1dbd2855ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba63b6d3fc5e42c1ab560d54a14752a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df856d20f9542668f3190b685ff32e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d721378dd419f96b277503f404e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d812aaeaa23a427b90fb702a0c04c718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da38d08ce8648c6b88f4f9c2d9b69fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1756b2bd95f345a09b39245d4f51f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57dca058eb74010ac42a26d2b568685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Use DistilRoBERTa (a smaller version of RoBERTa)\n",
    "model_name = \"distilbert/distilroberta-base\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "# add_prefix_space=True 参数确保在分词时在单词前添加空格\n",
    "# num_labels: 指定了分类标签的数量，使用 len(labels_unique) 计算\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_unique), id2label=itoc, label2id=ctoi)\n",
    "# 这段代码的目的是为token分类任务（如命名实体识别）准备模型和tokenizer。\n",
    "# 通过使用预训练的DistilRoBERTa模型，你可以在此基础上进行微调，以适应特定的分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086f558-ce09-4413-88c3-f65fa51c5b18",
   "metadata": {},
   "source": [
    "#### Now we need to align the tokenization output of the tokenizer to the labels as they do sub word tokenization\n",
    "#### Then finally, we will map this function to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331b5dc-3a9c-4caf-88f0-8b87d811dc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fbf8ecfacb48ee97df9fd5a7fc100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba08793785124dbcbeb3537c9fcb3599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"words\"],  # Words are already tokenized\n",
    "        truncation=True, # 允许截断过长的序列\n",
    "        padding=\"max_length\", # 填充到最大长度\n",
    "        is_split_into_words=True # 表明输入已经按词分割\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_idx, word_labels in enumerate(examples[\"labels\"]):  # Iterate over batch\n",
    "        # word_ids() 是 tokenized_inputs 对象的一个方法。\n",
    "        # 这个方法返回一个列表，表示每个 token 对应的原始单词的索引\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_idx)  # Get word IDs\n",
    "        # previous_word_idx 用于跟踪上一个处理的词的索引。这在处理子词（subwords）时特别重要\n",
    "        previous_word_idx = None\n",
    "        labels = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  # Ignore special tokens\n",
    "                # 当 word_idx != previous_word_idx 时，表示遇到了一个新词的开始，此时我们为这个新词分配标签\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(ctoi[word_labels[word_idx]])  # Assign label to first subword\n",
    "            # 如果 word_idx == previous_word_idx（且不为 None），说明当前 token 是前一个词的子词，\n",
    "            # 我们通常会忽略它（在 NER 任务中给予特殊标记 -100）。\n",
    "            else:\n",
    "                labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "        '''重要性：\n",
    "\n",
    "在处理使用子词 tokenization 的模型（如 BERT、RoBERTa 等）时，这种方法非常重要。\n",
    "它确保了在词被分割成多个 token 的情况下，只有第一个 token 获得实际的标签，而其他的子词 token 被适当地忽略。\n",
    "这种处理方式有助于保持原始标注的完整性，同时适应子词 tokenization 的特性。'''\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "'''tokenize_and_align_labels 函数返回的是一个经过处理的字典（dictionary），\n",
    "包含了tokenized的输入和对齐后的标签。\n",
    "让我详细解释一下这个函数的返回值。其中，labels 字段是函数处理的关键部分。\n",
    "它包含了与 input_ids 长度相同的标签列表。\n",
    "对于每个token，如果它是一个词的第一个子词，会被赋予相应的标签。\n",
    "对于子词（非词首token）和特殊token（如[CLS], [SEP]），标签被设置为 -100。'''\n",
    "# Apply tokenization with batched processing\n",
    "# 对于训练集和测试集中的每个样本（或每批样本），map 方法会调用 tokenize_and_align_labels 函数。\n",
    "# 这个函数会对输入文本进行分词，并将原始的词级别标签对齐到 token 级别。\n",
    "# 处理后的数据将包含分词后的输入和对应的 token 级别标签。\n",
    "train_dataset = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"words\"])\n",
    "test_dataset = test_dataset.remove_columns([\"words\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# 这是Hugging Face datasets库中Dataset对象的一个方法。\n",
    "# 它用于指定数据集应该以何种格式返回数据。\n",
    "# \"torch\" 参数：指定要将数据转换为PyTorch张量格式。\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7437912e-ac96-40eb-8e68-6544eceb049c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc7c01c-10de-4320-a9b1-fd65922b61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5fde1-2169-4027-8720-5a5ad366947c",
   "metadata": {},
   "source": [
    "#### Setting up test dataloader for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b36bd04-a3cb-48f9-ae85-fc83008d2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Move model to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2963d4-1cf9-4a09-b815-3d0324aa30fe",
   "metadata": {},
   "source": [
    "#### Defining a function for computing accuracy\n",
    "#### It'll evaluate the model on the test dataset/given dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba0a2c-47b6-41f2-bffd-b36099ce8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.0110\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # outputs.logits:\n",
    "\n",
    "            # 这是模型输出的一部分，通常是一个三维张量。\n",
    "            # 形状通常为 (batch_size, sequence_length, num_classes)。\n",
    "            # 每个元素代表模型对每个token属于每个类别的预测分数（或logits）。\n",
    "            # logits 是模型神经网络最后一层（通常是全连接层）的原始输出。\n",
    "            # 它们是未经过softmax或其他激活函数处理的原始scores。\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)  # Get most likely labels\n",
    "\n",
    "        # Flatten both predictions and labels\n",
    "        # 这行创建了一个布尔掩码，用于识别有效的（非填充）标签。\n",
    "        # 在之前的处理中，-100 被用作填充标签和特殊 token 的标签。\n",
    "        # 这个掩码将在真实标签不是 -100 的位置为 True，否则为 False。\n",
    "        mask = labels != -100  # Ignore padded labels\n",
    "        # predictions[mask] 操作：\n",
    "        # 这个操作会创建一个新的一维张量。\n",
    "        # 新张量只包含 predictions 中对应 mask 为 True 的元素。\n",
    "        '''predictions = [[1, 2, 3, 0],\n",
    "               [2, 1, 0, 0]]\n",
    "mask = [[True, True, True, False],\n",
    "        [True, True, False, False]]\n",
    "        那么 predictions[mask] 的结果将是[1, 2, 3, 2, 1]'''\n",
    "        correct += (predictions[mask] == labels[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation using the DataLoader\n",
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847155e-b5eb-4ccc-9889-42cf9632d29d",
   "metadata": {},
   "source": [
    "#### Finally, defining the training arguments for the trainer and initialzing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c482878-d4ac-422c-ae37-136ee3c290d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4407' max='4407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4407/4407 2:14:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.087213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.092754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.093473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4407, training_loss=0.08793052654296653, metrics={'train_runtime': 8099.8094, 'train_samples_per_second': 4.351, 'train_steps_per_second': 0.544, 'total_flos': 4608567785226240.0, 'train_loss': 0.08793052654296653, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pos_tagging_model\", # 模型和检查点的保存目录\n",
    "    evaluation_strategy=\"epoch\", # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",  # 每个 epoch 结束后保存模型\n",
    "    per_device_train_batch_size=8,  # 每个 epoch 结束后保存模型\n",
    "    per_device_eval_batch_size=8, # 每个设备的评估批次大小\n",
    "    num_train_epochs=3, # 训练的总 epoch 数\n",
    "    weight_decay=0.01, # L2 正则化系数\n",
    "    save_total_limit=2, # 最多保存的检查点数量\n",
    "    report_to=\"none\", # 不使用任何报告工具（如 TensorBoard）\n",
    "    logging_strategy=\"epoch\" # 每个 epoch 结束后记录日志\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # 要训练的模型\n",
    "    args=training_args, # 上面定义的训练参数\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc065870-c27f-42d8-abfb-8be6db70658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.9762\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de610956-45f4-4ad7-922f-f4f8189fc721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='182' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [182/182 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09347294270992279,\n",
       " 'eval_runtime': 81.5145,\n",
       " 'eval_samples_per_second': 17.801,\n",
       " 'eval_steps_per_second': 2.233,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "# evaluate() 方法在评估数据集（通常是测试集）上运行模型，并计算各种性能指标。\n",
    "# 它返回一个字典，其中包含评估指标的名称和对应的值。\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7290bc-4c7d-4cdc-98f4-2b65001b3f77",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d59546a-9676-4232-ab80-b0e20dc4b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9436973d-5060-46fb-98c4-3575eeceea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d70ab7428b14b6ba5a462a3d4c761e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fb8f5c741f4488b2668ec86b77ade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff2e3aab8e04ef4b21583fa0ac3bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c54539fefdd4afc9f9b4ec940ac8050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e85bfb659b4d55a25f52caf473419c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666a762124554b369499b8340d0e6dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d410ea6e541358582a792016aeabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632ca242bb3742fc94f44d9bb5dbf76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ddefe7f3b743f1a73a7e6eeb345124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model microsoft/Phi-3.5-mini-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.phi3.modeling_phi3.Phi3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\nwhile loading with Phi3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the pipeline\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 这是Hugging Face Transformers库中的一个高级API，用于简化模型的使用。\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 它自动处理模型加载、tokenization和推理过程。\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 指定了pipeline的任务类型为文本生成。\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 这意味着模型将被用于生成连续的文本。\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 指定了pipeline的任务类型为文本生成。\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 这意味着模型将被用于生成连续的文本。\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3.5-mini-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Trust the remote code; this is required for some models, but always check the code first!\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set this to \"cuda\" for GPU acceleration if available\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 for less memory usage and faster inference\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model microsoft/Phi-3.5-mini-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.phi3.modeling_phi3.Phi3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\nwhile loading with Phi3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\n\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "# 这是Hugging Face Transformers库中的一个高级API，用于简化模型的使用。\n",
    "# 它自动处理模型加载、tokenization和推理过程。\n",
    "# 指定了pipeline的任务类型为文本生成。\n",
    "# 这意味着模型将被用于生成连续的文本。\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    # 指定了pipeline的任务类型为文本生成。\n",
    "    # 这意味着模型将被用于生成连续的文本。\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=\"cpu\",  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5332c6a7-46b6-44b7-9d42-a81b9506e4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'处理过程：\\n\\n将数字ID序列映射回对应的token（单词或子词）。\\n将这些token拼接成一个连续的字符串。\\n移除所有的特殊token（如果 skip_special_tokens=True）。\\n处理任何需要的后处理步骤（如合并分割的单词）。\\n返回值：\\n\\n一个字符串，包含解码后的文本。\\n用途：\\n\\n在文本生成任务中，这通常是获取最终可读输出的最后一步。\\n它将模型的数值输出转换为人类可理解的文本。\\n示例： 假设 outputs[0] 是 [7993, 2000, 10169, 58, 7592, 11, 2129, 2024, 2017, 30] \\n解码后可能得到类似 \"Translate to French: Hello, how are you?\" 的文本。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    # Tokenize input\n",
    "    # 指定返回的结果应该是PyTorch张量。\n",
    "    # \"pt\"代表PyTorch。其他选项包括\"tf\"（TensorFlow）或\"np\"（NumPy数组）。\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move ALL input tensors to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Manually create position IDs and move them to the correct device\n",
    "    # 获取输入序列的长度。\n",
    "    # inputs[\"input_ids\"] 是一个形状为 (batch_size, sequence_length) 的张量。\n",
    "    # .shape[1] 取第二个维度，即序列长度。\n",
    "    # torch.arange(...):\n",
    "    # 创建一个从 0 到 (序列长度 - 1) 的整数序列。\n",
    "    # 例如，如果序列长度是 5，这将创建 [0, 1, 2, 3, 4]。\n",
    "    # 指定生成的张量数据类型为长整型\n",
    "    position_ids = torch.arange(inputs[\"input_ids\"].shape[1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate output while ensuring ALL tensors are on the correct device\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    # tokenizer.decode():\n",
    "    # 这是tokenizer的一个方法，用于将数字token ID序列转换回原始文本。\n",
    "    # 它执行的是tokenization的逆操作。\n",
    "    # 这个参数指示解码过程中要跳过特殊token。\n",
    "    # 特殊token包括如 [PAD], [CLS], [SEP], [MASK] 等，这些通常不应该出现在最终的文本输出中。\n",
    "    # 设置为 True 可以得到更干净、更可读的输出。\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "'''处理过程：\n",
    "\n",
    "将数字ID序列映射回对应的token（单词或子词）。\n",
    "将这些token拼接成一个连续的字符串。\n",
    "移除所有的特殊token（如果 skip_special_tokens=True）。\n",
    "处理任何需要的后处理步骤（如合并分割的单词）。\n",
    "返回值：\n",
    "\n",
    "一个字符串，包含解码后的文本。\n",
    "用途：\n",
    "\n",
    "在文本生成任务中，这通常是获取最终可读输出的最后一步。\n",
    "它将模型的数值输出转换为人类可理解的文本。\n",
    "示例： 假设 outputs[0] 是 [7993, 2000, 10169, 58, 7592, 11, 2129, 2024, 2017, 30] \n",
    "解码后可能得到类似 \"Translate to French: Hello, how are you?\" 的文本。'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d8735d-a14b-4ee0-9a18-d9508feca1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate text for each prompt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m(prompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Example prompts to test model behavior\n",
    "prompts = [\n",
    "    \"Write a short story about a robot discovering emotions.\",\n",
    "    \"Explain how transformers work in deep learning.\",\n",
    "    \"You are a helpful assistant. Answer: What is deep learning?\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    output = pipe(prompt, max_new_tokens=100)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated Output: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986f6ce-b5fa-421e-8f85-17647fdfdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Output with Custom Parameters:\n",
      "Explain quantum physics in simple terms.\n",
      "\n",
      "## Answer:\n",
      "Quantum physics is a branch of science that studies the behavior of the smallest particles in the universe, like atoms and photons (which are particles of light). Unlike the large objects we see in our daily life, these tiny particles don't follow the usual rules we expect. Here are some key points about quantum physics:\n",
      "\n",
      "1. **Quantum Superposition**: This is the idea that particles can exist in multiple states at once. For example,\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Explain quantum physics in simple terms.\",\n",
    "    max_new_tokens=100,  # Control text length\n",
    "    temperature=0.7,  # Higher values = more randomness\n",
    "    top_p=0.9,  # Use nucleus sampling for diversity\n",
    "    do_sample=True  # Enable sampling instead of greedy decoding\n",
    ")\n",
    "'''top_p=0.9:\n",
    "这是核采样（nucleus sampling）的参数，也称为 top-p 采样。\n",
    "它只考虑累积概率达到 top_p 的最可能的 token。\n",
    "0.9 意味着模型将从累积概率达到 90% 的最可能 token 中进行采样。\n",
    "这有助于保持多样性，同时避免选择非常不可能的 token。\n",
    "\n",
    "do_sample=True:\n",
    "启用采样而不是贪婪解码。\n",
    "当设置为 True 时，模型会根据概率分布进行采样，而不是总是选择最可能的下一个 token。\n",
    "这允许生成更多样化的文本，而不是每次都产生相同的输出。\n",
    "'''\n",
    "print(\"\\nGenerated Output with Custom Parameters:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d355bca-5c76-402b-a2e0-f8edc1e21309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the following sentences from English to French:\n",
      "\n",
      "1. I love programming.\n",
      "2. The weather is beautiful. \n",
      "3. I love deep learning. \n",
      "4. The sky is clear tonight.\n",
      "5. I am learning French.\n",
      "\n",
      "# Answer\n",
      "\n",
      "1. J'adore programmer.\n",
      "2. Le temps est magnifique.\n",
      "3. J'adore l'apprentissage profond.\n",
      "4. Le ciel est dégagé ce soir.\n",
      "5. J'apprends le français.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Translate the following sentences from English to French:\n",
    "\n",
    "1. I love programming.\n",
    "2. The weather is beautiful. \n",
    "3. I love deep learning. \"\"\"\n",
    "'''output 是pipeline返回的结果，通常是一个列表。\n",
    "output[0] 取第一个（也可能是唯一的）生成结果。\n",
    "['generated_text'] 访问生成的文本内容。'''\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n",
    "'''预期输出：\n",
    "\n",
    "输出将包括原始的提示文本（英语句子）以及模型生成的法语翻译。\n",
    "由于使用了zero-shot方法，模型是在没有特定翻译示例的情况下尝试完成任务的。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d777dcb-fb03-4618-9f7e-c443d0b7be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the third sentence from English to French, \n",
      "the first two are examples on how to translate them:\n",
      "\n",
      "1. I love programming. - J’aime la programmation.\n",
      "2. The weather is beautiful. - Il fait beau.\n",
      "3. I love deep learning. - \n",
      "\n",
      "# Answer\n",
      "Je t'aime l'apprentissage profond.\n",
      "\n",
      "Note: In French, possessive structures can be a bit different from English. Instead of\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Translate the third sentence from English to French, \n",
    "the first two are examples on how to translate them:\n",
    "\n",
    "1. I love programming. - J’aime la programmation.\n",
    "2. The weather is beautiful. - Il fait beau.\n",
    "3. I love deep learning. -\"\"\"\n",
    "\n",
    "output = pipe(few_shot_prompt, max_new_tokens=40)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f138c50-cb64-4c53-9f60-d3e63a174fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Learning Output:\n",
      "Summarize the following article in one sentence: \n",
      "An ANN consists of connected units or nodes called artificial neurons, \n",
      "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
      "biological neurons more closely have also been recently investigated and shown \n",
      "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
      "Each artificial neuron receives signals from connected neurons, then processes them and \n",
      "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
      "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
      "called the activation function. The strength of the signal at each connection is \n",
      "determined by a weight, which adjusts during the learning process. \n",
      "Typically, neurons are aggregated into layers. Different layers may perform \n",
      "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
      "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
      "A network is typically called a deep neural network if it has at least two hidden layers. \n",
      "Artificial neural networks are used for various tasks, including predictive modeling, \n",
      "adaptive control, and solving problems in artificial intelligence. \n",
      "They can learn from experience, and can derive conclusions from a complex and \n",
      "seemingly unrelated set of information.\n",
      "\n",
      "ANNs are computational models inspired by the human brain, consisting of interconnected nodes or neurons that process and transmit information through weighted connections, with the ability to learn and adapt from data.\n",
      "\n",
      "\n",
      "## Response: ANNs are computational models that mimic the brain's neurons and synapses, process information through interconnected nodes, and learn from data.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Summarize the following article in one sentence: \n",
    "An ANN consists of connected units or nodes called artificial neurons, \n",
    "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
    "biological neurons more closely have also been recently investigated and shown \n",
    "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
    "Each artificial neuron receives signals from connected neurons, then processes them and \n",
    "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
    "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
    "called the activation function. The strength of the signal at each connection is \n",
    "determined by a weight, which adjusts during the learning process. \n",
    "Typically, neurons are aggregated into layers. Different layers may perform \n",
    "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
    "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
    "A network is typically called a deep neural network if it has at least two hidden layers. \n",
    "Artificial neural networks are used for various tasks, including predictive modeling, \n",
    "adaptive control, and solving problems in artificial intelligence. \n",
    "They can learn from experience, and can derive conclusions from a complex and \n",
    "seemingly unrelated set of information.\"\"\"\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "\n",
    "print(\"\\nZero-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
