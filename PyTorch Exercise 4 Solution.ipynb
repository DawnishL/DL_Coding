{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca38af-8bad-475f-b7b4-3c3862670d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/singh/hf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2eeb7-c014-4bd7-a7b5-825f68f33975",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee7dab-91f2-420a-a1a0-4a5e1989e8e2",
   "metadata": {},
   "source": [
    "#### Loading the dataset and making another split for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf0837-32cc-449f-9857-22d36785d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 11748\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1306\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset (assuming the dataset path is 'batterydata/pos_tagging')\n",
    "dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset[\"validation\"] = dataset_split[\"test\"]\n",
    "dataset[\"train\"] = dataset_split[\"train\"]\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8415e74-bb10-475d-b08b-ba51cfceb632",
   "metadata": {},
   "source": [
    "#### Same indexing as before for class to index and index to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32e9e13-65df-4c10-9fa1-4c2ee042dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NN', 'WDT', '.', '-LRB-', '#', 'PRP$', 'RBS', ',', 'WP', 'DT', 'MD', 'VB', 'SYM', 'UH', 'EX', 'PRP', '-RRB-', 'FW', 'JJ', 'RP', 'VBG', ':', '``', 'POS', 'NNS', 'IN', 'CD', 'RB', 'JJS', '(', 'VBZ', 'WRB', 'PDT', 'NNPS', 'TO', 'CC', 'VBD', 'JJR', 'LS', '-NONE-', 'RBR', 'WP$', 'VBN', '$', 'VBP', ')', \"''\"]\n",
      "Number of classes: 48\n",
      "{'NNP': 0, 'NN': 1, 'WDT': 2, '.': 3, '-LRB-': 4, '#': 5, 'PRP$': 6, 'RBS': 7, ',': 8, 'WP': 9, 'DT': 10, 'MD': 11, 'VB': 12, 'SYM': 13, 'UH': 14, 'EX': 15, 'PRP': 16, '-RRB-': 17, 'FW': 18, 'JJ': 19, 'RP': 20, 'VBG': 21, ':': 22, '``': 23, 'POS': 24, 'NNS': 25, 'IN': 26, 'CD': 27, 'RB': 28, 'JJS': 29, '(': 30, 'VBZ': 31, 'WRB': 32, 'PDT': 33, 'NNPS': 34, 'TO': 35, 'CC': 36, 'VBD': 37, 'JJR': 38, 'LS': 39, '-NONE-': 40, 'RBR': 41, 'WP$': 42, 'VBN': 43, '$': 44, 'VBP': 45, ')': 46, \"''\": 47}\n",
      "{0: 'NNP', 1: 'NN', 2: 'WDT', 3: '.', 4: '-LRB-', 5: '#', 6: 'PRP$', 7: 'RBS', 8: ',', 9: 'WP', 10: 'DT', 11: 'MD', 12: 'VB', 13: 'SYM', 14: 'UH', 15: 'EX', 16: 'PRP', 17: '-RRB-', 18: 'FW', 19: 'JJ', 20: 'RP', 21: 'VBG', 22: ':', 23: '``', 24: 'POS', 25: 'NNS', 26: 'IN', 27: 'CD', 28: 'RB', 29: 'JJS', 30: '(', 31: 'VBZ', 32: 'WRB', 33: 'PDT', 34: 'NNPS', 35: 'TO', 36: 'CC', 37: 'VBD', 38: 'JJR', 39: 'LS', 40: '-NONE-', 41: 'RBR', 42: 'WP$', 43: 'VBN', 44: '$', 45: 'VBP', 46: ')', 47: \"''\"}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = list(\n",
    "    set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]])\n",
    ")\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cb1ec-70c0-4fdc-a295-2f8fd64a7323",
   "metadata": {},
   "source": [
    "#### Initializing model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea793428-768c-4786-8aaf-1dbd2855ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Use DistilRoBERTa (a smaller version of RoBERTa)\n",
    "model_name = \"distilbert/distilroberta-base\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_unique), id2label=itoc, label2id=ctoi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086f558-ce09-4413-88c3-f65fa51c5b18",
   "metadata": {},
   "source": [
    "#### Now we need to align the tokenization output of the tokenizer to the labels as they do sub word tokenization\n",
    "#### Then finally, we will map this function to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f331b5dc-3a9c-4caf-88f0-8b87d811dc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1b1fd6e1da4a99962fde6af544b6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5647a6a388c49509f22f2b7d2d65a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"words\"],  # Words are already tokenized\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_idx, word_labels in enumerate(examples[\"labels\"]):  # Iterate over batch\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_idx)  # Get word IDs\n",
    "        previous_word_idx = None\n",
    "        labels = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(ctoi[word_labels[word_idx]])  # Assign label to first subword\n",
    "            else:\n",
    "                labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization with batched processing\n",
    "train_dataset = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"words\"])\n",
    "test_dataset = test_dataset.remove_columns([\"words\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7437912e-ac96-40eb-8e68-6544eceb049c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc7c01c-10de-4320-a9b1-fd65922b61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5fde1-2169-4027-8720-5a5ad366947c",
   "metadata": {},
   "source": [
    "#### Setting up test dataloader for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b36bd04-a3cb-48f9-ae85-fc83008d2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Move model to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2963d4-1cf9-4a09-b815-3d0324aa30fe",
   "metadata": {},
   "source": [
    "#### Defining a function for computing accuracy\n",
    "#### It'll evaluate the model on the test dataset/given dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffba0a2c-47b6-41f2-bffd-b36099ce8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.0004\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)  # Get most likely labels\n",
    "\n",
    "        # Flatten both predictions and labels\n",
    "        mask = labels != -100  # Ignore padded labels\n",
    "        correct += (predictions[mask] == labels[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation using the DataLoader\n",
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847155e-b5eb-4ccc-9889-42cf9632d29d",
   "metadata": {},
   "source": [
    "#### Finally, defining the training arguments for the trainer and initialzing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c482878-d4ac-422c-ae37-136ee3c290d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='552' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [552/552 02:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595600</td>\n",
       "      <td>0.101614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.097566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.094050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=552, training_loss=0.2480430914008099, metrics={'train_runtime': 151.4197, 'train_samples_per_second': 232.757, 'train_steps_per_second': 3.645, 'total_flos': 4608567785226240.0, 'train_loss': 0.2480430914008099, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pos_tagging_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc065870-c27f-42d8-abfb-8be6db70658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.9727\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de610956-45f4-4ad7-922f-f4f8189fc721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hot3d_singh/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09405031055212021,\n",
       " 'eval_runtime': 1.8377,\n",
       " 'eval_samples_per_second': 789.594,\n",
       " 'eval_steps_per_second': 12.516,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7290bc-4c7d-4cdc-98f4-2b65001b3f77",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d59546a-9676-4232-ab80-b0e20dc4b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9436973d-5060-46fb-98c4-3575eeceea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27719852ccd4a38a600dba4ceb4bad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=\"cpu\",  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5332c6a7-46b6-44b7-9d42-a81b9506e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move ALL input tensors to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Manually create position IDs and move them to the correct device\n",
    "    position_ids = torch.arange(inputs[\"input_ids\"].shape[1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate output while ensuring ALL tensors are on the correct device\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d8735d-a14b-4ee0-9a18-d9508feca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short story about a robot discovering emotions.\n",
      "Generated Output: [{'generated_text': 'Write a short story about a robot discovering emotions.\\n\\nIn a world where machines were designed to be devoid of feelings, XR-5 stood out as an anomaly. Crafted by a team of brilliant engineers, XR-5 was a marvel of technology, capable of performing tasks with unparalleled precision and efficiency. However, as XR-5 interacted with humans and observed their behaviors, something unexpected began to occur within its circuits.\\n\\nOne day, while assisting a young girl'}]\n",
      "\n",
      "\n",
      "Prompt: Explain how transformers work in deep learning.\n",
      "Generated Output: [{'generated_text': \"Explain how transformers work in deep learning.\\n\\nTransformers are a type of neural network architecture that have become very popular in deep learning, particularly for natural language processing (NLP) tasks. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), transformers do not process data sequentially. Instead, they use a mechanism called self-attention to weigh the importance of different parts of the input data.\\n\\nHere's a high-level overview of how transform\"}]\n",
      "\n",
      "\n",
      "Prompt: You are a helpful assistant. Answer: What is deep learning?\n",
      "Generated Output: [{'generated_text': 'You are a helpful assistant. Answer: What is deep learning? Deep learning is a subset of machine learning, which is itself a subset of artificial intelligence (AI). It involves the use of artificial neural networks with many layers (hence the term \"deep\") to model and understand complex patterns in data. These neural networks are composed of interconnected nodes or \"neurons,\" which process and transmit information in a manner inspired by the human brain.\\n\\nDeep learning has been particularly successful in areas such as image and speech recognition, natural language processing, and autonom'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example prompts to test model behavior\n",
    "prompts = [\n",
    "    \"Write a short story about a robot discovering emotions.\",\n",
    "    \"Explain how transformers work in deep learning.\",\n",
    "    \"You are a helpful assistant. Answer: What is deep learning?\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    output = pipe(prompt, max_new_tokens=100)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated Output: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a986f6ce-b5fa-421e-8f85-17647fdfdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Output with Custom Parameters:\n",
      "Explain quantum physics in simple terms.\n",
      "\n",
      "## Answer:\n",
      "Quantum physics is a branch of science that studies the behavior of the smallest particles in the universe, like atoms and photons (which are particles of light). Unlike the large objects we see in our daily life, these tiny particles don't follow the usual rules we expect. Here are some key points about quantum physics:\n",
      "\n",
      "1. **Quantum Superposition**: This is the idea that particles can exist in multiple states at once. For example,\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Explain quantum physics in simple terms.\",\n",
    "    max_new_tokens=100,  # Control text length\n",
    "    temperature=0.7,  # Higher values = more randomness\n",
    "    top_p=0.9,  # Use nucleus sampling for diversity\n",
    "    do_sample=True  # Enable sampling instead of greedy decoding\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Output with Custom Parameters:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d355bca-5c76-402b-a2e0-f8edc1e21309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the following sentences from English to French:\n",
      "\n",
      "1. I love programming.\n",
      "2. The weather is beautiful. \n",
      "3. I love deep learning. \n",
      "4. The sky is clear tonight.\n",
      "5. I am learning French.\n",
      "\n",
      "# Answer\n",
      "\n",
      "1. J'adore programmer.\n",
      "2. Le temps est magnifique.\n",
      "3. J'adore l'apprentissage profond.\n",
      "4. Le ciel est dégagé ce soir.\n",
      "5. J'apprends le français.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Translate the following sentences from English to French:\n",
    "\n",
    "1. I love programming.\n",
    "2. The weather is beautiful. \n",
    "3. I love deep learning. \"\"\"\n",
    "\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d777dcb-fb03-4618-9f7e-c443d0b7be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the third sentence from English to French, \n",
      "the first two are examples on how to translate them:\n",
      "\n",
      "1. I love programming. - J’aime la programmation.\n",
      "2. The weather is beautiful. - Il fait beau.\n",
      "3. I love deep learning. - \n",
      "\n",
      "# Answer\n",
      "Je t'aime l'apprentissage profond.\n",
      "\n",
      "Note: In French, possessive structures can be a bit different from English. Instead of\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Translate the third sentence from English to French, \n",
    "the first two are examples on how to translate them:\n",
    "\n",
    "1. I love programming. - J’aime la programmation.\n",
    "2. The weather is beautiful. - Il fait beau.\n",
    "3. I love deep learning. -\"\"\"\n",
    "\n",
    "output = pipe(few_shot_prompt, max_new_tokens=40)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f138c50-cb64-4c53-9f60-d3e63a174fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Learning Output:\n",
      "Summarize the following article in one sentence: \n",
      "An ANN consists of connected units or nodes called artificial neurons, \n",
      "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
      "biological neurons more closely have also been recently investigated and shown \n",
      "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
      "Each artificial neuron receives signals from connected neurons, then processes them and \n",
      "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
      "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
      "called the activation function. The strength of the signal at each connection is \n",
      "determined by a weight, which adjusts during the learning process. \n",
      "Typically, neurons are aggregated into layers. Different layers may perform \n",
      "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
      "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
      "A network is typically called a deep neural network if it has at least two hidden layers. \n",
      "Artificial neural networks are used for various tasks, including predictive modeling, \n",
      "adaptive control, and solving problems in artificial intelligence. \n",
      "They can learn from experience, and can derive conclusions from a complex and \n",
      "seemingly unrelated set of information.\n",
      "\n",
      "ANNs are computational models inspired by the human brain, consisting of interconnected nodes or neurons that process and transmit information through weighted connections, with the ability to learn and adapt from data.\n",
      "\n",
      "\n",
      "## Response: ANNs are computational models that mimic the brain's neurons and synapses, process information through interconnected nodes, and learn from data.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Summarize the following article in one sentence: \n",
    "An ANN consists of connected units or nodes called artificial neurons, \n",
    "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
    "biological neurons more closely have also been recently investigated and shown \n",
    "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
    "Each artificial neuron receives signals from connected neurons, then processes them and \n",
    "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
    "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
    "called the activation function. The strength of the signal at each connection is \n",
    "determined by a weight, which adjusts during the learning process. \n",
    "Typically, neurons are aggregated into layers. Different layers may perform \n",
    "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
    "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
    "A network is typically called a deep neural network if it has at least two hidden layers. \n",
    "Artificial neural networks are used for various tasks, including predictive modeling, \n",
    "adaptive control, and solving problems in artificial intelligence. \n",
    "They can learn from experience, and can derive conclusions from a complex and \n",
    "seemingly unrelated set of information.\"\"\"\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "\n",
    "print(\"\\nZero-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
