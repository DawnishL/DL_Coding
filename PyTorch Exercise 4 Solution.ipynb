{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dca38af-8bad-475f-b7b4-3c3862670d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/singh/hf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2eeb7-c014-4bd7-a7b5-825f68f33975",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee7dab-91f2-420a-a1a0-4a5e1989e8e2",
   "metadata": {},
   "source": [
    "#### Loading the dataset and making another split for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf0837-32cc-449f-9857-22d36785d36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9528dd4267e34e0784f22766c05c9f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\datasets--batterydata--pos_tagging. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998851f04ddb49f58f6820a2dca02e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/5.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272a454c84c2489684d44022b6cf1ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/601k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade21de069414daaa6d552db1403f4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096ec804181f4f47a6684e3c9c39b319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 11748\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1306\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset (assuming the dataset path is 'batterydata/pos_tagging')\n",
    "dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset[\"validation\"] = dataset_split[\"test\"]\n",
    "dataset[\"train\"] = dataset_split[\"train\"]\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8415e74-bb10-475d-b08b-ba51cfceb632",
   "metadata": {},
   "source": [
    "#### Same indexing as before for class to index and index to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32e9e13-65df-4c10-9fa1-4c2ee042dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBS', 'CC', 'CD', 'PRP', 'VB', 'LS', 'JJ', '-LRB-', 'NNS', '``', \"''\", 'JJR', 'JJS', '(', ')', 'WP', 'IN', 'UH', 'SYM', 'POS', 'NNP', '-RRB-', 'VBN', '$', ',', 'WDT', 'PDT', 'WRB', 'TO', 'RP', 'DT', '-NONE-', ':', 'RB', 'PRP$', 'VBG', 'RBR', 'MD', 'NN', 'EX', 'VBD', 'FW', 'VBP', 'NNPS', 'WP$', '#', 'VBZ', '.']\n",
      "Number of classes: 48\n",
      "{'RBS': 0, 'CC': 1, 'CD': 2, 'PRP': 3, 'VB': 4, 'LS': 5, 'JJ': 6, '-LRB-': 7, 'NNS': 8, '``': 9, \"''\": 10, 'JJR': 11, 'JJS': 12, '(': 13, ')': 14, 'WP': 15, 'IN': 16, 'UH': 17, 'SYM': 18, 'POS': 19, 'NNP': 20, '-RRB-': 21, 'VBN': 22, '$': 23, ',': 24, 'WDT': 25, 'PDT': 26, 'WRB': 27, 'TO': 28, 'RP': 29, 'DT': 30, '-NONE-': 31, ':': 32, 'RB': 33, 'PRP$': 34, 'VBG': 35, 'RBR': 36, 'MD': 37, 'NN': 38, 'EX': 39, 'VBD': 40, 'FW': 41, 'VBP': 42, 'NNPS': 43, 'WP$': 44, '#': 45, 'VBZ': 46, '.': 47}\n",
      "{0: 'RBS', 1: 'CC', 2: 'CD', 3: 'PRP', 4: 'VB', 5: 'LS', 6: 'JJ', 7: '-LRB-', 8: 'NNS', 9: '``', 10: \"''\", 11: 'JJR', 12: 'JJS', 13: '(', 14: ')', 15: 'WP', 16: 'IN', 17: 'UH', 18: 'SYM', 19: 'POS', 20: 'NNP', 21: '-RRB-', 22: 'VBN', 23: '$', 24: ',', 25: 'WDT', 26: 'PDT', 27: 'WRB', 28: 'TO', 29: 'RP', 30: 'DT', 31: '-NONE-', 32: ':', 33: 'RB', 34: 'PRP$', 35: 'VBG', 36: 'RBR', 37: 'MD', 38: 'NN', 39: 'EX', 40: 'VBD', 41: 'FW', 42: 'VBP', 43: 'NNPS', 44: 'WP$', 45: '#', 46: 'VBZ', 47: '.'}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = list(\n",
    "    set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]])\n",
    ")\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cb1ec-70c0-4fdc-a295-2f8fd64a7323",
   "metadata": {},
   "source": [
    "#### Initializing model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea793428-768c-4786-8aaf-1dbd2855ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba63b6d3fc5e42c1ab560d54a14752a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df856d20f9542668f3190b685ff32e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d721378dd419f96b277503f404e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d812aaeaa23a427b90fb702a0c04c718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da38d08ce8648c6b88f4f9c2d9b69fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1756b2bd95f345a09b39245d4f51f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57dca058eb74010ac42a26d2b568685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Use DistilRoBERTa (a smaller version of RoBERTa)\n",
    "model_name = \"distilbert/distilroberta-base\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "# add_prefix_space=True å‚æ•°ç¡®ä¿åœ¨åˆ†è¯æ—¶åœ¨å•è¯å‰æ·»åŠ ç©ºæ ¼\n",
    "# num_labels: æŒ‡å®šäº†åˆ†ç±»æ ‡ç­¾çš„æ•°é‡ï¼Œä½¿ç”¨ len(labels_unique) è®¡ç®—\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(labels_unique), id2label=itoc, label2id=ctoi)\n",
    "# è¿™æ®µä»£ç çš„ç›®çš„æ˜¯ä¸ºtokenåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚å‘½åå®ä½“è¯†åˆ«ï¼‰å‡†å¤‡æ¨¡å‹å’Œtokenizerã€‚\n",
    "# é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„DistilRoBERTaæ¨¡å‹ï¼Œä½ å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”ç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086f558-ce09-4413-88c3-f65fa51c5b18",
   "metadata": {},
   "source": [
    "#### Now we need to align the tokenization output of the tokenizer to the labels as they do sub word tokenization\n",
    "#### Then finally, we will map this function to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331b5dc-3a9c-4caf-88f0-8b87d811dc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fbf8ecfacb48ee97df9fd5a7fc100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba08793785124dbcbeb3537c9fcb3599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"words\"],  # Words are already tokenized\n",
    "        truncation=True, # å…è®¸æˆªæ–­è¿‡é•¿çš„åºåˆ—\n",
    "        padding=\"max_length\", # å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
    "        is_split_into_words=True # è¡¨æ˜è¾“å…¥å·²ç»æŒ‰è¯åˆ†å‰²\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_idx, word_labels in enumerate(examples[\"labels\"]):  # Iterate over batch\n",
    "        # word_ids() æ˜¯ tokenized_inputs å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ã€‚\n",
    "        # è¿™ä¸ªæ–¹æ³•è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ª token å¯¹åº”çš„åŸå§‹å•è¯çš„ç´¢å¼•\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_idx)  # Get word IDs\n",
    "        # previous_word_idx ç”¨äºè·Ÿè¸ªä¸Šä¸€ä¸ªå¤„ç†çš„è¯çš„ç´¢å¼•ã€‚è¿™åœ¨å¤„ç†å­è¯ï¼ˆsubwordsï¼‰æ—¶ç‰¹åˆ«é‡è¦\n",
    "        previous_word_idx = None\n",
    "        labels = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  # Ignore special tokens\n",
    "                # å½“ word_idx != previous_word_idx æ—¶ï¼Œè¡¨ç¤ºé‡åˆ°äº†ä¸€ä¸ªæ–°è¯çš„å¼€å§‹ï¼Œæ­¤æ—¶æˆ‘ä»¬ä¸ºè¿™ä¸ªæ–°è¯åˆ†é…æ ‡ç­¾\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(ctoi[word_labels[word_idx]])  # Assign label to first subword\n",
    "            # å¦‚æœ word_idx == previous_word_idxï¼ˆä¸”ä¸ä¸º Noneï¼‰ï¼Œè¯´æ˜å½“å‰ token æ˜¯å‰ä¸€ä¸ªè¯çš„å­è¯ï¼Œ\n",
    "            # æˆ‘ä»¬é€šå¸¸ä¼šå¿½ç•¥å®ƒï¼ˆåœ¨ NER ä»»åŠ¡ä¸­ç»™äºˆç‰¹æ®Šæ ‡è®° -100ï¼‰ã€‚\n",
    "            else:\n",
    "                labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "        '''é‡è¦æ€§ï¼š\n",
    "\n",
    "åœ¨å¤„ç†ä½¿ç”¨å­è¯ tokenization çš„æ¨¡å‹ï¼ˆå¦‚ BERTã€RoBERTa ç­‰ï¼‰æ—¶ï¼Œè¿™ç§æ–¹æ³•éå¸¸é‡è¦ã€‚\n",
    "å®ƒç¡®ä¿äº†åœ¨è¯è¢«åˆ†å‰²æˆå¤šä¸ª token çš„æƒ…å†µä¸‹ï¼Œåªæœ‰ç¬¬ä¸€ä¸ª token è·å¾—å®é™…çš„æ ‡ç­¾ï¼Œè€Œå…¶ä»–çš„å­è¯ token è¢«é€‚å½“åœ°å¿½ç•¥ã€‚\n",
    "è¿™ç§å¤„ç†æ–¹å¼æœ‰åŠ©äºä¿æŒåŸå§‹æ ‡æ³¨çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶é€‚åº”å­è¯ tokenization çš„ç‰¹æ€§ã€‚'''\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "'''tokenize_and_align_labels å‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªç»è¿‡å¤„ç†çš„å­—å…¸ï¼ˆdictionaryï¼‰ï¼Œ\n",
    "åŒ…å«äº†tokenizedçš„è¾“å…¥å’Œå¯¹é½åçš„æ ‡ç­¾ã€‚\n",
    "è®©æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸ªå‡½æ•°çš„è¿”å›å€¼ã€‚å…¶ä¸­ï¼Œlabels å­—æ®µæ˜¯å‡½æ•°å¤„ç†çš„å…³é”®éƒ¨åˆ†ã€‚\n",
    "å®ƒåŒ…å«äº†ä¸ input_ids é•¿åº¦ç›¸åŒçš„æ ‡ç­¾åˆ—è¡¨ã€‚\n",
    "å¯¹äºæ¯ä¸ªtokenï¼Œå¦‚æœå®ƒæ˜¯ä¸€ä¸ªè¯çš„ç¬¬ä¸€ä¸ªå­è¯ï¼Œä¼šè¢«èµ‹äºˆç›¸åº”çš„æ ‡ç­¾ã€‚\n",
    "å¯¹äºå­è¯ï¼ˆéè¯é¦–tokenï¼‰å’Œç‰¹æ®Štokenï¼ˆå¦‚[CLS], [SEP]ï¼‰ï¼Œæ ‡ç­¾è¢«è®¾ç½®ä¸º -100ã€‚'''\n",
    "# Apply tokenization with batched processing\n",
    "# å¯¹äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼ˆæˆ–æ¯æ‰¹æ ·æœ¬ï¼‰ï¼Œmap æ–¹æ³•ä¼šè°ƒç”¨ tokenize_and_align_labels å‡½æ•°ã€‚\n",
    "# è¿™ä¸ªå‡½æ•°ä¼šå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶å°†åŸå§‹çš„è¯çº§åˆ«æ ‡ç­¾å¯¹é½åˆ° token çº§åˆ«ã€‚\n",
    "# å¤„ç†åçš„æ•°æ®å°†åŒ…å«åˆ†è¯åçš„è¾“å…¥å’Œå¯¹åº”çš„ token çº§åˆ«æ ‡ç­¾ã€‚\n",
    "train_dataset = dataset[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"words\"])\n",
    "test_dataset = test_dataset.remove_columns([\"words\"])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# è¿™æ˜¯Hugging Face datasetsåº“ä¸­Datasetå¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ã€‚\n",
    "# å®ƒç”¨äºæŒ‡å®šæ•°æ®é›†åº”è¯¥ä»¥ä½•ç§æ ¼å¼è¿”å›æ•°æ®ã€‚\n",
    "# \"torch\" å‚æ•°ï¼šæŒ‡å®šè¦å°†æ•°æ®è½¬æ¢ä¸ºPyTorchå¼ é‡æ ¼å¼ã€‚\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7437912e-ac96-40eb-8e68-6544eceb049c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc7c01c-10de-4320-a9b1-fd65922b61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5fde1-2169-4027-8720-5a5ad366947c",
   "metadata": {},
   "source": [
    "#### Setting up test dataloader for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b36bd04-a3cb-48f9-ae85-fc83008d2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Move model to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2963d4-1cf9-4a09-b815-3d0324aa30fe",
   "metadata": {},
   "source": [
    "#### Defining a function for computing accuracy\n",
    "#### It'll evaluate the model on the test dataset/given dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba0a2c-47b6-41f2-bffd-b36099ce8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.0110\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # outputs.logits:\n",
    "\n",
    "            # è¿™æ˜¯æ¨¡å‹è¾“å‡ºçš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ã€‚\n",
    "            # å½¢çŠ¶é€šå¸¸ä¸º (batch_size, sequence_length, num_classes)ã€‚\n",
    "            # æ¯ä¸ªå…ƒç´ ä»£è¡¨æ¨¡å‹å¯¹æ¯ä¸ªtokenå±äºæ¯ä¸ªç±»åˆ«çš„é¢„æµ‹åˆ†æ•°ï¼ˆæˆ–logitsï¼‰ã€‚\n",
    "            # logits æ˜¯æ¨¡å‹ç¥ç»ç½‘ç»œæœ€åä¸€å±‚ï¼ˆé€šå¸¸æ˜¯å…¨è¿æ¥å±‚ï¼‰çš„åŸå§‹è¾“å‡ºã€‚\n",
    "            # å®ƒä»¬æ˜¯æœªç»è¿‡softmaxæˆ–å…¶ä»–æ¿€æ´»å‡½æ•°å¤„ç†çš„åŸå§‹scoresã€‚\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)  # Get most likely labels\n",
    "\n",
    "        # Flatten both predictions and labels\n",
    "        # è¿™è¡Œåˆ›å»ºäº†ä¸€ä¸ªå¸ƒå°”æ©ç ï¼Œç”¨äºè¯†åˆ«æœ‰æ•ˆçš„ï¼ˆéå¡«å……ï¼‰æ ‡ç­¾ã€‚\n",
    "        # åœ¨ä¹‹å‰çš„å¤„ç†ä¸­ï¼Œ-100 è¢«ç”¨ä½œå¡«å……æ ‡ç­¾å’Œç‰¹æ®Š token çš„æ ‡ç­¾ã€‚\n",
    "        # è¿™ä¸ªæ©ç å°†åœ¨çœŸå®æ ‡ç­¾ä¸æ˜¯ -100 çš„ä½ç½®ä¸º Trueï¼Œå¦åˆ™ä¸º Falseã€‚\n",
    "        mask = labels != -100  # Ignore padded labels\n",
    "        # predictions[mask] æ“ä½œï¼š\n",
    "        # è¿™ä¸ªæ“ä½œä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ä¸€ç»´å¼ é‡ã€‚\n",
    "        # æ–°å¼ é‡åªåŒ…å« predictions ä¸­å¯¹åº” mask ä¸º True çš„å…ƒç´ ã€‚\n",
    "        '''predictions = [[1, 2, 3, 0],\n",
    "               [2, 1, 0, 0]]\n",
    "mask = [[True, True, True, False],\n",
    "        [True, True, False, False]]\n",
    "        é‚£ä¹ˆ predictions[mask] çš„ç»“æœå°†æ˜¯[1, 2, 3, 2, 1]'''\n",
    "        correct += (predictions[mask] == labels[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation using the DataLoader\n",
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847155e-b5eb-4ccc-9889-42cf9632d29d",
   "metadata": {},
   "source": [
    "#### Finally, defining the training arguments for the trainer and initialzing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c482878-d4ac-422c-ae37-136ee3c290d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4407' max='4407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4407/4407 2:14:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.087213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.092754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.093473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4407, training_loss=0.08793052654296653, metrics={'train_runtime': 8099.8094, 'train_samples_per_second': 4.351, 'train_steps_per_second': 0.544, 'total_flos': 4608567785226240.0, 'train_loss': 0.08793052654296653, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pos_tagging_model\", # æ¨¡å‹å’Œæ£€æŸ¥ç‚¹çš„ä¿å­˜ç›®å½•\n",
    "    evaluation_strategy=\"epoch\", # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œè¯„ä¼°\n",
    "    save_strategy=\"epoch\",  # æ¯ä¸ª epoch ç»“æŸåä¿å­˜æ¨¡å‹\n",
    "    per_device_train_batch_size=8,  # æ¯ä¸ª epoch ç»“æŸåä¿å­˜æ¨¡å‹\n",
    "    per_device_eval_batch_size=8, # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs=3, # è®­ç»ƒçš„æ€» epoch æ•°\n",
    "    weight_decay=0.01, # L2 æ­£åˆ™åŒ–ç³»æ•°\n",
    "    save_total_limit=2, # æœ€å¤šä¿å­˜çš„æ£€æŸ¥ç‚¹æ•°é‡\n",
    "    report_to=\"none\", # ä¸ä½¿ç”¨ä»»ä½•æŠ¥å‘Šå·¥å…·ï¼ˆå¦‚ TensorBoardï¼‰\n",
    "    logging_strategy=\"epoch\" # æ¯ä¸ª epoch ç»“æŸåè®°å½•æ—¥å¿—\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # è¦è®­ç»ƒçš„æ¨¡å‹\n",
    "    args=training_args, # ä¸Šé¢å®šä¹‰çš„è®­ç»ƒå‚æ•°\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc065870-c27f-42d8-abfb-8be6db70658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Accuracy on Test Set: 0.9762\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(model, test_dataloader)\n",
    "print(f\"POS Tagging Accuracy on Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de610956-45f4-4ad7-922f-f4f8189fc721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='182' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [182/182 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09347294270992279,\n",
       " 'eval_runtime': 81.5145,\n",
       " 'eval_samples_per_second': 17.801,\n",
       " 'eval_steps_per_second': 2.233,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "# evaluate() æ–¹æ³•åœ¨è¯„ä¼°æ•°æ®é›†ï¼ˆé€šå¸¸æ˜¯æµ‹è¯•é›†ï¼‰ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¹¶è®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡ã€‚\n",
    "# å®ƒè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„åç§°å’Œå¯¹åº”çš„å€¼ã€‚\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7290bc-4c7d-4cdc-98f4-2b65001b3f77",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d59546a-9676-4232-ab80-b0e20dc4b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9436973d-5060-46fb-98c4-3575eeceea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d70ab7428b14b6ba5a462a3d4c761e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\scratch\\singh\\hf\\hub\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fb8f5c741f4488b2668ec86b77ade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff2e3aab8e04ef4b21583fa0ac3bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c54539fefdd4afc9f9b4ec940ac8050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e85bfb659b4d55a25f52caf473419c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666a762124554b369499b8340d0e6dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020d410ea6e541358582a792016aeabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632ca242bb3742fc94f44d9bb5dbf76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ddefe7f3b743f1a73a7e6eeb345124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model microsoft/Phi-3.5-mini-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.phi3.modeling_phi3.Phi3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: é¡µé¢æ–‡ä»¶å¤ªå°ï¼Œæ— æ³•å®Œæˆæ“ä½œã€‚ (os error 1455)\n\nwhile loading with Phi3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: é¡µé¢æ–‡ä»¶å¤ªå°ï¼Œæ— æ³•å®Œæˆæ“ä½œã€‚ (os error 1455)\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the pipeline\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# è¿™æ˜¯Hugging Face Transformersåº“ä¸­çš„ä¸€ä¸ªé«˜çº§APIï¼Œç”¨äºç®€åŒ–æ¨¡å‹çš„ä½¿ç”¨ã€‚\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# å®ƒè‡ªåŠ¨å¤„ç†æ¨¡å‹åŠ è½½ã€tokenizationå’Œæ¨ç†è¿‡ç¨‹ã€‚\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# æŒ‡å®šäº†pipelineçš„ä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆã€‚\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# è¿™æ„å‘³ç€æ¨¡å‹å°†è¢«ç”¨äºç”Ÿæˆè¿ç»­çš„æ–‡æœ¬ã€‚\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# æŒ‡å®šäº†pipelineçš„ä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆã€‚\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# è¿™æ„å‘³ç€æ¨¡å‹å°†è¢«ç”¨äºç”Ÿæˆè¿ç»­çš„æ–‡æœ¬ã€‚\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3.5-mini-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Trust the remote code; this is required for some models, but always check the code first!\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set this to \"cuda\" for GPU acceleration if available\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 for less memory usage and faster inference\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32me:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model microsoft/Phi-3.5-mini-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.phi3.modeling_phi3.Phi3ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: é¡µé¢æ–‡ä»¶å¤ªå°ï¼Œæ— æ³•å®Œæˆæ“ä½œã€‚ (os error 1455)\n\nwhile loading with Phi3ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4245, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4791, in _load_pretrained_model\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"e:\\Uni Stuttgart\\è¯¾ç¨‹\\Intro to DL\\my_env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 504, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: é¡µé¢æ–‡ä»¶å¤ªå°ï¼Œæ— æ³•å®Œæˆæ“ä½œã€‚ (os error 1455)\n\n\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "# è¿™æ˜¯Hugging Face Transformersåº“ä¸­çš„ä¸€ä¸ªé«˜çº§APIï¼Œç”¨äºç®€åŒ–æ¨¡å‹çš„ä½¿ç”¨ã€‚\n",
    "# å®ƒè‡ªåŠ¨å¤„ç†æ¨¡å‹åŠ è½½ã€tokenizationå’Œæ¨ç†è¿‡ç¨‹ã€‚\n",
    "# æŒ‡å®šäº†pipelineçš„ä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆã€‚\n",
    "# è¿™æ„å‘³ç€æ¨¡å‹å°†è¢«ç”¨äºç”Ÿæˆè¿ç»­çš„æ–‡æœ¬ã€‚\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    # æŒ‡å®šäº†pipelineçš„ä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆã€‚\n",
    "    # è¿™æ„å‘³ç€æ¨¡å‹å°†è¢«ç”¨äºç”Ÿæˆè¿ç»­çš„æ–‡æœ¬ã€‚\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=\"cpu\",  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5332c6a7-46b6-44b7-9d42-a81b9506e4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å¤„ç†è¿‡ç¨‹ï¼š\\n\\nå°†æ•°å­—IDåºåˆ—æ˜ å°„å›å¯¹åº”çš„tokenï¼ˆå•è¯æˆ–å­è¯ï¼‰ã€‚\\nå°†è¿™äº›tokenæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²ã€‚\\nç§»é™¤æ‰€æœ‰çš„ç‰¹æ®Štokenï¼ˆå¦‚æœ skip_special_tokens=Trueï¼‰ã€‚\\nå¤„ç†ä»»ä½•éœ€è¦çš„åå¤„ç†æ­¥éª¤ï¼ˆå¦‚åˆå¹¶åˆ†å‰²çš„å•è¯ï¼‰ã€‚\\nè¿”å›å€¼ï¼š\\n\\nä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŒ…å«è§£ç åçš„æ–‡æœ¬ã€‚\\nç”¨é€”ï¼š\\n\\nåœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¿™é€šå¸¸æ˜¯è·å–æœ€ç»ˆå¯è¯»è¾“å‡ºçš„æœ€åä¸€æ­¥ã€‚\\nå®ƒå°†æ¨¡å‹çš„æ•°å€¼è¾“å‡ºè½¬æ¢ä¸ºäººç±»å¯ç†è§£çš„æ–‡æœ¬ã€‚\\nç¤ºä¾‹ï¼š å‡è®¾ outputs[0] æ˜¯ [7993, 2000, 10169, 58, 7592, 11, 2129, 2024, 2017, 30] \\nè§£ç åå¯èƒ½å¾—åˆ°ç±»ä¼¼ \"Translate to French: Hello, how are you?\" çš„æ–‡æœ¬ã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    # Tokenize input\n",
    "    # æŒ‡å®šè¿”å›çš„ç»“æœåº”è¯¥æ˜¯PyTorchå¼ é‡ã€‚\n",
    "    # \"pt\"ä»£è¡¨PyTorchã€‚å…¶ä»–é€‰é¡¹åŒ…æ‹¬\"tf\"ï¼ˆTensorFlowï¼‰æˆ–\"np\"ï¼ˆNumPyæ•°ç»„ï¼‰ã€‚\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move ALL input tensors to the same device as the model\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    # Manually create position IDs and move them to the correct device\n",
    "    # è·å–è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚\n",
    "    # inputs[\"input_ids\"] æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size, sequence_length) çš„å¼ é‡ã€‚\n",
    "    # .shape[1] å–ç¬¬äºŒä¸ªç»´åº¦ï¼Œå³åºåˆ—é•¿åº¦ã€‚\n",
    "    # torch.arange(...):\n",
    "    # åˆ›å»ºä¸€ä¸ªä» 0 åˆ° (åºåˆ—é•¿åº¦ - 1) çš„æ•´æ•°åºåˆ—ã€‚\n",
    "    # ä¾‹å¦‚ï¼Œå¦‚æœåºåˆ—é•¿åº¦æ˜¯ 5ï¼Œè¿™å°†åˆ›å»º [0, 1, 2, 3, 4]ã€‚\n",
    "    # æŒ‡å®šç”Ÿæˆçš„å¼ é‡æ•°æ®ç±»å‹ä¸ºé•¿æ•´å‹\n",
    "    position_ids = torch.arange(inputs[\"input_ids\"].shape[1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate output while ensuring ALL tensors are on the correct device\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    # tokenizer.decode():\n",
    "    # è¿™æ˜¯tokenizerçš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†æ•°å­—token IDåºåˆ—è½¬æ¢å›åŸå§‹æ–‡æœ¬ã€‚\n",
    "    # å®ƒæ‰§è¡Œçš„æ˜¯tokenizationçš„é€†æ“ä½œã€‚\n",
    "    # è¿™ä¸ªå‚æ•°æŒ‡ç¤ºè§£ç è¿‡ç¨‹ä¸­è¦è·³è¿‡ç‰¹æ®Štokenã€‚\n",
    "    # ç‰¹æ®ŠtokenåŒ…æ‹¬å¦‚ [PAD], [CLS], [SEP], [MASK] ç­‰ï¼Œè¿™äº›é€šå¸¸ä¸åº”è¯¥å‡ºç°åœ¨æœ€ç»ˆçš„æ–‡æœ¬è¾“å‡ºä¸­ã€‚\n",
    "    # è®¾ç½®ä¸º True å¯ä»¥å¾—åˆ°æ›´å¹²å‡€ã€æ›´å¯è¯»çš„è¾“å‡ºã€‚\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "'''å¤„ç†è¿‡ç¨‹ï¼š\n",
    "\n",
    "å°†æ•°å­—IDåºåˆ—æ˜ å°„å›å¯¹åº”çš„tokenï¼ˆå•è¯æˆ–å­è¯ï¼‰ã€‚\n",
    "å°†è¿™äº›tokenæ‹¼æ¥æˆä¸€ä¸ªè¿ç»­çš„å­—ç¬¦ä¸²ã€‚\n",
    "ç§»é™¤æ‰€æœ‰çš„ç‰¹æ®Štokenï¼ˆå¦‚æœ skip_special_tokens=Trueï¼‰ã€‚\n",
    "å¤„ç†ä»»ä½•éœ€è¦çš„åå¤„ç†æ­¥éª¤ï¼ˆå¦‚åˆå¹¶åˆ†å‰²çš„å•è¯ï¼‰ã€‚\n",
    "è¿”å›å€¼ï¼š\n",
    "\n",
    "ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŒ…å«è§£ç åçš„æ–‡æœ¬ã€‚\n",
    "ç”¨é€”ï¼š\n",
    "\n",
    "åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¿™é€šå¸¸æ˜¯è·å–æœ€ç»ˆå¯è¯»è¾“å‡ºçš„æœ€åä¸€æ­¥ã€‚\n",
    "å®ƒå°†æ¨¡å‹çš„æ•°å€¼è¾“å‡ºè½¬æ¢ä¸ºäººç±»å¯ç†è§£çš„æ–‡æœ¬ã€‚\n",
    "ç¤ºä¾‹ï¼š å‡è®¾ outputs[0] æ˜¯ [7993, 2000, 10169, 58, 7592, 11, 2129, 2024, 2017, 30] \n",
    "è§£ç åå¯èƒ½å¾—åˆ°ç±»ä¼¼ \"Translate to French: Hello, how are you?\" çš„æ–‡æœ¬ã€‚'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d8735d-a14b-4ee0-9a18-d9508feca1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate text for each prompt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m(prompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Example prompts to test model behavior\n",
    "prompts = [\n",
    "    \"Write a short story about a robot discovering emotions.\",\n",
    "    \"Explain how transformers work in deep learning.\",\n",
    "    \"You are a helpful assistant. Answer: What is deep learning?\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    output = pipe(prompt, max_new_tokens=100)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated Output: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986f6ce-b5fa-421e-8f85-17647fdfdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Output with Custom Parameters:\n",
      "Explain quantum physics in simple terms.\n",
      "\n",
      "## Answer:\n",
      "Quantum physics is a branch of science that studies the behavior of the smallest particles in the universe, like atoms and photons (which are particles of light). Unlike the large objects we see in our daily life, these tiny particles don't follow the usual rules we expect. Here are some key points about quantum physics:\n",
      "\n",
      "1. **Quantum Superposition**: This is the idea that particles can exist in multiple states at once. For example,\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Explain quantum physics in simple terms.\",\n",
    "    max_new_tokens=100,  # Control text length\n",
    "    temperature=0.7,  # Higher values = more randomness\n",
    "    top_p=0.9,  # Use nucleus sampling for diversity\n",
    "    do_sample=True  # Enable sampling instead of greedy decoding\n",
    ")\n",
    "'''top_p=0.9:\n",
    "è¿™æ˜¯æ ¸é‡‡æ ·ï¼ˆnucleus samplingï¼‰çš„å‚æ•°ï¼Œä¹Ÿç§°ä¸º top-p é‡‡æ ·ã€‚\n",
    "å®ƒåªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° top_p çš„æœ€å¯èƒ½çš„ tokenã€‚\n",
    "0.9 æ„å‘³ç€æ¨¡å‹å°†ä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° 90% çš„æœ€å¯èƒ½ token ä¸­è¿›è¡Œé‡‡æ ·ã€‚\n",
    "è¿™æœ‰åŠ©äºä¿æŒå¤šæ ·æ€§ï¼ŒåŒæ—¶é¿å…é€‰æ‹©éå¸¸ä¸å¯èƒ½çš„ tokenã€‚\n",
    "\n",
    "do_sample=True:\n",
    "å¯ç”¨é‡‡æ ·è€Œä¸æ˜¯è´ªå©ªè§£ç ã€‚\n",
    "å½“è®¾ç½®ä¸º True æ—¶ï¼Œæ¨¡å‹ä¼šæ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œè€Œä¸æ˜¯æ€»æ˜¯é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ª tokenã€‚\n",
    "è¿™å…è®¸ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½äº§ç”Ÿç›¸åŒçš„è¾“å‡ºã€‚\n",
    "'''\n",
    "print(\"\\nGenerated Output with Custom Parameters:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d355bca-5c76-402b-a2e0-f8edc1e21309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the following sentences from English to French:\n",
      "\n",
      "1. I love programming.\n",
      "2. The weather is beautiful. \n",
      "3. I love deep learning. \n",
      "4. The sky is clear tonight.\n",
      "5. I am learning French.\n",
      "\n",
      "# Answer\n",
      "\n",
      "1. J'adore programmer.\n",
      "2. Le temps est magnifique.\n",
      "3. J'adore l'apprentissage profond.\n",
      "4. Le ciel est dÃ©gagÃ© ce soir.\n",
      "5. J'apprends le franÃ§ais.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Translate the following sentences from English to French:\n",
    "\n",
    "1. I love programming.\n",
    "2. The weather is beautiful. \n",
    "3. I love deep learning. \"\"\"\n",
    "'''output æ˜¯pipelineè¿”å›çš„ç»“æœï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚\n",
    "output[0] å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿå¯èƒ½æ˜¯å”¯ä¸€çš„ï¼‰ç”Ÿæˆç»“æœã€‚\n",
    "['generated_text'] è®¿é—®ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ã€‚'''\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n",
    "'''é¢„æœŸè¾“å‡ºï¼š\n",
    "\n",
    "è¾“å‡ºå°†åŒ…æ‹¬åŸå§‹çš„æç¤ºæ–‡æœ¬ï¼ˆè‹±è¯­å¥å­ï¼‰ä»¥åŠæ¨¡å‹ç”Ÿæˆçš„æ³•è¯­ç¿»è¯‘ã€‚\n",
    "ç”±äºä½¿ç”¨äº†zero-shotæ–¹æ³•ï¼Œæ¨¡å‹æ˜¯åœ¨æ²¡æœ‰ç‰¹å®šç¿»è¯‘ç¤ºä¾‹çš„æƒ…å†µä¸‹å°è¯•å®Œæˆä»»åŠ¡çš„ã€‚'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d777dcb-fb03-4618-9f7e-c443d0b7be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Learning Output:\n",
      "Translate the third sentence from English to French, \n",
      "the first two are examples on how to translate them:\n",
      "\n",
      "1. I love programming. - Jâ€™aime la programmation.\n",
      "2. The weather is beautiful. - Il fait beau.\n",
      "3. I love deep learning. - \n",
      "\n",
      "# Answer\n",
      "Je t'aime l'apprentissage profond.\n",
      "\n",
      "Note: In French, possessive structures can be a bit different from English. Instead of\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Translate the third sentence from English to French, \n",
    "the first two are examples on how to translate them:\n",
    "\n",
    "1. I love programming. - Jâ€™aime la programmation.\n",
    "2. The weather is beautiful. - Il fait beau.\n",
    "3. I love deep learning. -\"\"\"\n",
    "\n",
    "output = pipe(few_shot_prompt, max_new_tokens=40)\n",
    "print(\"\\nFew-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f138c50-cb64-4c53-9f60-d3e63a174fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Learning Output:\n",
      "Summarize the following article in one sentence: \n",
      "An ANN consists of connected units or nodes called artificial neurons, \n",
      "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
      "biological neurons more closely have also been recently investigated and shown \n",
      "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
      "Each artificial neuron receives signals from connected neurons, then processes them and \n",
      "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
      "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
      "called the activation function. The strength of the signal at each connection is \n",
      "determined by a weight, which adjusts during the learning process. \n",
      "Typically, neurons are aggregated into layers. Different layers may perform \n",
      "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
      "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
      "A network is typically called a deep neural network if it has at least two hidden layers. \n",
      "Artificial neural networks are used for various tasks, including predictive modeling, \n",
      "adaptive control, and solving problems in artificial intelligence. \n",
      "They can learn from experience, and can derive conclusions from a complex and \n",
      "seemingly unrelated set of information.\n",
      "\n",
      "ANNs are computational models inspired by the human brain, consisting of interconnected nodes or neurons that process and transmit information through weighted connections, with the ability to learn and adapt from data.\n",
      "\n",
      "\n",
      "## Response: ANNs are computational models that mimic the brain's neurons and synapses, process information through interconnected nodes, and learn from data.\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = \"\"\"Summarize the following article in one sentence: \n",
    "An ANN consists of connected units or nodes called artificial neurons, \n",
    "which loosely model the neurons in the brain. Artificial neuron models that mimic \n",
    "biological neurons more closely have also been recently investigated and shown \n",
    "to significantly improve performance. These are connected by edges, which model the synapses in the brain. \n",
    "Each artificial neuron receives signals from connected neurons, then processes them and \n",
    "sends a signal to other connected neurons. The \"signal\" is a real number, \n",
    "and the output of each neuron is computed by some non-linear function of the sum of its inputs, \n",
    "called the activation function. The strength of the signal at each connection is \n",
    "determined by a weight, which adjusts during the learning process. \n",
    "Typically, neurons are aggregated into layers. Different layers may perform \n",
    "different transformations on their inputs. Signals travel from the first layer (the input layer) \n",
    "to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). \n",
    "A network is typically called a deep neural network if it has at least two hidden layers. \n",
    "Artificial neural networks are used for various tasks, including predictive modeling, \n",
    "adaptive control, and solving problems in artificial intelligence. \n",
    "They can learn from experience, and can derive conclusions from a complex and \n",
    "seemingly unrelated set of information.\"\"\"\n",
    "output = pipe(zero_shot_prompt, max_new_tokens=100)\n",
    "\n",
    "print(\"\\nZero-Shot Learning Output:\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
