{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- huggingface_hub\n",
    "- datasets\n",
    "- tqdm\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: huggingface_hub in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: packaging in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# for pip, check conda online!\n",
    "%pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the POS tagging dataset from the Hugging Face hub and prepares it for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 13054\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have a training and a test dataset, so we use some training samples for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 11748\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1306\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 这行代码将原始的训练集（dataset[\"train\"]）拆分成两部分。test_size=0.1表示将10%的数据用于测试集（在这里实际上是用作验证集）\n",
    "# shuffle=True表示在拆分数据集时随机打乱数据集。\n",
    "# train_test_split函数将数据集分割成训练集和测试集（或在这个情况下，是验证集）\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "# 这行将刚才拆分出的10%的数据赋值给dataset字典中的\"validation\"键，作为验证集\n",
    "dataset[\"validation\"] = dataset_split[\"test\"]\n",
    "# 这行将剩下的90%的数据重新赋值给dataset字典中的\"train\"键，作为新的训练集\n",
    "dataset[\"train\"] = dataset_split[\"train\"]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': ['Many', 'retailers', 'fear', 'a', 'price', 'war', 'will', 'erupt', 'if', 'cash-strapped', 'companies', 'such', 'as', 'Campeau', 'Corp.', 'slash', 'tags', 'to', 'spur', 'sales', '.'], 'labels': ['JJ', 'NNS', 'VBP', 'DT', 'NN', 'NN', 'MD', 'VB', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', 'NNP', 'VB', 'NNS', 'TO', 'VB', 'NNS', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unpack GloVe 300d embeddings from a zip file, build a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "\n",
    "word_to_index = dict()\n",
    "embeddings = []\n",
    "# 使用zipfile.ZipFile打开包含GloVe词嵌入的ZIP文件。\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        # line原来长这样：cat 0.123 -0.456 0.789 ... 0.321；split之后长这样：values = [\"cat\", \"0.123\", \"-0.456\", \"0.789\", ..., \"0.321\"]\n",
    "        values = line.split()\n",
    "        # 第一个值是单词，需要从字节串解码为UTF-8字符串\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        # 将剩余的值转换为浮点数，然后创建一个PyTorch张量。\n",
    "        features = torch.tensor([float(value) for value in values[1:]])\n",
    "        word_to_index[word] = idx\n",
    "        embeddings.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add padding and unknown tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: 300\n",
      "Padding token id: 400001\n",
      "Unknown token id: 400000\n"
     ]
    }
   ],
   "source": [
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "# We also add a '<pad>' token to the vocabulary for padding sequences\n",
    "'''这里添加了一个特殊的<pad>标记到word_to_index字典中，并将其ID设置为字典的当前长度。\n",
    "这个标记通常用于填充序列，使它们具有相同的长度。'''\n",
    "word_to_index[\"<pad>\"] = len(word_to_index)\n",
    "padding_token_id = word_to_index[\"<pad>\"]\n",
    "unk_token_id = word_to_index[\"<unk>\"]\n",
    "# 为<pad>标记添加一个全零向量作为其嵌入。这个向量的维度与其他词的嵌入相同。\n",
    "embeddings.append(torch.zeros(embeddings[0].shape))\n",
    "'''背景：\n",
    "        1.embeddings是一个列表，其中每个元素都是一个表示单个词的嵌入向量的张量。\n",
    "        2.每个嵌入向量都是一个1D张量，比如shape可能是(300,)，表示300维的词嵌入。\n",
    "        3.列表中的元素数量等于词汇表的大小（包括特殊标记如<pad>和<unk>）。\n",
    "   torch.stack() 函数的作用：\n",
    "        1.假设我们有10000个词，每个词的嵌入是300维的。\n",
    "        2.在堆叠之前，我们有一个包含10000个形状为(300,)的张量的列表。\n",
    "        3.堆叠后，我们得到一个形状为(10000, 300)的2D张量。 '''\n",
    "# Convert the list of tensors to a single tensor\n",
    "embeddings = torch.stack(embeddings)\n",
    "# embeddings.size(1) 返回的是 embeddings 张量的第二个维度的大小\n",
    "# size()是 PyTorch 张量的一个方法，用于获取张量的维度信息;当不带参数调用时（如 embeddings.size()），它返回一个包含所有维度大小的元组\n",
    "print(f\"Embedding shape: {embeddings.size(1)}\")\n",
    "print(f\"Padding token id: {padding_token_id}\")\n",
    "print(f\"Unknown token id: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to map labels to indices and vice versa, and print the number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', \"''\", 'NNP', 'WP', 'TO', 'POS', 'NNPS', '(', 'EX', 'PDT', 'VB', 'FW', 'RBR', 'RP', 'MD', 'JJS', 'VBG', '-LRB-', 'NN', 'VBZ', '#', 'WDT', '``', '.', 'RBS', 'JJ', 'VBN', 'VBD', 'LS', '-NONE-', 'IN', 'DT', ':', '-RRB-', 'NNS', 'WRB', 'JJR', 'UH', 'RB', 'PRP$', '$', 'SYM', 'WP$', 'CC', 'CD', ',', 'PRP', 'VBP']\n",
      "Number of classes: 48\n",
      "{')': 0, \"''\": 1, 'NNP': 2, 'WP': 3, 'TO': 4, 'POS': 5, 'NNPS': 6, '(': 7, 'EX': 8, 'PDT': 9, 'VB': 10, 'FW': 11, 'RBR': 12, 'RP': 13, 'MD': 14, 'JJS': 15, 'VBG': 16, '-LRB-': 17, 'NN': 18, 'VBZ': 19, '#': 20, 'WDT': 21, '``': 22, '.': 23, 'RBS': 24, 'JJ': 25, 'VBN': 26, 'VBD': 27, 'LS': 28, '-NONE-': 29, 'IN': 30, 'DT': 31, ':': 32, '-RRB-': 33, 'NNS': 34, 'WRB': 35, 'JJR': 36, 'UH': 37, 'RB': 38, 'PRP$': 39, '$': 40, 'SYM': 41, 'WP$': 42, 'CC': 43, 'CD': 44, ',': 45, 'PRP': 46, 'VBP': 47}\n",
      "{0: ')', 1: \"''\", 2: 'NNP', 3: 'WP', 4: 'TO', 5: 'POS', 6: 'NNPS', 7: '(', 8: 'EX', 9: 'PDT', 10: 'VB', 11: 'FW', 12: 'RBR', 13: 'RP', 14: 'MD', 15: 'JJS', 16: 'VBG', 17: '-LRB-', 18: 'NN', 19: 'VBZ', 20: '#', 21: 'WDT', 22: '``', 23: '.', 24: 'RBS', 25: 'JJ', 26: 'VBN', 27: 'VBD', 28: 'LS', 29: '-NONE-', 30: 'IN', 31: 'DT', 32: ':', 33: '-RRB-', 34: 'NNS', 35: 'WRB', 36: 'JJR', 37: 'UH', 38: 'RB', 39: 'PRP$', 40: '$', 41: 'SYM', 42: 'WP$', 43: 'CC', 44: 'CD', 45: ',', 46: 'PRP', 47: 'VBP'}\n"
     ]
    }
   ],
   "source": [
    "'''双重循环的列表推导式解读：\n",
    "result = []\n",
    "for sample in dataset[\"train\"]:\n",
    "    for label in sample[\"labels\"]:\n",
    "        result.append(label)'''\n",
    "# 这行代码使用列表推导和集合来获取训练集中所有唯一的标签。\n",
    "# 它遍历训练集中的每个样本，收集所有标签，然后使用set()去重，最后转换回列表\n",
    "labels_unique = list(\n",
    "    set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]])\n",
    ")\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "'''enumerate(labels_unique) 会产生如下的序列:\n",
    "(0, 'O')(1, 'B-PER')(2, 'I-PER')(3, 'B-ORG')'''\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009c795dd2c24d1e95b86a59411bc1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23dfe65c1a4ffcb5cc81130784e160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0aa07ed6d6408e910ed3b47de6663f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['words', 'labels', 'token_ids', 'label_ids'])\n",
      "{'words': ['Many', 'retailers', 'fear', 'a', 'price', 'war', 'will', 'erupt', 'if', 'cash-strapped', 'companies', 'such', 'as', 'Campeau', 'Corp.', 'slash', 'tags', 'to', 'spur', 'sales', '.'], 'labels': ['JJ', 'NNS', 'VBP', 'DT', 'NN', 'NN', 'MD', 'VB', 'IN', 'JJ', 'NNS', 'JJ', 'IN', 'NNP', 'NNP', 'VB', 'NNS', 'TO', 'VB', 'NNS', '.'], 'token_ids': [109, 5192, 1655, 7, 626, 136, 43, 20454, 83, 168152, 337, 125, 19, 149818, 1018, 9421, 15648, 4, 8217, 526, 2], 'label_ids': [25, 34, 47, 31, 18, 18, 14, 10, 30, 25, 34, 25, 30, 2, 2, 10, 34, 4, 10, 34, 23]}\n"
     ]
    }
   ],
   "source": [
    "# mapping: 一个字典，用于查找映射关系;default: 可选参数，当键不在字典中时返回的默认值\n",
    "# 这个函数遍历 keys 列表中的每个元素，并尝试在 mapping 字典中查找对应的值\n",
    "# 返回值： 返回一个新的列表，包含所有映射后的值\n",
    "def map_list_using_dict(mapping, keys: list, default=None):\n",
    "    return [mapping.get(key, default) for key in keys]\n",
    "\n",
    "\n",
    "# 返回值： 返回一个整数列表，表示输入单词列表中每个单词对应的索引\n",
    "def map_tokens_to_indices(tokens: list[str]):\n",
    "    # Return the index of each token or the index of the '<unk>' token if a token is not in the vocabulary\n",
    "    return map_list_using_dict(\n",
    "        word_to_index, [token.lower() for token in tokens], unk_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "# 返回一个整数列表，表示输入标签列表中每个标签对应的索引。\n",
    "def map_labels_to_indices(labels: list):\n",
    "    # TODO: Implement the mapping of the labels to indices\n",
    "    return map_list_using_dict(\n",
    "        ctoi, [label for label in labels]\n",
    "    )\n",
    "\n",
    "\n",
    "# 函数功能： 对数据集中的每个样本进行处理，将单词转换为token索引，将标签转换为标签索引。\n",
    "def prepare_dataset(dataset):\n",
    "    # return map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, dataset)\n",
    "    # dataset.map() 方法是 Hugging Face 的 Dataset 类中的一个方法；它的作用是对数据集中的每个元素应用一个函数，从而转换整个数据集\n",
    "    # lambda是python中的匿名函数，x是这个函数的参数，代表数据集中的一个样本\n",
    "    # lambda x 函数作为参数传递给 dataset.map()。\n",
    "    # dataset.map() 会遍历数据集中的每个样本，并对每个样本调用这个 lambda 函数。\n",
    "    # x 代表 dataset 中的一个样本\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            # x[\"words\"] 被传递给 map_tokens_to_indices 函数，将单词列表转换为token索引列表\n",
    "            \"token_ids\": map_tokens_to_indices(x[\"words\"]),\n",
    "            # x[\"labels\"] 被传递给 map_labels_to_indices 函数，将标签列表转换为标签索引列表\n",
    "            \"label_ids\": map_labels_to_indices(x[\"labels\"]),\n",
    "        },\n",
    "        # num_proc=1: 指定使用1个进程进行处理。这可能是为了确保处理的顺序性或避免多进程带来的潜在问题。\n",
    "        num_proc=1,\n",
    "    )\n",
    "    return dataset\n",
    "# 返回处理后的数据集，其中每个样本都包含 \"token_ids\" 和 \"label_ids\" 字段\n",
    "\n",
    "dataset = prepare_dataset(dataset)\n",
    "dataset_train_tokenized = dataset[\"train\"]\n",
    "dataset_validation_tokenized = dataset[\"validation\"]\n",
    "\n",
    "# Print the first sample in the tokenized training dataset\n",
    "print(dataset_train_tokenized[0].keys())\n",
    "print(dataset_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again pad inputs to the maximum sequence length in the batch.\\\n",
    "But this time, we also have to pad the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,    316,  35091,     16,     20,     43,    255,   2423,    689,\n",
      "            548,      5,   1748,    604,    182,    147,      2, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [  4212,   2952,      1,     42,    919,    657,     93,   1405,    544,\n",
      "             22,    800, 400000,    183,    131,     29,  19979,    129,    321,\n",
      "             21,   5330,   7764,      1,    717,    135, 400000,      4,    800,\n",
      "              2, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [  1995,  70914,      9,  83674,    807,  35718,    573,     55,   1424,\n",
      "              3, 400000,   1356,    105,     42,   1995,  70914,      1,    114,\n",
      "           5599,     82,    167,      1,    675,    322,    509,     75,      0,\n",
      "             96,   8163,   1977,    513,      6,      0,   1332,   4079,      2],\n",
      "        [ 28740,      9,   1990,   1050,   1017,    680,     80,  55979,     93,\n",
      "              1,     46,   4876,   1103,      7,    593,      1,     25,  10904,\n",
      "            825,      2, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [  1896,      1,    212,      1,      0,   3598,    263,    317,    288,\n",
      "              2, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    18,     15,   1428,    854,     90,      2, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    77,   1812,     12,   4502,    414,     33,   4037,   1003,     77,\n",
      "           2194,     21,    120,   5858,   1995,  85442,      5,   6547,   1995,\n",
      "          11597,   1087,     13,      4,   7910,      0,   3312,    953,      2,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [     0,    128,     16,   1343,     20,     31,   1112, 400000,      4,\n",
      "           3829,     47,   7440,      6,  13053,      5,  13864,      2, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001]])\n",
      "tensor([[    31,     18,     18,     27,     46,     14,     10,     25,     25,\n",
      "             18,     43,     44,     34,     25,     18,     23, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [     2,      2,     45,     21,     27,     44,     44,     25,     34,\n",
      "             30,     44,     44,      2,     30,     31,     18,     18,     26,\n",
      "             30,      2,      2,     45,     27,     38,     44,      4,     44,\n",
      "             23, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [     2,      2,      5,     25,     18,     19,     38,     44,     34,\n",
      "             30,     25,     18,     30,     21,      2,      2,     45,     38,\n",
      "             44,     34,     25,     45,     26,      2,      2,     30,     31,\n",
      "             24,     38,     27,     18,     30,     31,     25,     18,     23],\n",
      "        [     2,      5,     44,     25,     18,     27,     40,     44,     44,\n",
      "             45,     43,     44,     34,     31,     18,     45,     30,     26,\n",
      "             34,     23, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    38,     45,     38,     45,     31,     18,     27,     38,     25,\n",
      "             23, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    46,     27,     38,     18,     18,     23, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    31,     18,     30,      2,     14,     10,     26,     46,     31,\n",
      "             18,     30,     38,     16,      2,      2,     43,     16,      2,\n",
      "              2,     10,     30,      4,     10,     31,      2,     18,     23,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
      "        [    31,     18,     27,     29,     46,     19,     26,     29,      4,\n",
      "             10,     39,     34,     30,      2,     43,      2,     23, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001]])\n"
     ]
    }
   ],
   "source": [
    "# batch: 输入的批次，通常是一个列表，包含多个样本（每个样本是一个字典）\n",
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\", \"label_ids\"], padding_value=-1):\n",
    "    # Pad keys_to_pad to the maximum length in batch\n",
    "    \n",
    "    #创建一个新的字典来存储填充后的批次\n",
    "    padded_batch = {}\n",
    "    for key in keys_to_pad:\n",
    "        # Get maximum length in batch\n",
    "        # key 是当前正在处理的特征名称，例如 \"token_ids\" 或 \"label_ids\"。\n",
    "        max_len = max([len(sample[key]) for sample in batch])\n",
    "        # Pad all samples to the maximum length\n",
    "        padded_batch[key] = torch.tensor(\n",
    "            [\n",
    "                sample[key] + [padding_value] * (max_len - len(sample[key]))\n",
    "                for sample in batch\n",
    "            ]\n",
    "        )\n",
    "    # Add remaining keys to the batch\n",
    "    # 对于不需要填充的键：直接将所有样本的该键值转换为张量，不进行填充。\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = torch.tensor([sample[key] for sample in batch])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32, shuffle=False):\n",
    "    # Create a DataLoader for the dataset\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        # 用于处理和组合样本列表以形成小批量的函数\n",
    "        # 使用 partial 函数创建一个新的函数，这个新函数基于 pad_inputs，但预设了 padding_value 参数为 padding_token_id。\n",
    "        # 这意味着当 DataLoader 调用这个函数来创建批次时，它会使用 pad_inputs 来处理可变长度的序列，并用 padding_token_id 进行填充。\n",
    "        # partial允许我们基于一个已有的函数创建一个新的函数，同时预设一些参数；这在很多情况下都非常有用，特别是当你需要多次使用同一个函数，但每次只改变其中的一部分参数时。\n",
    "        # 为什么需要把padding_value设为padding_token_id\n",
    "        # padding_token_id 通常是词汇表中专门为填充保留的一个特殊标记的ID\n",
    "        # 这里的padding_token_id是词汇表中的最后一个ID\n",
    "        collate_fn=partial(pad_inputs, padding_value=padding_token_id),\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a DataLoader for the training dataset with the selected columns\n",
    "dataloader_train = get_dataloader(\n",
    "    # 这行代码使用with_format方法选择了dataset_train_tokenized数据集中的\"token_ids\"和\"label_ids\"两列。\n",
    "    # 这样可以确保在后续处理中只使用这两列数据。\n",
    "    dataset_train_tokenized.with_format(columns=[\"token_ids\", \"label_ids\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "dataloader_validation = get_dataloader(\n",
    "    dataset_validation_tokenized.with_format(columns=[\"token_ids\", \"label_ids\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label_ids\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs\n",
    "\n",
    "So far, we have not paid attention to which device the PyTorch operations are running on.\\\n",
    "By default, they run on the CPU, however, a GPU is usually much faster when performing tensor operations.\\\n",
    "For this, you will need to have a supported GPU available on the device where you execute this code.\\\n",
    "Our servers at the IMS provide GPUs (strauss, nandu, kiwi).\\\n",
    "You can either remotely connect your editor and run the code there, or connect to a remote Python Kernel.\n",
    "\n",
    "Once there is a supported GPU available on your machine that runs the code, you can copy tensors and even models using the method `.to(device)` to `device`.\\\n",
    "`device` can be specified using `torch.device`:\n",
    "```python\n",
    "# 'cuda' for GPU (optionally specify device id, e.g., 'cuda:0' for the first GPU) and 'cpu' for CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "> Note that not specifying a device id might use all GPUs! Therefore, always set the device id or restrict the available GPUs, e.g., using the environment variable `CUDA_VISIBLE_DEVICES`.\n",
    "> You can set this variable first using `export CUDA_VISIBLE_DEVICES=3` so that any executed command afterward will use the GPU with id 3 (the 4th GPU) or directly set it for your command using `CUDA_VISIBLE_DEVICES=3 command`.\n",
    "\n",
    "You may also allocate tensors on a specific device during intialization:\n",
    "```python\n",
    "a = torch.tensor(..., device=device)\n",
    "```\n",
    "This works for all the tensor creation operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our neural network consists of one fully connected linear layer\n",
    "\n",
    "The softmax is part of the loss function in PyTorch, so you can omit this in the forward function.\n",
    "\n",
    "The embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePOSTagger(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    # embedding_vectors: 预训练的词嵌入向量;num_classes: 输出类别的数量（即可能的词性标签数量）;hidden_dim: 隐藏层的维度\n",
    "    def __init__(self, embedding_vectors, num_classes, hidden_dim):\n",
    "        super().__init__()\n",
    "        # PyTorch's embedding layer maps from indices to embeddings, freeze will tell PyTorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        # 这行代码创建了一个预训练的嵌入层\n",
    "        # torch.nn.Embedding.from_pretrained(): 这是PyTorch中创建预训练嵌入层的方法。\n",
    "        # 它允许你使用已经训练好的词嵌入来初始化Embedding层。\n",
    "        # freeze=True: 这个参数设置为True表示在训练过程中，这些嵌入权重将不会被更新。这通常用于保持预训练嵌入的原始信息，特别是当你认为这些预训练嵌入已经足够好，或者你的训练数据较少时\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            embedding_vectors, freeze=True\n",
    "        )\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        # 一个全连接层，将嵌入向量映射到指定的隐藏维度，然后再映射到输出类别的数量（即可能的词性标签数量）\n",
    "        # embedding_vectors.size(1) 是嵌入向量的维度，即嵌入向量中每个词的维度。\n",
    "        self.hidden_layer = torch.nn.Linear(embedding_vectors.size(1), hidden_dim)\n",
    "        # 另一个全连接层，将隐藏层的输出映射到类别数量的维度\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # PyTorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        z_1 = self.hidden_layer(emmedded_inputs)\n",
    "        # 使用 Leaky ReLU 激活函数，负斜率为 0.2 d\n",
    "        '''Leaky ReLU：\n",
    "定义：f(x) = x if x > 0 else αx; 其中 α 是一个小的正数，通常在 0.01 到 0.2 之间; 在代码中，α 被称为 negative_slope（负斜率）\n",
    "Leaky ReLU 的工作原理：对于正输入：和 ReLU 一样，直接输出该值; 对于负输入：输出一个很小的负值（输入乘以 α）'''\n",
    "        a_1 = torch.nn.functional.leaky_relu(z_1, negative_slope=0.2)\n",
    "        z_2 = self.output_layer(a_1)\n",
    "        return z_2  # softmax is applied in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model, loss and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- As optimizer we use Adam (adapts the learning rate per weight)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and optimizer and move model to device\n",
    "model = SimplePOSTagger(embedding_vectors=embeddings, num_classes=len(ctoi), hidden_dim=128).to(DEVICE)\n",
    "# ignore_index=padding_token_id: 在计算损失时忽略填充标记，这对于处理变长序列很重要\n",
    "# reduction 参数决定了如何汇总单个样本的损失值。在我们的例子中，我们使用'mean'，这意味着将所有样本的损失值相加，然后除以样本数。\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_token_id)\n",
    "# 使用 Adam 优化器；model.parameters(): 优化模型的所有可训练参数；lr=0.01: 设置学习率为 0.01\n",
    "# Adam（Adaptive Moment Estimation）优化器是一种广泛使用的深度学习优化算法。它结合了其他几种优化算法的优点，特别适合处理大规模数据和参数。\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# 创建一个字典来存储训练过程中的损失和准确率；初始值设为 '------'，可能用于后续的打印或记录\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, model):\n",
    "    # data_iter：数据集的迭代器；model：要评估的模型\n",
    "    correct_count = 0 # 正确预测的数量\n",
    "    total_count = 0 # 总的预测数量\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "\n",
    "        # move data to device since our model is on the device\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # predict only\n",
    "        # 在预测阶段，我们不需要计算梯度，因此使用torch.no_grad()上下文管理器可以节省内存和计算资源。\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_ids)\n",
    "        '''outputs 的结构:\n",
    "            这是模型的原始输出。\n",
    "            通常形状为 (batch_size, sequence_length, num_classes)。\n",
    "            每个位置包含一个概率分布，表示该位置属于每个类别的可能性'''\n",
    "        # torch.argmax() 或 tensor.argmax() 返回指定维度上最大值的索引。\n",
    "        # dim=2 表示在第三个维度（类别维度）上进行操作\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "        # 这行代码的目的是计算每个序列的实际长度，忽略填充标记。\n",
    "        # token_ids 每行代表一个序列，可能包含实际标记和填充标记。\n",
    "        # 400001 是填充标记的 ID\n",
    "        # 这是一个布尔运算，对 token_ids 中的每个元素进行。结果是一个布尔张量，与 token_ids 形状相同。true是1，false是0\n",
    "        # .sum(dim=1)在第二个维度（序列长度维度）上求和；这会将每个序列的布尔值相加，得到每个序列中非填充标记的数量\n",
    "        # inputs_lengths结果是一个一维张量，长度等于批次大小。每个元素表示对应序列的实际长度（不包括填充）\n",
    "        inputs_lengths = (token_ids != 400001).sum(dim=1)\n",
    "        \n",
    "        # 这行代码的目的是计算所有序列的实际长度之和。\n",
    "        total_count += inputs_lengths.sum()\n",
    "        # iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        # 外层循环遍历批次中的每个序列\n",
    "        for i in range(batch_size):\n",
    "            # 内层循环遍历当前序列中的每个有效标记\n",
    "            # inputs_lengths[i] 是第 i 个序列的实际长度（不包括填充）\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                # outputs_classes[i][j] 是模型对第 i 个序列中第 j 个标记的预测\n",
    "                # labels[i][j] 是第 i 个序列中第 j 个标记的真实标签\n",
    "                # int(...) 将布尔值转换为整数（True 变为 1，False 变为 0）\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    # total_count是一个 PyTorch 张量，存储了总的标记数量；.float()将张量转换为浮点类型\n",
    "    # 这个方法将单元素张量转换为标准的 Python 数值类型，以便进行除法运算。\n",
    "    return correct_count / total_count.float().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation dataset: 0.00797896731986182\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(dataloader_validation, model)\n",
    "print(f\"Accuracy on the validation dataset: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc3f81f1ed9484aa39159a8a8279afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7345 [00:00<?, ?it/s, accuracy=------, loss=------]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(dataloader_train), postfix=metric_dict)\n",
    "\n",
    "# evaluate on validation set first\n",
    "# 这行代码是用来更新和格式化验证集上的准确率，并将其添加到 metric_dict 字典中\n",
    "# evaluate(dataloader_validation, model)函数调用会返回一个 0 到 1 之间的浮点数，表示模型在验证集上的准确率\n",
    "# 100 * ...将准确率转换为百分比形式。\n",
    "# :6.2f：6 表示总字段宽度为 6 个字符；.2 表示保留 2 位小数。最终结果可能看起来像 \" 75.00%\n",
    "# {'accuracy': ...}创建一个字典，键为 'accuracy'，值为格式化后的准确率字符串\n",
    "# metric_dict.update(...)将新创建的字典合并到 metric_dict 中。如果 metric_dict 已经有 'accuracy' 键，它会被更新；如果没有，会被添加。\n",
    "metric_dict.update({'accuracy': f'{100*evaluate(dataloader_validation, model):6.2f}%'})\n",
    "# pbar是一个 tqdm 对象，代表一个进度条。它在之前的代码中被创建，用于可视化训练进度。用于在进度条后面设置额外的信息（postfix）。这些信息会在进度条的右侧显示\n",
    "# metric_dict字典包含了我们想要显示的指标，通常包括像 'loss'（损失）和 'accuracy'（准确率）这样的键值对\n",
    "pbar.set_postfix(metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    # 设置进度条描述为当前轮数和总轮数\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    # 循环遍历训练数据的每个批次\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        # extract input and labels and move data to device since our model is on the device\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #前向传播\n",
    "        outputs = model(token_ids)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) containing the class index\n",
    "        # criterion是预先定义的损失函数，通常是交叉熵损失（CrossEntropyLoss）用于分类任务\n",
    "        # outputs是模型的原始输出；对于序列标注任务，其形状通常是 (batch_size, sequence_length, num_classes)；每个位置包含一个概率分布，表示该位置属于每个类别的可能性\n",
    "        # permute 方法用于重新排列张量的维度；这样操作后，输出的形状变为 (batch_size, num_classes, sequence_length)\n",
    "        '''为什么需要 permute：\n",
    "            1.PyTorch 的 CrossEntropyLoss 期望输入的形状是 (batch_size, num_classes, *)，其中 * 表示任意数量的其他维度。\n",
    "            2.在序列标注任务中，我们需要将 num_classes 维度移到第二位，以符合损失函数的要求。'''\n",
    "        loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        # loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # 清除之前的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update()\n",
    "        # .item() 方法从张量中提取数值，将其转换为 Python 标量（如 float）。这是必要的，因为我们需要一个普通的 Python 数值来格式化字符串\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(dataloader_validation, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_indices_to_labels(indices: list):\n",
    "    return map_list_using_dict(itoc, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': ['The', 'building', 'houses', 'about', '4,500', 'Chase', 'workers', ',', 'most', 'of', 'whom', 'will', 'be', 'moved', 'to', 'downtown', 'Brooklyn', 'after', 'the', 'bank', \"'s\", 'new', 'back', 'office', 'center', 'is', 'completed', 'in', '1993', '.'], 'labels': ['DT', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ',', 'RBS', 'IN', 'WP', 'MD', 'VB', 'VBN', 'TO', 'NN', 'NNP', 'IN', 'DT', 'NN', 'POS', 'JJ', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.'], 'token_ids': [0, 447, 1631, 59, 14298, 4212, 537, 1, 96, 3, 1231, 43, 30, 554, 4, 2522, 4430, 49, 0, 231, 9, 50, 137, 283, 313, 14, 1315, 6, 1251, 2], 'label_ids': [31, 18, 34, 30, 44, 2, 34, 45, 24, 30, 3, 14, 10, 26, 4, 18, 2, 30, 31, 18, 5, 25, 25, 18, 18, 19, 26, 30, 44, 23]}\n",
      "Input: The building houses about 4,500 Chase workers , most of whom will be moved to downtown Brooklyn after the bank 's new back office center is completed in 1993 .\n",
      "Prediction:   ['DT', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ',', 'RBS', 'IN', 'WP', 'MD', 'VB', 'VBD', 'TO', 'NN', 'NNP', 'IN', 'DT', 'NN', 'POS', 'NNP', 'RB', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']\n",
      "Ground truth: ['DT', 'NN', 'NNS', 'IN', 'CD', 'NNP', 'NNS', ',', 'RBS', 'IN', 'WP', 'MD', 'VB', 'VBN', 'TO', 'NN', 'NNP', 'IN', 'DT', 'NN', 'POS', 'JJ', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'IN', 'CD', '.']\n",
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the validation dataset\n",
    "# random.choice用于从序列（如列表、元组或字符串）中随机选择一个元素\n",
    "sample = random.choice(dataset_validation_tokenized)\n",
    "print(sample)\n",
    "# build input vector and add batch dimension\n",
    "# unsqueeze() 的基本功能：是 PyTorch 中的一个张量操作，用于在指定位置添加一个新的维度。新维度的大小为 1。\n",
    "# dim=0 参数：指定在张量的第 0 维（最外层）添加新维度\n",
    "'''sample[\"token_ids\"])是一个列表\n",
    "torch.tensor(sample[\"token_ids\"]) 创建一个形状为 [sequence_length] 的一维张量\n",
    "1.列表：token_ids = [5, 8, 2, 10, 3, 1]\n",
    "2.一维张量：token_ids = tensor([5, 8, 2, 10, 3, 1])\n",
    "3.二维张量：token_ids = tensor([[5, 8, 2, 10, 3, 1]])'''\n",
    "sample_tensor = torch.tensor(sample[\"token_ids\"]).unsqueeze(dim=0).to(DEVICE)\n",
    "'''unsqueeze(dim=0) 将形状从 [sequence_length] 变为 [1, sequence_length]，模拟一个批次大小为 1 的输入。\n",
    "这一步是为了满足模型输入的要求。大多数深度学习模型期望输入是批次（batch）形式，即使是单个样本。\n",
    "处理完成后，我们不再需要批次维度，特别是在处理单个样本时;\n",
    "squeeze(dim=0) 移除第一个维度（批次维度），将形状从 [1, sequence_length, num_classes] 变为 [sequence_length, num_classes]'''\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "# 每个 output 是一个表示单个位置所有类别概率的向量\n",
    "# argmax() 函数找出张量中最大值的索引。dim=0 表示在第一个维度（即类别维度）上寻找最大值\n",
    "# .item() 将单元素张量转换为 Python 标量\n",
    "'''对 outputs 中的每个 output 执行上述操作。\n",
    "结果是一个列表，包含每个位置的预测标签。'''\n",
    "predictions = [itoc[output.argmax(dim=0).item()] for output in outputs]\n",
    "# print() 函数可以接受多个参数。这些参数之间用逗号分隔。\n",
    "print(\"Input:\", ' '.join(sample[\"words\"]))\n",
    "print(f\"Prediction:   {predictions}\")\n",
    "print(f\"Ground truth: {sample['labels']}\")\n",
    "# zip() 函数将 predictions 和 sample[\"labels\"] 这两个列表对应位置的元素配对\n",
    "# 遍历 zip() 生成的每对元素；pred 是预测标签，gt 是真实标签；if pred == gt 检查预测是否正确；如果预测正确（pred == gt），则生成一个 1；否则不生成任何值\n",
    "accuracy = sum([1 for pred, gt in zip(predictions, sample[\"labels\"]) if pred == gt]) / len(sample[\"labels\"])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not have to tokenize our data so far since tokens were given.\\\n",
    "For tokenizing text, you can again use the tokenization from the sentiment analysis task, but it has some trouble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'text.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_simple(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "print(tokenize_simple(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation is not properly split, but for POS tagging to work correctly, we need punctuation is separate tokens too.\\\n",
    "We can extract words and punctuation using a regular expression (regex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_regex(text: str):\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "print(tokenize_regex(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also packages like spacy that help you with tokenization.\\\n",
    "We have to install it first and then download some files for the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (2.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\uni stuttgart\\课程\\intro to dl\\my_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.7/12.2 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 22.1 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 11.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 19.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 5.2/5.4 MB 26.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install spacy using pip\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# download resources for english\n",
    "# `run` has to be replaced by `python` if run in a shell\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    # token.text 是 spaCy 库中 Token 对象的一个属性，用于获取标记（token）的原始文本形式\n",
    "    return [token.text for token in nlp(text.lower())]\n",
    "\n",
    "print(tokenize(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We render a nice text box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b16c116e1649c6a67b767970a4482f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='This movie is terrible', description='Sentence:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "sentence_widget = widgets.Text(\n",
    "    value=\"This movie is terrible\",\n",
    "    placeholder=\"Type something\",\n",
    "    description=\"Sentence:\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(sentence_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Prepare the input, and feed it through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentence_widget.value\n",
    "\n",
    "# convert text to token ids\n",
    "\n",
    "# build input vector and add batch dimension\n",
    "\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    \n",
    "\n",
    "# print prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
