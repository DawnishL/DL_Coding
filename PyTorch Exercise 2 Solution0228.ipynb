{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedd1f5e213c409eae153facaf3c3b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/421 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Uni Stuttgart\\课程\\Intro to DL\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\datasets--SetFit--sst5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088771a0fa0c432295306d4d7aaad4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cbf271b8124ebe8995a14ce67eb79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.jsonl:   0%|          | 0.00/171k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e59d69206c4ef69ddfc3ea3c4f3a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/343k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4a30f2d2a8418c8480ae1c477856b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0410713d51124c7abaf2294278cfd5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0828c6fc92724efda16de146297ac040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n",
      "{'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 4, 'label_text': 'very positive'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"SetFit/sst5\")\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Build vocabulary and pre-processing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def build_vocab(sentences: list[str]):\n",
    "    # vocab 是一个 set 类型的对象，用于存储唯一的单词。\n",
    "    # update() 方法用于将一个可迭代对象中的所有元素添加到集合中。\n",
    "    # 它将 tokenize(sentence) 返回的所有单词添加到 vocab 中。\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        vocab.update(tokenize(sentence))\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab(dataset[\"train\"][\"text\"])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_to_index(token):\n",
    "    # 这个函数的主要用途是在将文本转换为数字序列时使用，这是许多自然语言处理任务的预处理步骤。\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return vocab.get(token, -1)\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token) for token in tokenize(text)]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        # 输入 x 是数据集中的一个样本。\n",
    "        # 假设每个样本有一个 \"text\" 字段，包含原始文本。\n",
    "        # 使用 map_text_to_indices(x[\"text\"]) 将文本转换为 token 索引列表。\n",
    "        # 返回一个新的字典，包含 \"token_ids\" 字段，其值是转换后的索引列表。\n",
    "        lambda x: {\"token_ids\": map_text_to_indices(x[\"text\"])}, num_proc=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Create a function that takes a batch of sequences of token ids (list of list of ints) and converts them into one-hot encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding function for a batch of sentences\n",
    "def one_hot_encode_batch(sentences: list[list[int]]):\n",
    "    # Note that we are assuming that the sentences have the same length\n",
    "    sequence_length = len(sentences[0])\n",
    "    batch_size = len(sentences)\n",
    "\n",
    "    # Create a tensor of zeros with the desired shape (including the batch dimension)\n",
    "    one_hot_vectors = torch.zeros(\n",
    "        batch_size, sequence_length, vocab_size, dtype=torch.float32\n",
    "    )\n",
    "    # 遍历 sentences 列表中的每个句子。\n",
    "    for i, indices in enumerate(sentences):\n",
    "        # 遍历当前句子中的每个单词索引。j 是单词在句子中的位置，idx 是单词的索引值。\n",
    "        for j, idx in enumerate(indices):\n",
    "            # Set the appropriate index to 1.0, but only if the index is not -1\n",
    "            # 查单词索引是否有效（不是 -1）。索引 -1 通常用于表示未知单词或填充符号。\n",
    "            if idx >= 0:\n",
    "                # 在 one_hot_vectors 张量中，将对应位置设置为 1.0。\n",
    "                # i 表示批次中的第几个句子。\n",
    "                # j 表示句子中的第几个单词。\n",
    "                # idx 表示该单词在词汇表中的索引。\n",
    "                one_hot_vectors[i, j, idx] = 1.0\n",
    "\n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd381bc1f13c4141affae611f5f2be8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b56e5554f84afaafc102a77bf0b2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad116dbc5c44cada47d9cea9185777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 4, 'label_text': 'very positive', 'token_ids': [2595, 12425, 14585, 14854, 14762, 8410, 13079, 16340, 4271, 11038, 14762, 9799, 7145, 14762, 16379, 6888, 10894]}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = prepare_dataset(dataset)\n",
    "print(preprocessed_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collate function for dynamic truncation\n",
    "def collate_fn(batch):\n",
    "    ## Truncate all sentences in the batch to the shortest length\n",
    "\n",
    "    # Find the minimum length of the sentences in the batch\n",
    "    min_length = min([len(example[\"token_ids\"]) for example in batch])\n",
    "    \n",
    "    # We don't convert the inputs to tensors here because we will apply one-hot encoding and therefore converting to tensors in the model on-the-fly\n",
    "    # 对于每个样本，提取 \"token_ids\" 字段，并截取到 min_length。\n",
    "    inputs = [example[\"token_ids\"][:min_length] for example in batch]\n",
    "    # 同样使用列表推导式，从每个样本中提取 \"label\" 字段。\n",
    "    # 使用 torch.tensor() 将标签列表转换为 PyTorch 张量。\n",
    "    labels = torch.tensor([example[\"label\"] for example in batch])\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3546, 4271, 9799, 8493, 2330, 5659, 11877, 14585, 14762, 14608, 10104, 16211, 7181, 10061, 12952, 2810, 9799], [2595, 7874, 14585, 14445, 14762, 15571, 14854, 12751, 6499, 14952, 9799, 6828, 14585, 14094, 15712, 12566, 9628], [1922, 6328, 6976, 3219, 6442, 9799, 2023, 13635, 6902, 8018, 13538, 14762, 15667, 5881, 10617, 7818, 10766], [9799, 8488, 3719, 2222, 7851, 2595, 4602, 16363, 8879, 14762, 1686, 3546, 7647, 4410, 14585, 1765, 9799], [2210, 2595, 11512, 10659, 6528, 4271, 9799, 14782, 4271, 539, 14762, 9799, 3992, 4271, 4520, 2595, 11044], [3992, 15535, 225, 9892, 15923, 14952, 15141, 2595, 15252, 12935, 6976, 7517, 8023, 14585, 6919, 288, 14585], [9799, 2330, 14762, 4726, 13880, 9272, 5431, 6976, 6729, 5305, 15677, 5659, 7061, 11146, 2595, 461, 212], [15657, 14952, 9799, 9210, 14762, 8636, 2595, 2684, 145, 6122, 6225, 14468, 3958, 9272, 5659, 16323, 9184]]\n",
      "tensor([1, 4, 2, 3, 3, 1, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for train and test datasets\n",
    "train_dataloader = DataLoader(\n",
    "    # 当 DataLoader 从数据集中抽取一批样本时，它会调用 collate_fn 函数来将这些单独的样本组合成一个批次。\n",
    "    # 这个函数定义了如何处理可能长度不同的样本，以及如何将它们组合成适合模型输入的格式。\n",
    "    preprocessed_dataset[\"train\"], batch_size=8, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    preprocessed_dataset[\"validation\"], batch_size=32, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    preprocessed_dataset[\"test\"], batch_size=32, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inputs, labels = batch\n",
    "    print(inputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_classes):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids: list[list[int]]):\n",
    "        # Instead of the embedding layer, we will use one-hot encoding\n",
    "        # Note: you could also use torch's Embedding layer initialized with the one-hot vectors\n",
    "        encodings = one_hot_encode_batch(input_ids)\n",
    "        # Sum the one-hot vectors to get the bag of words representation\n",
    "        bag_of_words = encodings.sum(dim=1)\n",
    "        # Apply the hidden layer and the output layer\n",
    "        a_1 = torch.relu(self.hidden_layer(bag_of_words))\n",
    "        # No activation function is applied to the output layer because we will use CrossEntropyLoss which applies softmax\n",
    "        z_2 = self.output_layer(a_1)\n",
    "        return z_2\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "hidden_dim = 256\n",
    "num_classes = 5\n",
    "model = SentimentModel(vocab_size, hidden_dim, num_classes)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5278\n",
      "Epoch 2/10, Loss: 1.2820\n",
      "Epoch 3/10, Loss: 0.9254\n",
      "Epoch 4/10, Loss: 0.6539\n",
      "Epoch 5/10, Loss: 0.4555\n",
      "Epoch 6/10, Loss: 0.3649\n",
      "Epoch 7/10, Loss: 0.3210\n",
      "Epoch 8/10, Loss: 0.2873\n",
      "Epoch 9/10, Loss: 0.2652\n",
      "Epoch 10/10, Loss: 0.2416\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # model.train() 是一个非常重要的 PyTorch 方法调用，\n",
    "    # 用于将模型设置为训练模式。这行代码通常在开始训练循环之前调用\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Bonus: Evaluate the model by means of accuracy (percentage of correctly predicted classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # 这个函数用于找出张量中的最大值。当应用于二维张量时，它可以沿着指定的维度找出最大值。\n",
    "            # 在分类任务中，它通常是一个形状为 (batch_size, num_classes) 的二维张量。\n",
    "            # torch.max() 返回两个值：最大值和最大值的索引。\n",
    "            # 在这里，我们使用 _ 忽略了最大值，只保留了索引。\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3222\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_accuracy = evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
